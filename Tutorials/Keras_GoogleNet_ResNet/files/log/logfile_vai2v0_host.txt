 Copyright 2021 Xilinx Inc.
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
     http://www.apache.org/licenses/LICENSE-2.0
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.



/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/run_all.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/run_cifar10.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/run_fmnist.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/code/build_app.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/code/build_cifar10_test.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/code/build_fmnist_test.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/code/build_get_dpu_fps.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/code/check_cifar10_accuracy.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/code/check_fmnist_accuracy.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/code/fps_cifar10.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/code/fps_fmnist.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/run_all_cifar10_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/run_all_fmnist_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/run_all_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/target_zcu102/code/build_app.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/target_zcu102/code/build_cifar10_test.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/target_zcu102/code/build_fmnist_test.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/target_zcu102/code/build_get_dpu_fps.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/target_zcu102/code/check_cifar10_accuracy.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/target_zcu102/code/check_fmnist_accuracy.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/target_zcu102/code/fps_cifar10.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/target_zcu102/code/fps_fmnist.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/target_zcu102/run_all_cifar10_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/target_zcu102/run_all_fmnist_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_vck190/target_zcu102/run_all_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu102/code/build_app.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu102/code/build_cifar10_test.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu102/code/build_fmnist_test.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu102/code/build_get_dpu_fps.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu102/code/check_cifar10_accuracy.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu102/code/check_fmnist_accuracy.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu102/code/fps_cifar10.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu102/code/fps_fmnist.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu102/run_all_cifar10_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu102/run_all_fmnist_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu102/run_all_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/code/build_app.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/code/build_cifar10_test.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/code/build_fmnist_test.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/code/build_get_dpu_fps.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/code/check_cifar10_accuracy.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/code/check_fmnist_accuracy.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/code/fps_cifar10.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/code/fps_fmnist.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/run_all_cifar10_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/run_all_fmnist_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/run_all_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/target_zcu102/code/build_app.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/target_zcu102/code/build_cifar10_test.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/target_zcu102/code/build_fmnist_test.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/target_zcu102/code/build_get_dpu_fps.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/target_zcu102/code/check_cifar10_accuracy.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/target_zcu102/code/check_fmnist_accuracy.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/target_zcu102/code/fps_cifar10.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/target_zcu102/code/fps_fmnist.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/target_zcu102/run_all_cifar10_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/target_zcu102/run_all_fmnist_target.sh
/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/target_zcu104/target_zcu102/run_all_target.sh
This script is located in:  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files
Directory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist created 
Directory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/train created 
Directory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/valid created 
Directory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/test created 
Directory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/calib created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/train/top created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/train/trouser created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/train/pullover created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/train/dress created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/train/coat created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/train/sandal created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/train/shirt created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/train/sneaker created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/train/bag created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/train/ankleBoot created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/valid/top created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/valid/trouser created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/valid/pullover created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/valid/dress created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/valid/coat created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/valid/sandal created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/valid/shirt created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/valid/sneaker created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/valid/bag created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/valid/ankleBoot created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/test/top created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/test/trouser created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/test/pullover created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/test/dress created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/test/coat created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/test/sandal created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/test/shirt created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/test/sneaker created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/test/bag created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/test/ankleBoot created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/calib/top created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/calib/trouser created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/calib/pullover created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/calib/dress created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/calib/coat created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/calib/sandal created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/calib/shirt created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/calib/sneaker created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/calib/bag created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist/calib/ankleBoot created 
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz
 8192/29515 [=======>......................] - ETA: 0s32768/29515 [=================================] - 0s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz
    8192/26421880 [..............................] - ETA: 0s  409600/26421880 [..............................] - ETA: 3s  778240/26421880 [..............................] - ETA: 3s 1032192/26421880 [>.............................] - ETA: 3s 1212416/26421880 [>.............................] - ETA: 4s 1392640/26421880 [>.............................] - ETA: 4s 1589248/26421880 [>.............................] - ETA: 4s 1867776/26421880 [=>............................] - ETA: 4s 2097152/26421880 [=>............................] - ETA: 4s 2359296/26421880 [=>............................] - ETA: 4s 2686976/26421880 [==>...........................] - ETA: 4s 3080192/26421880 [==>...........................] - ETA: 4s 3538944/26421880 [===>..........................] - ETA: 4s 4030464/26421880 [===>..........................] - ETA: 3s 4521984/26421880 [====>.........................] - ETA: 3s 5013504/26421880 [====>.........................] - ETA: 3s 5423104/26421880 [=====>........................] - ETA: 3s 5701632/26421880 [=====>........................] - ETA: 3s 5996544/26421880 [=====>........................] - ETA: 3s 6291456/26421880 [======>.......................] - ETA: 3s 6586368/26421880 [======>.......................] - ETA: 3s 6873088/26421880 [======>.......................] - ETA: 3s 7127040/26421880 [=======>......................] - ETA: 3s 7389184/26421880 [=======>......................] - ETA: 3s 7634944/26421880 [=======>......................] - ETA: 3s 7913472/26421880 [=======>......................] - ETA: 3s 8192000/26421880 [========>.....................] - ETA: 2s 8536064/26421880 [========>.....................] - ETA: 2s 8929280/26421880 [=========>....................] - ETA: 2s 9371648/26421880 [=========>....................] - ETA: 2s 9863168/26421880 [==========>...................] - ETA: 2s10354688/26421880 [==========>...................] - ETA: 2s10764288/26421880 [===========>..................] - ETA: 2s11059200/26421880 [===========>..................] - ETA: 2s11354112/26421880 [===========>..................] - ETA: 2s11649024/26421880 [============>.................] - ETA: 2s11943936/26421880 [============>.................] - ETA: 2s12222464/26421880 [============>.................] - ETA: 2s12484608/26421880 [=============>................] - ETA: 2s12746752/26421880 [=============>................] - ETA: 2s12992512/26421880 [=============>................] - ETA: 2s13287424/26421880 [==============>...............] - ETA: 2s13582336/26421880 [==============>...............] - ETA: 2s13942784/26421880 [==============>...............] - ETA: 1s14360576/26421880 [===============>..............] - ETA: 1s14827520/26421880 [===============>..............] - ETA: 1s15319040/26421880 [================>.............] - ETA: 1s15810560/26421880 [================>.............] - ETA: 1s16285696/26421880 [=================>............] - ETA: 1s16580608/26421880 [=================>............] - ETA: 1s16875520/26421880 [==================>...........] - ETA: 1s17170432/26421880 [==================>...........] - ETA: 1s17465344/26421880 [==================>...........] - ETA: 1s17743872/26421880 [===================>..........] - ETA: 1s18022400/26421880 [===================>..........] - ETA: 1s18284544/26421880 [===================>..........] - ETA: 1s18546688/26421880 [====================>.........] - ETA: 1s18792448/26421880 [====================>.........] - ETA: 1s19103744/26421880 [====================>.........] - ETA: 1s19431424/26421880 [=====================>........] - ETA: 1s19824640/26421880 [=====================>........] - ETA: 1s20283392/26421880 [======================>.......] - ETA: 0s20774912/26421880 [======================>.......] - ETA: 0s21266432/26421880 [=======================>......] - ETA: 0s21757952/26421880 [=======================>......] - ETA: 0s22102016/26421880 [========================>.....] - ETA: 0s22396928/26421880 [========================>.....] - ETA: 0s22691840/26421880 [========================>.....] - ETA: 0s22986752/26421880 [=========================>....] - ETA: 0s23265280/26421880 [=========================>....] - ETA: 0s23543808/26421880 [=========================>....] - ETA: 0s23805952/26421880 [==========================>...] - ETA: 0s24068096/26421880 [==========================>...] - ETA: 0s24313856/26421880 [==========================>...] - ETA: 0s24592384/26421880 [==========================>...] - ETA: 0s24903680/26421880 [===========================>..] - ETA: 0s25264128/26421880 [===========================>..] - ETA: 0s25706496/26421880 [============================>.] - ETA: 0s26198016/26421880 [============================>.] - ETA: 0s26427392/26421880 [==============================] - 4s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz
8192/5148 [===============================================] - 0s 0us/step
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz
   8192/4422102 [..............................] - ETA: 0s 114688/4422102 [..............................] - ETA: 1s 532480/4422102 [==>...........................] - ETA: 0s1015808/4422102 [=====>........................] - ETA: 0s1499136/4422102 [=========>....................] - ETA: 0s1843200/4422102 [===========>..................] - ETA: 0s2236416/4422102 [==============>...............] - ETA: 0s2727936/4422102 [=================>............] - ETA: 0s3219456/4422102 [====================>.........] - ETA: 0s3710976/4422102 [========================>.....] - ETA: 0s4202496/4422102 [===========================>..] - ETA: 0s4423680/4422102 [==============================] - 1s 0us/step
60000 images in original train dataset
10000 images in original test  dataset
classes histogram in train and test dataset:  [1000 1000 1000 1000 1000 1000 1000 1000 1000 1000]
num images in train folder =  60000
num images in val folder   =  5000
num images in pred folder  =  5000

FINISHED CREATING DATASET

This script is located in:  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files
Directory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10 created 
Directory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/train created 
Directory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/valid created 
Directory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/test created 
Directory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/calib created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/train/airplane created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/train/automobile created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/train/bird created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/train/cat created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/train/deer created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/train/dog created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/train/frog created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/train/horse created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/train/ship created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/train/truck created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/valid/airplane created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/valid/automobile created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/valid/bird created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/valid/cat created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/valid/deer created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/valid/dog created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/valid/frog created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/valid/horse created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/valid/ship created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/valid/truck created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/test/airplane created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/test/automobile created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/test/bird created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/test/cat created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/test/deer created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/test/dog created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/test/frog created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/test/horse created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/test/ship created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/test/truck created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/calib/airplane created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/calib/automobile created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/calib/bird created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/calib/cat created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/calib/deer created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/calib/dog created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/calib/frog created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/calib/horse created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/calib/ship created 
subDirectory /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10/calib/truck created 
Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz
     8192/170498071 [..............................] - ETA: 38:04    40960/170498071 [..............................] - ETA: 15:27    90112/170498071 [..............................] - ETA: 10:45   204800/170498071 [..............................] - ETA: 6:15    434176/170498071 [..............................] - ETA: 3:40   876544/170498071 [..............................] - ETA: 1:59   892928/170498071 [..............................] - ETA: 2:08  1384448/170498071 [..............................] - ETA: 1:28  1794048/170498071 [..............................] - ETA: 1:13  2138112/170498071 [..............................] - ETA: 1:05  2531328/170498071 [..............................] - ETA: 58s   2957312/170498071 [..............................] - ETA: 53s  3350528/170498071 [..............................] - ETA: 49s  3743744/170498071 [..............................] - ETA: 46s  4161536/170498071 [..............................] - ETA: 43s  4562944/170498071 [..............................] - ETA: 41s  4972544/170498071 [..............................] - ETA: 39s  5382144/170498071 [..............................] - ETA: 38s  5791744/170498071 [>.............................] - ETA: 36s  6184960/170498071 [>.............................] - ETA: 35s  6610944/170498071 [>.............................] - ETA: 34s  7004160/170498071 [>.............................] - ETA: 33s  7446528/170498071 [>.............................] - ETA: 32s  7856128/170498071 [>.............................] - ETA: 32s  8298496/170498071 [>.............................] - ETA: 31s  8708096/170498071 [>.............................] - ETA: 30s  9150464/170498071 [>.............................] - ETA: 29s  9560064/170498071 [>.............................] - ETA: 29s 10035200/170498071 [>.............................] - ETA: 28s 10510336/170498071 [>.............................] - ETA: 28s 10985472/170498071 [>.............................] - ETA: 27s 11444224/170498071 [=>............................] - ETA: 27s 11919360/170498071 [=>............................] - ETA: 26s 12378112/170498071 [=>............................] - ETA: 26s 12853248/170498071 [=>............................] - ETA: 25s 13328384/170498071 [=>............................] - ETA: 25s 13795328/170498071 [=>............................] - ETA: 25s 14278656/170498071 [=>............................] - ETA: 24s 14721024/170498071 [=>............................] - ETA: 24s 15212544/170498071 [=>............................] - ETA: 24s 15622144/170498071 [=>............................] - ETA: 26s 16400384/170498071 [=>............................] - ETA: 25s 17014784/170498071 [=>............................] - ETA: 24s 18767872/170498071 [==>...........................] - ETA: 22s 18980864/170498071 [==>...........................] - ETA: 22s 19423232/170498071 [==>...........................] - ETA: 22s 19898368/170498071 [==>...........................] - ETA: 22s 20045824/170498071 [==>...........................] - ETA: 23s 21962752/170498071 [==>...........................] - ETA: 21s 22355968/170498071 [==>...........................] - ETA: 21s 22749184/170498071 [===>..........................] - ETA: 21s 23150592/170498071 [===>..........................] - ETA: 21s 23535616/170498071 [===>..........................] - ETA: 21s 23961600/170498071 [===>..........................] - ETA: 21s 24371200/170498071 [===>..........................] - ETA: 20s 24788992/170498071 [===>..........................] - ETA: 20s 25206784/170498071 [===>..........................] - ETA: 20s 25632768/170498071 [===>..........................] - ETA: 20s 26058752/170498071 [===>..........................] - ETA: 20s 26484736/170498071 [===>..........................] - ETA: 20s 26910720/170498071 [===>..........................] - ETA: 20s 27336704/170498071 [===>..........................] - ETA: 20s 27762688/170498071 [===>..........................] - ETA: 20s 28205056/170498071 [===>..........................] - ETA: 19s 28647424/170498071 [====>.........................] - ETA: 19s 29081600/170498071 [====>.........................] - ETA: 19s 29515776/170498071 [====>.........................] - ETA: 19s 29958144/170498071 [====>.........................] - ETA: 19s 30416896/170498071 [====>.........................] - ETA: 19s 30908416/170498071 [====>.........................] - ETA: 19s 31399936/170498071 [====>.........................] - ETA: 19s 31891456/170498071 [====>.........................] - ETA: 18s 32382976/170498071 [====>.........................] - ETA: 18s 32825344/170498071 [====>.........................] - ETA: 18s 33316864/170498071 [====>.........................] - ETA: 18s 33759232/170498071 [====>.........................] - ETA: 18s 34201600/170498071 [=====>........................] - ETA: 18s 34643968/170498071 [=====>........................] - ETA: 18s 35086336/170498071 [=====>........................] - ETA: 18s 35528704/170498071 [=====>........................] - ETA: 18s 35971072/170498071 [=====>........................] - ETA: 17s 36413440/170498071 [=====>........................] - ETA: 17s 36855808/170498071 [=====>........................] - ETA: 17s 37298176/170498071 [=====>........................] - ETA: 17s 37740544/170498071 [=====>........................] - ETA: 17s 38182912/170498071 [=====>........................] - ETA: 17s 38625280/170498071 [=====>........................] - ETA: 17s 39067648/170498071 [=====>........................] - ETA: 17s 39510016/170498071 [=====>........................] - ETA: 17s 39952384/170498071 [======>.......................] - ETA: 17s 40402944/170498071 [======>.......................] - ETA: 17s 40853504/170498071 [======>.......................] - ETA: 17s 41295872/170498071 [======>.......................] - ETA: 16s 41754624/170498071 [======>.......................] - ETA: 16s 42213376/170498071 [======>.......................] - ETA: 16s 42672128/170498071 [======>.......................] - ETA: 16s 43130880/170498071 [======>.......................] - ETA: 16s 43589632/170498071 [======>.......................] - ETA: 16s 44048384/170498071 [======>.......................] - ETA: 16s 44507136/170498071 [======>.......................] - ETA: 16s 44965888/170498071 [======>.......................] - ETA: 16s 45424640/170498071 [======>.......................] - ETA: 16s 45883392/170498071 [=======>......................] - ETA: 16s 46342144/170498071 [=======>......................] - ETA: 16s 46800896/170498071 [=======>......................] - ETA: 15s 47259648/170498071 [=======>......................] - ETA: 15s 47718400/170498071 [=======>......................] - ETA: 15s 48177152/170498071 [=======>......................] - ETA: 15s 48635904/170498071 [=======>......................] - ETA: 15s 49094656/170498071 [=======>......................] - ETA: 15s 49553408/170498071 [=======>......................] - ETA: 15s 50012160/170498071 [=======>......................] - ETA: 15s 50470912/170498071 [=======>......................] - ETA: 15s 50929664/170498071 [=======>......................] - ETA: 15s 51388416/170498071 [========>.....................] - ETA: 15s 51847168/170498071 [========>.....................] - ETA: 15s 52305920/170498071 [========>.....................] - ETA: 14s 52764672/170498071 [========>.....................] - ETA: 14s 53223424/170498071 [========>.....................] - ETA: 14s 53682176/170498071 [========>.....................] - ETA: 14s 54140928/170498071 [========>.....................] - ETA: 14s 54599680/170498071 [========>.....................] - ETA: 14s 55091200/170498071 [========>.....................] - ETA: 14s 55582720/170498071 [========>.....................] - ETA: 14s 56074240/170498071 [========>.....................] - ETA: 14s 56565760/170498071 [========>.....................] - ETA: 14s 57057280/170498071 [=========>....................] - ETA: 14s 57516032/170498071 [=========>....................] - ETA: 14s 58007552/170498071 [=========>....................] - ETA: 14s 58466304/170498071 [=========>....................] - ETA: 13s 58925056/170498071 [=========>....................] - ETA: 13s 59383808/170498071 [=========>....................] - ETA: 13s 59842560/170498071 [=========>....................] - ETA: 13s 60301312/170498071 [=========>....................] - ETA: 13s 60760064/170498071 [=========>....................] - ETA: 13s 61218816/170498071 [=========>....................] - ETA: 13s 61677568/170498071 [=========>....................] - ETA: 13s 62136320/170498071 [=========>....................] - ETA: 13s 62595072/170498071 [==========>...................] - ETA: 13s 63053824/170498071 [==========>...................] - ETA: 13s 63512576/170498071 [==========>...................] - ETA: 13s 63987712/170498071 [==========>...................] - ETA: 13s 64446464/170498071 [==========>...................] - ETA: 13s 64905216/170498071 [==========>...................] - ETA: 13s 65380352/170498071 [==========>...................] - ETA: 12s 65839104/170498071 [==========>...................] - ETA: 12s 66306048/170498071 [==========>...................] - ETA: 12s 66756608/170498071 [==========>...................] - ETA: 12s 67215360/170498071 [==========>...................] - ETA: 12s 67674112/170498071 [==========>...................] - ETA: 12s 68132864/170498071 [==========>...................] - ETA: 12s 68591616/170498071 [===========>..................] - ETA: 12s 69066752/170498071 [===========>..................] - ETA: 12s 69525504/170498071 [===========>..................] - ETA: 12s 69984256/170498071 [===========>..................] - ETA: 12s 70443008/170498071 [===========>..................] - ETA: 12s 70901760/170498071 [===========>..................] - ETA: 12s 71360512/170498071 [===========>..................] - ETA: 12s 71819264/170498071 [===========>..................] - ETA: 12s 72310784/170498071 [===========>..................] - ETA: 11s 72802304/170498071 [===========>..................] - ETA: 11s 73293824/170498071 [===========>..................] - ETA: 11s 73768960/170498071 [===========>..................] - ETA: 11s 74235904/170498071 [============>.................] - ETA: 11s 74686464/170498071 [============>.................] - ETA: 11s 75128832/170498071 [============>.................] - ETA: 11s 75571200/170498071 [============>.................] - ETA: 11s 76013568/170498071 [============>.................] - ETA: 11s 76455936/170498071 [============>.................] - ETA: 11s 76898304/170498071 [============>.................] - ETA: 11s 77348864/170498071 [============>.................] - ETA: 11s 77783040/170498071 [============>.................] - ETA: 11s 78225408/170498071 [============>.................] - ETA: 11s 78667776/170498071 [============>.................] - ETA: 11s 79110144/170498071 [============>.................] - ETA: 11s 79552512/170498071 [============>.................] - ETA: 11s 79978496/170498071 [=============>................] - ETA: 10s 80404480/170498071 [=============>................] - ETA: 10s 80830464/170498071 [=============>................] - ETA: 10s 81256448/170498071 [=============>................] - ETA: 10s 81698816/170498071 [=============>................] - ETA: 10s 82157568/170498071 [=============>................] - ETA: 10s 82649088/170498071 [=============>................] - ETA: 10s 83140608/170498071 [=============>................] - ETA: 10s 83632128/170498071 [=============>................] - ETA: 10s 84123648/170498071 [=============>................] - ETA: 10s 84615168/170498071 [=============>................] - ETA: 10s 85090304/170498071 [=============>................] - ETA: 10s 85581824/170498071 [==============>...............] - ETA: 10s 86040576/170498071 [==============>...............] - ETA: 10s 86515712/170498071 [==============>...............] - ETA: 10s 86958080/170498071 [==============>...............] - ETA: 10s 87400448/170498071 [==============>...............] - ETA: 9s  87842816/170498071 [==============>...............] - ETA: 9s 88268800/170498071 [==============>...............] - ETA: 9s 88694784/170498071 [==============>...............] - ETA: 9s 89120768/170498071 [==============>...............] - ETA: 9s 89546752/170498071 [==============>...............] - ETA: 9s 89972736/170498071 [==============>...............] - ETA: 9s 90390528/170498071 [==============>...............] - ETA: 9s 90808320/170498071 [==============>...............] - ETA: 9s 91234304/170498071 [===============>..............] - ETA: 9s 91643904/170498071 [===============>..............] - ETA: 9s 92053504/170498071 [===============>..............] - ETA: 9s 92463104/170498071 [===============>..............] - ETA: 9s 92872704/170498071 [===============>..............] - ETA: 9s 93298688/170498071 [===============>..............] - ETA: 9s 93708288/170498071 [===============>..............] - ETA: 9s 94117888/170498071 [===============>..............] - ETA: 9s 94511104/170498071 [===============>..............] - ETA: 9s 94937088/170498071 [===============>..............] - ETA: 9s 95346688/170498071 [===============>..............] - ETA: 9s 95756288/170498071 [===============>..............] - ETA: 8s 96149504/170498071 [===============>..............] - ETA: 8s 96559104/170498071 [===============>..............] - ETA: 8s 96968704/170498071 [================>.............] - ETA: 8s 97460224/170498071 [================>.............] - ETA: 8s 97951744/170498071 [================>.............] - ETA: 8s 98443264/170498071 [================>.............] - ETA: 8s 98934784/170498071 [================>.............] - ETA: 8s 99426304/170498071 [================>.............] - ETA: 8s 99835904/170498071 [================>.............] - ETA: 8s100311040/170498071 [================>.............] - ETA: 8s100720640/170498071 [================>.............] - ETA: 8s101130240/170498071 [================>.............] - ETA: 8s101539840/170498071 [================>.............] - ETA: 8s101949440/170498071 [================>.............] - ETA: 8s102342656/170498071 [=================>............] - ETA: 8s102744064/170498071 [=================>............] - ETA: 8s103129088/170498071 [=================>............] - ETA: 8s103522304/170498071 [=================>............] - ETA: 8s103915520/170498071 [=================>............] - ETA: 7s104325120/170498071 [=================>............] - ETA: 7s104718336/170498071 [=================>............] - ETA: 7s105111552/170498071 [=================>............] - ETA: 7s105504768/170498071 [=================>............] - ETA: 7s105897984/170498071 [=================>............] - ETA: 7s106307584/170498071 [=================>............] - ETA: 7s106700800/170498071 [=================>............] - ETA: 7s107094016/170498071 [=================>............] - ETA: 7s107487232/170498071 [=================>............] - ETA: 7s107880448/170498071 [=================>............] - ETA: 7s108273664/170498071 [==================>...........] - ETA: 7s108765184/170498071 [==================>...........] - ETA: 7s109256704/170498071 [==================>...........] - ETA: 7s109748224/170498071 [==================>...........] - ETA: 7s110239744/170498071 [==================>...........] - ETA: 7s110682112/170498071 [==================>...........] - ETA: 7s111091712/170498071 [==================>...........] - ETA: 7s111501312/170498071 [==================>...........] - ETA: 7s111910912/170498071 [==================>...........] - ETA: 7s112304128/170498071 [==================>...........] - ETA: 6s112697344/170498071 [==================>...........] - ETA: 6s113090560/170498071 [==================>...........] - ETA: 6s113500160/170498071 [==================>...........] - ETA: 6s113893376/170498071 [===================>..........] - ETA: 6s114286592/170498071 [===================>..........] - ETA: 6s114679808/170498071 [===================>..........] - ETA: 6s115073024/170498071 [===================>..........] - ETA: 6s115466240/170498071 [===================>..........] - ETA: 6s115859456/170498071 [===================>..........] - ETA: 6s116252672/170498071 [===================>..........] - ETA: 6s116645888/170498071 [===================>..........] - ETA: 6s117055488/170498071 [===================>..........] - ETA: 6s117448704/170498071 [===================>..........] - ETA: 6s117841920/170498071 [===================>..........] - ETA: 6s118235136/170498071 [===================>..........] - ETA: 6s118644736/170498071 [===================>..........] - ETA: 6s119070720/170498071 [===================>..........] - ETA: 6s119562240/170498071 [====================>.........] - ETA: 6s120053760/170498071 [====================>.........] - ETA: 6s120545280/170498071 [====================>.........] - ETA: 6s121036800/170498071 [====================>.........] - ETA: 5s121528320/170498071 [====================>.........] - ETA: 5s121954304/170498071 [====================>.........] - ETA: 5s122363904/170498071 [====================>.........] - ETA: 5s122789888/170498071 [====================>.........] - ETA: 5s123199488/170498071 [====================>.........] - ETA: 5s123592704/170498071 [====================>.........] - ETA: 5s123985920/170498071 [====================>.........] - ETA: 5s124387328/170498071 [====================>.........] - ETA: 5s124772352/170498071 [====================>.........] - ETA: 5s125165568/170498071 [=====================>........] - ETA: 5s125575168/170498071 [=====================>........] - ETA: 5s125968384/170498071 [=====================>........] - ETA: 5s126377984/170498071 [=====================>........] - ETA: 5s126787584/170498071 [=====================>........] - ETA: 5s127197184/170498071 [=====================>........] - ETA: 5s127606784/170498071 [=====================>........] - ETA: 5s128016384/170498071 [=====================>........] - ETA: 5s128425984/170498071 [=====================>........] - ETA: 5s128835584/170498071 [=====================>........] - ETA: 5s129245184/170498071 [=====================>........] - ETA: 4s129654784/170498071 [=====================>........] - ETA: 4s130064384/170498071 [=====================>........] - ETA: 4s130473984/170498071 [=====================>........] - ETA: 4s130899968/170498071 [======================>.......] - ETA: 4s131391488/170498071 [======================>.......] - ETA: 4s131883008/170498071 [======================>.......] - ETA: 4s132374528/170498071 [======================>.......] - ETA: 4s132866048/170498071 [======================>.......] - ETA: 4s133357568/170498071 [======================>.......] - ETA: 4s133808128/170498071 [======================>.......] - ETA: 4s134225920/170498071 [======================>.......] - ETA: 4s134651904/170498071 [======================>.......] - ETA: 4s135061504/170498071 [======================>.......] - ETA: 4s135471104/170498071 [======================>.......] - ETA: 4s135880704/170498071 [======================>.......] - ETA: 4s136306688/170498071 [======================>.......] - ETA: 4s136716288/170498071 [=======================>......] - ETA: 4s137125888/170498071 [=======================>......] - ETA: 4s137551872/170498071 [=======================>......] - ETA: 3s137961472/170498071 [=======================>......] - ETA: 3s138371072/170498071 [=======================>......] - ETA: 3s138780672/170498071 [=======================>......] - ETA: 3s139206656/170498071 [=======================>......] - ETA: 3s139624448/170498071 [=======================>......] - ETA: 3s140042240/170498071 [=======================>......] - ETA: 3s140468224/170498071 [=======================>......] - ETA: 3s140894208/170498071 [=======================>......] - ETA: 3s141320192/170498071 [=======================>......] - ETA: 3s141746176/170498071 [=======================>......] - ETA: 3s142163968/170498071 [========================>.....] - ETA: 3s142581760/170498071 [========================>.....] - ETA: 3s143007744/170498071 [========================>.....] - ETA: 3s143433728/170498071 [========================>.....] - ETA: 3s143859712/170498071 [========================>.....] - ETA: 3s144285696/170498071 [========================>.....] - ETA: 3s144711680/170498071 [========================>.....] - ETA: 3s145170432/170498071 [========================>.....] - ETA: 3s145661952/170498071 [========================>.....] - ETA: 2s146153472/170498071 [========================>.....] - ETA: 2s146644992/170498071 [========================>.....] - ETA: 2s147103744/170498071 [========================>.....] - ETA: 2s147529728/170498071 [========================>.....] - ETA: 2s147972096/170498071 [=========================>....] - ETA: 2s148398080/170498071 [=========================>....] - ETA: 2s148824064/170498071 [=========================>....] - ETA: 2s149250048/170498071 [=========================>....] - ETA: 2s149676032/170498071 [=========================>....] - ETA: 2s150118400/170498071 [=========================>....] - ETA: 2s150544384/170498071 [=========================>....] - ETA: 2s150986752/170498071 [=========================>....] - ETA: 2s151429120/170498071 [=========================>....] - ETA: 2s151855104/170498071 [=========================>....] - ETA: 2s152297472/170498071 [=========================>....] - ETA: 2s152739840/170498071 [=========================>....] - ETA: 2s153165824/170498071 [=========================>....] - ETA: 2s153608192/170498071 [==========================>...] - ETA: 2s154050560/170498071 [==========================>...] - ETA: 1s154476544/170498071 [==========================>...] - ETA: 1s154918912/170498071 [==========================>...] - ETA: 1s155353088/170498071 [==========================>...] - ETA: 1s155787264/170498071 [==========================>...] - ETA: 1s156229632/170498071 [==========================>...] - ETA: 1s156672000/170498071 [==========================>...] - ETA: 1s157114368/170498071 [==========================>...] - ETA: 1s157556736/170498071 [==========================>...] - ETA: 1s157999104/170498071 [==========================>...] - ETA: 1s158441472/170498071 [==========================>...] - ETA: 1s158883840/170498071 [==========================>...] - ETA: 1s159334400/170498071 [===========================>..] - ETA: 1s159768576/170498071 [===========================>..] - ETA: 1s160210944/170498071 [===========================>..] - ETA: 1s160669696/170498071 [===========================>..] - ETA: 1s161112064/170498071 [===========================>..] - ETA: 1s161570816/170498071 [===========================>..] - ETA: 1s162013184/170498071 [===========================>..] - ETA: 1s162471936/170498071 [===========================>..] - ETA: 0s162914304/170498071 [===========================>..] - ETA: 0s163405824/170498071 [===========================>..] - ETA: 0s163880960/170498071 [===========================>..] - ETA: 0s164339712/170498071 [===========================>..] - ETA: 0s164798464/170498071 [===========================>..] - ETA: 0s165257216/170498071 [============================>.] - ETA: 0s165699584/170498071 [============================>.] - ETA: 0s166158336/170498071 [============================>.] - ETA: 0s166600704/170498071 [============================>.] - ETA: 0s167075840/170498071 [============================>.] - ETA: 0s167534592/170498071 [============================>.] - ETA: 0s167993344/170498071 [============================>.] - ETA: 0s168452096/170498071 [============================>.] - ETA: 0s168910848/170498071 [============================>.] - ETA: 0s169369600/170498071 [============================>.] - ETA: 0s169828352/170498071 [============================>.] - ETA: 0s170287104/170498071 [============================>.] - ETA: 0s170500096/170498071 [==============================] - 20s 0us/step
50000 images in original train dataset
10000 images in original test  dataset
classes histogram in train and test dataset:  [6000 6000 6000 6000 6000 6000 6000 6000 6000 6000]
classes histogram in train and test dataset:  [1000 1000 1000 1000 1000 1000 1000 1000 1000 1000]
num images in train folder =  50000
num images in val folder   =  5000
num images in pred folder  =  5000

FINISHED CREATING DATASET

 
##########################################################################
TRAIN & EVAL LeNet on CIFAR10
##########################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2022-06-09 06:13:59.473225: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 06:13:59.523980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 06:13:59.526035: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 06:13:59.566190: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 06:13:59.589251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 06:13:59.595732: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 06:13:59.639612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 06:13:59.667827: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 06:13:59.748696: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 06:13:59.753965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 06:13:59.755950: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 06:13:59.778612: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 06:13:59.780945: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559e5a15b4b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 06:13:59.780970: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 06:13:59.882655: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559e58c7e600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 06:13:59.882699: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 06:13:59.885342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 06:13:59.885448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 06:13:59.885487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 06:13:59.885549: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 06:13:59.885584: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 06:13:59.885618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 06:13:59.885652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 06:13:59.885687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 06:13:59.889949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 06:13:59.890058: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 06:13:59.895718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 06:13:59.895744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 06:13:59.895754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 06:13:59.899720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22580 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 06:14:02.528828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 06:14:02.931635: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 06:14:04.584183: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
[INFO] compiling model...
[INFO] training model...
Epoch 1/20
Epoch 1/20

Epoch 00001: val_loss improved from inf to 1.28330, saving model to keras_model/cifar10/LeNet/best_chkpt.hdf5
2500/2500 - 26s - loss: 1.5531 - acc: 0.4859 - val_loss: 1.2833 - val_acc: 0.5780
Epoch 2/20
Epoch 1/20

Epoch 00002: val_loss improved from 1.28330 to 1.15974, saving model to keras_model/cifar10/LeNet/best_chkpt.hdf5
2500/2500 - 24s - loss: 1.1060 - acc: 0.6505 - val_loss: 1.1597 - val_acc: 0.6410
Epoch 3/20
Epoch 1/20

Epoch 00003: val_loss improved from 1.15974 to 1.09369, saving model to keras_model/cifar10/LeNet/best_chkpt.hdf5
2500/2500 - 24s - loss: 0.9410 - acc: 0.7077 - val_loss: 1.0937 - val_acc: 0.6560
Epoch 4/20
Epoch 1/20

Epoch 00004: val_loss improved from 1.09369 to 1.08505, saving model to keras_model/cifar10/LeNet/best_chkpt.hdf5
2500/2500 - 24s - loss: 0.8110 - acc: 0.7543 - val_loss: 1.0850 - val_acc: 0.6745
Epoch 5/20
Epoch 1/20

Epoch 00005: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.7202 - acc: 0.7871 - val_loss: 1.1103 - val_acc: 0.6683
Epoch 6/20
Epoch 1/20

Epoch 00006: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.6097 - acc: 0.8274 - val_loss: 1.1383 - val_acc: 0.6794
Epoch 7/20
Epoch 1/20

Epoch 00007: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.5353 - acc: 0.8542 - val_loss: 1.2048 - val_acc: 0.6794
Epoch 8/20
Epoch 1/20

Epoch 00008: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.4589 - acc: 0.8843 - val_loss: 1.2323 - val_acc: 0.6721
Epoch 9/20
Epoch 1/20

Epoch 00009: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.3881 - acc: 0.9106 - val_loss: 1.3203 - val_acc: 0.6770
Epoch 10/20
Epoch 1/20

Epoch 00010: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.3367 - acc: 0.9298 - val_loss: 1.3783 - val_acc: 0.6704
Epoch 11/20
Epoch 1/20

Epoch 00011: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.2797 - acc: 0.9526 - val_loss: 1.4565 - val_acc: 0.6734
Epoch 12/20
Epoch 1/20

Epoch 00012: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.2473 - acc: 0.9637 - val_loss: 1.4889 - val_acc: 0.6700
Epoch 13/20
Epoch 1/20

Epoch 00013: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.2146 - acc: 0.9761 - val_loss: 1.5672 - val_acc: 0.6729
Epoch 14/20
Epoch 1/20

Epoch 00014: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.1922 - acc: 0.9841 - val_loss: 1.6217 - val_acc: 0.6678
Epoch 15/20
Epoch 1/20

Epoch 00015: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.1761 - acc: 0.9894 - val_loss: 1.6824 - val_acc: 0.6640
Epoch 16/20
Epoch 1/20

Epoch 00016: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.1621 - acc: 0.9926 - val_loss: 1.7576 - val_acc: 0.6647
Epoch 17/20
Epoch 1/20

Epoch 00017: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.1542 - acc: 0.9950 - val_loss: 1.8013 - val_acc: 0.6700
Epoch 18/20
Epoch 1/20

Epoch 00018: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.1464 - acc: 0.9967 - val_loss: 1.7734 - val_acc: 0.6666
Epoch 19/20
Epoch 1/20

Epoch 00019: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.1413 - acc: 0.9975 - val_loss: 1.8625 - val_acc: 0.6666
Epoch 20/20
Epoch 1/20

Epoch 00020: val_loss did not improve from 1.08505
2500/2500 - 23s - loss: 0.1381 - acc: 0.9978 - val_loss: 1.8610 - val_acc: 0.6686


Elapsed time for Keras training (s):  472.303181


[INFO] evaluating network on Test and Validation datasets...
  32/5000 [..............................] - ETA: 0s - loss: 2.7196 - acc: 0.5938 512/5000 [==>...........................] - ETA: 0s - loss: 1.9854 - acc: 0.6504 992/5000 [====>.........................] - ETA: 0s - loss: 1.9643 - acc: 0.64011472/5000 [=======>......................] - ETA: 0s - loss: 1.8650 - acc: 0.65621952/5000 [==========>...................] - ETA: 0s - loss: 1.8397 - acc: 0.66242432/5000 [=============>................] - ETA: 0s - loss: 1.7971 - acc: 0.66982912/5000 [================>.............] - ETA: 0s - loss: 1.7737 - acc: 0.67653392/5000 [===================>..........] - ETA: 0s - loss: 1.8334 - acc: 0.66983872/5000 [======================>.......] - ETA: 0s - loss: 1.8491 - acc: 0.67024352/5000 [=========================>....] - ETA: 0s - loss: 1.8531 - acc: 0.66874832/5000 [===========================>..] - ETA: 0s - loss: 1.8552 - acc: 0.66855000/5000 [==============================] - 1s 108us/sample - loss: 1.8484 - acc: 0.6690
Validation Loss: 1.848
validation Accuracy: 0.669
  32/5000 [..............................] - ETA: 0s - loss: 1.7950 - acc: 0.7812 480/5000 [=>............................] - ETA: 0s - loss: 1.7573 - acc: 0.6792 960/5000 [====>.........................] - ETA: 0s - loss: 1.9248 - acc: 0.66981440/5000 [=======>......................] - ETA: 0s - loss: 1.9411 - acc: 0.66251920/5000 [==========>...................] - ETA: 0s - loss: 1.8997 - acc: 0.67082400/5000 [=============>................] - ETA: 0s - loss: 1.8565 - acc: 0.67372848/5000 [================>.............] - ETA: 0s - loss: 1.8299 - acc: 0.67873296/5000 [==================>...........] - ETA: 0s - loss: 1.7785 - acc: 0.68453744/5000 [=====================>........] - ETA: 0s - loss: 1.7589 - acc: 0.68704224/5000 [========================>.....] - ETA: 0s - loss: 1.7826 - acc: 0.68444704/5000 [===========================>..] - ETA: 0s - loss: 1.7778 - acc: 0.68495000/5000 [==============================] - 1s 110us/sample - loss: 1.7863 - acc: 0.6832
Test Loss: 1.786
Test Accuracy: 0.683
              precision    recall  f1-score   support

    airplane       0.71      0.72      0.72       500
  automobile       0.78      0.78      0.78       500
        bird       0.59      0.56      0.57       500
         cat       0.49      0.48      0.49       500
        deer       0.68      0.62      0.65       500
         dog       0.56      0.59      0.57       500
        frog       0.73      0.80      0.76       500
       horse       0.75      0.70      0.73       500
        ship       0.80      0.80      0.80       500
       truck       0.73      0.78      0.75       500

    accuracy                           0.68      5000
   macro avg       0.68      0.68      0.68      5000
weighted avg       0.68      0.68      0.68      5000


TRAINING LeNet FINISHED

 
##########################################################################
TRAIN & EVAL miniVggNet  on CIFAR10
##########################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2022-06-09 06:22:42.742382: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 06:22:42.786720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 06:22:42.787004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 06:22:42.788451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 06:22:42.789725: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 06:22:42.790098: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 06:22:42.791732: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 06:22:42.792846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 06:22:42.795901: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 06:22:42.797416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 06:22:42.797827: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 06:22:42.821682: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 06:22:42.823431: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c36fd2cba0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 06:22:42.823446: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 06:22:42.929860: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c3701d31a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 06:22:42.929901: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 06:22:42.932614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 06:22:42.932766: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 06:22:42.932815: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 06:22:42.932839: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 06:22:42.932863: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 06:22:42.932887: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 06:22:42.932910: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 06:22:42.932935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 06:22:42.938104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 06:22:42.938212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 06:22:42.942209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 06:22:42.942234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 06:22:42.942244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 06:22:42.945688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22582 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 06:22:44.844577: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 06:22:44.952235: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 06:22:45.481911: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
[INFO] compiling model...
[INFO] training model...
Epoch 1/40
Epoch 1/40

Epoch 00001: val_loss improved from inf to 1.06706, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 23s - loss: 1.4774 - acc: 0.5189 - val_loss: 1.0671 - val_acc: 0.6625
Epoch 2/40
Epoch 1/40

Epoch 00002: val_loss improved from 1.06706 to 0.87189, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 1.0674 - acc: 0.6611 - val_loss: 0.8719 - val_acc: 0.7340
Epoch 3/40
Epoch 1/40

Epoch 00003: val_loss improved from 0.87189 to 0.78948, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.9219 - acc: 0.7134 - val_loss: 0.7895 - val_acc: 0.7642
Epoch 4/40
Epoch 1/40

Epoch 00004: val_loss did not improve from 0.78948
1250/1250 - 20s - loss: 0.8372 - acc: 0.7437 - val_loss: 0.7949 - val_acc: 0.7647
Epoch 5/40
Epoch 1/40

Epoch 00005: val_loss improved from 0.78948 to 0.72954, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.7796 - acc: 0.7666 - val_loss: 0.7295 - val_acc: 0.7820
Epoch 6/40
Epoch 1/40

Epoch 00006: val_loss improved from 0.72954 to 0.71235, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.7289 - acc: 0.7828 - val_loss: 0.7124 - val_acc: 0.7939
Epoch 7/40
Epoch 1/40

Epoch 00007: val_loss improved from 0.71235 to 0.69317, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.6892 - acc: 0.7989 - val_loss: 0.6932 - val_acc: 0.7966
Epoch 8/40
Epoch 1/40

Epoch 00008: val_loss improved from 0.69317 to 0.67394, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.6558 - acc: 0.8093 - val_loss: 0.6739 - val_acc: 0.8055
Epoch 9/40
Epoch 1/40

Epoch 00009: val_loss improved from 0.67394 to 0.65051, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.6301 - acc: 0.8180 - val_loss: 0.6505 - val_acc: 0.8134
Epoch 10/40
Epoch 1/40

Epoch 00010: val_loss improved from 0.65051 to 0.63713, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.6063 - acc: 0.8275 - val_loss: 0.6371 - val_acc: 0.8205
Epoch 11/40
Epoch 1/40

Epoch 00011: val_loss improved from 0.63713 to 0.62243, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.5789 - acc: 0.8355 - val_loss: 0.6224 - val_acc: 0.8215
Epoch 12/40
Epoch 1/40

Epoch 00012: val_loss did not improve from 0.62243
1250/1250 - 21s - loss: 0.5643 - acc: 0.8414 - val_loss: 0.6322 - val_acc: 0.8212
Epoch 13/40
Epoch 1/40

Epoch 00013: val_loss did not improve from 0.62243
1250/1250 - 21s - loss: 0.5454 - acc: 0.8471 - val_loss: 0.6229 - val_acc: 0.8277
Epoch 14/40
Epoch 1/40

Epoch 00014: val_loss improved from 0.62243 to 0.60979, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.5271 - acc: 0.8549 - val_loss: 0.6098 - val_acc: 0.8302
Epoch 15/40
Epoch 1/40

Epoch 00015: val_loss improved from 0.60979 to 0.60928, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.5136 - acc: 0.8584 - val_loss: 0.6093 - val_acc: 0.8292
Epoch 16/40
Epoch 1/40

Epoch 00016: val_loss improved from 0.60928 to 0.59860, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.4967 - acc: 0.8639 - val_loss: 0.5986 - val_acc: 0.8342
Epoch 17/40
Epoch 1/40

Epoch 00017: val_loss did not improve from 0.59860
1250/1250 - 21s - loss: 0.4824 - acc: 0.8694 - val_loss: 0.6140 - val_acc: 0.8309
Epoch 18/40
Epoch 1/40

Epoch 00018: val_loss did not improve from 0.59860
1250/1250 - 21s - loss: 0.4713 - acc: 0.8730 - val_loss: 0.6078 - val_acc: 0.8275
Epoch 19/40
Epoch 1/40

Epoch 00019: val_loss improved from 0.59860 to 0.59111, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.4658 - acc: 0.8747 - val_loss: 0.5911 - val_acc: 0.8337
Epoch 20/40
Epoch 1/40

Epoch 00020: val_loss improved from 0.59111 to 0.58638, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.4575 - acc: 0.8774 - val_loss: 0.5864 - val_acc: 0.8396
Epoch 21/40
Epoch 1/40

Epoch 00021: val_loss improved from 0.58638 to 0.58626, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.4402 - acc: 0.8833 - val_loss: 0.5863 - val_acc: 0.8382
Epoch 22/40
Epoch 1/40

Epoch 00022: val_loss did not improve from 0.58626
1250/1250 - 21s - loss: 0.4346 - acc: 0.8851 - val_loss: 0.5945 - val_acc: 0.8396
Epoch 23/40
Epoch 1/40

Epoch 00023: val_loss improved from 0.58626 to 0.58178, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.4231 - acc: 0.8891 - val_loss: 0.5818 - val_acc: 0.8445
Epoch 24/40
Epoch 1/40

Epoch 00024: val_loss improved from 0.58178 to 0.57941, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.4132 - acc: 0.8923 - val_loss: 0.5794 - val_acc: 0.8411
Epoch 25/40
Epoch 1/40

Epoch 00025: val_loss did not improve from 0.57941
1250/1250 - 21s - loss: 0.4104 - acc: 0.8939 - val_loss: 0.5872 - val_acc: 0.8392
Epoch 26/40
Epoch 1/40

Epoch 00026: val_loss did not improve from 0.57941
1250/1250 - 21s - loss: 0.3988 - acc: 0.8979 - val_loss: 0.5795 - val_acc: 0.8391
Epoch 27/40
Epoch 1/40

Epoch 00027: val_loss did not improve from 0.57941
1250/1250 - 21s - loss: 0.3923 - acc: 0.8998 - val_loss: 0.5806 - val_acc: 0.8426
Epoch 28/40
Epoch 1/40

Epoch 00028: val_loss did not improve from 0.57941
1250/1250 - 21s - loss: 0.3871 - acc: 0.9019 - val_loss: 0.5802 - val_acc: 0.8435
Epoch 29/40
Epoch 1/40

Epoch 00029: val_loss did not improve from 0.57941
1250/1250 - 21s - loss: 0.3805 - acc: 0.9039 - val_loss: 0.5817 - val_acc: 0.8426
Epoch 30/40
Epoch 1/40

Epoch 00030: val_loss improved from 0.57941 to 0.56760, saving model to keras_model/cifar10/miniVggNet/best_chkpt.hdf5
1250/1250 - 21s - loss: 0.3793 - acc: 0.9049 - val_loss: 0.5676 - val_acc: 0.8421
Epoch 31/40
Epoch 1/40

Epoch 00031: val_loss did not improve from 0.56760
1250/1250 - 21s - loss: 0.3719 - acc: 0.9072 - val_loss: 0.5917 - val_acc: 0.8390
Epoch 32/40
Epoch 1/40

Epoch 00032: val_loss did not improve from 0.56760
1250/1250 - 21s - loss: 0.3619 - acc: 0.9094 - val_loss: 0.5773 - val_acc: 0.8398
Epoch 33/40
Epoch 1/40

Epoch 00033: val_loss did not improve from 0.56760
1250/1250 - 21s - loss: 0.3602 - acc: 0.9108 - val_loss: 0.5845 - val_acc: 0.8396
Epoch 34/40
Epoch 1/40

Epoch 00034: val_loss did not improve from 0.56760
1250/1250 - 21s - loss: 0.3540 - acc: 0.9144 - val_loss: 0.5855 - val_acc: 0.8416
Epoch 35/40
Epoch 1/40

Epoch 00035: val_loss did not improve from 0.56760
1250/1250 - 20s - loss: 0.3519 - acc: 0.9137 - val_loss: 0.5747 - val_acc: 0.8477
Epoch 36/40
Epoch 1/40

Epoch 00036: val_loss did not improve from 0.56760
1250/1250 - 21s - loss: 0.3490 - acc: 0.9135 - val_loss: 0.5875 - val_acc: 0.8424
Epoch 37/40
Epoch 1/40

Epoch 00037: val_loss did not improve from 0.56760
1250/1250 - 21s - loss: 0.3399 - acc: 0.9173 - val_loss: 0.5947 - val_acc: 0.8435
Epoch 38/40
Epoch 1/40

Epoch 00038: val_loss did not improve from 0.56760
1250/1250 - 21s - loss: 0.3370 - acc: 0.9195 - val_loss: 0.5877 - val_acc: 0.8449
Epoch 39/40
Epoch 1/40

Epoch 00039: val_loss did not improve from 0.56760
1250/1250 - 21s - loss: 0.3323 - acc: 0.9202 - val_loss: 0.5829 - val_acc: 0.8416
Epoch 40/40
Epoch 1/40

Epoch 00040: val_loss did not improve from 0.56760
1250/1250 - 21s - loss: 0.3260 - acc: 0.9232 - val_loss: 0.5754 - val_acc: 0.8477


Elapsed time for Keras training (s):  839.977167


[INFO] evaluating network on Test and Validation datasets...
  64/5000 [..............................] - ETA: 0s - loss: 0.5946 - acc: 0.8594 704/5000 [===>..........................] - ETA: 0s - loss: 0.5782 - acc: 0.85651408/5000 [=======>......................] - ETA: 0s - loss: 0.5700 - acc: 0.84942112/5000 [===========>..................] - ETA: 0s - loss: 0.5493 - acc: 0.85702752/5000 [===============>..............] - ETA: 0s - loss: 0.5471 - acc: 0.85613392/5000 [===================>..........] - ETA: 0s - loss: 0.5586 - acc: 0.85204032/5000 [=======================>......] - ETA: 0s - loss: 0.5684 - acc: 0.84904672/5000 [===========================>..] - ETA: 0s - loss: 0.5777 - acc: 0.84675000/5000 [==============================] - 0s 80us/sample - loss: 0.5757 - acc: 0.8474
Validation Loss: 0.576
validation Accuracy: 0.847
  64/5000 [..............................] - ETA: 0s - loss: 0.4190 - acc: 0.8594 704/5000 [===>..........................] - ETA: 0s - loss: 0.6217 - acc: 0.81681344/5000 [=======>......................] - ETA: 0s - loss: 0.6181 - acc: 0.82291984/5000 [==========>...................] - ETA: 0s - loss: 0.6007 - acc: 0.83172624/5000 [==============>...............] - ETA: 0s - loss: 0.5864 - acc: 0.83733264/5000 [==================>...........] - ETA: 0s - loss: 0.5854 - acc: 0.83983968/5000 [======================>.......] - ETA: 0s - loss: 0.5855 - acc: 0.84154672/5000 [===========================>..] - ETA: 0s - loss: 0.5971 - acc: 0.83865000/5000 [==============================] - 0s 80us/sample - loss: 0.6020 - acc: 0.8384
Test Loss: 0.602
Test Accuracy: 0.838
              precision    recall  f1-score   support

    airplane       0.86      0.87      0.87       500
  automobile       0.90      0.92      0.91       500
        bird       0.79      0.77      0.78       500
         cat       0.72      0.67      0.70       500
        deer       0.79      0.82      0.81       500
         dog       0.77      0.76      0.76       500
        frog       0.84      0.89      0.86       500
       horse       0.90      0.88      0.89       500
        ship       0.91      0.91      0.91       500
       truck       0.89      0.89      0.89       500

    accuracy                           0.84      5000
   macro avg       0.84      0.84      0.84      5000
weighted avg       0.84      0.84      0.84      5000


TRAINING miniVggNet FINISHED

 
##########################################################################
TRAIN & EVAL miniGoogleNet on CIFAR10
##########################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2022-06-09 06:37:32.628462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 06:37:32.675932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 06:37:32.676210: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 06:37:32.677614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 06:37:32.678850: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 06:37:32.679206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 06:37:32.680846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 06:37:32.682054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 06:37:32.685084: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 06:37:32.686887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 06:37:32.687354: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 06:37:32.708607: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 06:37:32.710765: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5590d0921df0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 06:37:32.710797: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 06:37:32.791125: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5590d0d9c110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 06:37:32.791164: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 06:37:32.793898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 06:37:32.793983: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 06:37:32.794009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 06:37:32.794032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 06:37:32.794055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 06:37:32.794077: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 06:37:32.794100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 06:37:32.794124: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 06:37:32.798243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 06:37:32.798326: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 06:37:32.802304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 06:37:32.802327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 06:37:32.802338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 06:37:32.805591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22515 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 06:37:36.417324: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 06:37:36.530234: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 06:37:37.052132: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
[INFO] compiling model...
[INFO] training model...
Epoch 1/70
Epoch 1/70

Epoch 00001: val_loss improved from inf to 1.29232, saving model to keras_model/cifar10/miniGoogleNet/best_chkpt.hdf5
714/714 - 53s - loss: 1.4401 - acc: 0.4749 - val_loss: 1.2923 - val_acc: 0.5461
Epoch 2/70
Epoch 1/70

Epoch 00002: val_loss improved from 1.29232 to 1.22740, saving model to keras_model/cifar10/miniGoogleNet/best_chkpt.hdf5
714/714 - 47s - loss: 1.0216 - acc: 0.6371 - val_loss: 1.2274 - val_acc: 0.5961
Epoch 3/70
Epoch 1/70

Epoch 00003: val_loss improved from 1.22740 to 0.93973, saving model to keras_model/cifar10/miniGoogleNet/best_chkpt.hdf5
714/714 - 48s - loss: 0.8464 - acc: 0.7024 - val_loss: 0.9397 - val_acc: 0.6825
Epoch 4/70
Epoch 1/70

Epoch 00004: val_loss improved from 0.93973 to 0.69069, saving model to keras_model/cifar10/miniGoogleNet/best_chkpt.hdf5
714/714 - 48s - loss: 0.7358 - acc: 0.7426 - val_loss: 0.6907 - val_acc: 0.7594
Epoch 5/70
Epoch 1/70

Epoch 00005: val_loss improved from 0.69069 to 0.64811, saving model to keras_model/cifar10/miniGoogleNet/best_chkpt.hdf5
714/714 - 48s - loss: 0.6663 - acc: 0.7691 - val_loss: 0.6481 - val_acc: 0.7813
Epoch 6/70
Epoch 1/70

Epoch 00006: val_loss did not improve from 0.64811
714/714 - 47s - loss: 0.6060 - acc: 0.7898 - val_loss: 0.7321 - val_acc: 0.7562
Epoch 7/70
Epoch 1/70

Epoch 00007: val_loss did not improve from 0.64811
714/714 - 47s - loss: 0.5544 - acc: 0.8093 - val_loss: 0.7570 - val_acc: 0.7559
Epoch 8/70
Epoch 1/70

Epoch 00008: val_loss improved from 0.64811 to 0.63759, saving model to keras_model/cifar10/miniGoogleNet/best_chkpt.hdf5
714/714 - 48s - loss: 0.5172 - acc: 0.8222 - val_loss: 0.6376 - val_acc: 0.8026
Epoch 9/70
Epoch 1/70

Epoch 00009: val_loss did not improve from 0.63759
714/714 - 47s - loss: 0.4883 - acc: 0.8319 - val_loss: 0.6939 - val_acc: 0.7773
Epoch 10/70
Epoch 1/70

Epoch 00010: val_loss did not improve from 0.63759
714/714 - 46s - loss: 0.4591 - acc: 0.8419 - val_loss: 0.7787 - val_acc: 0.7451
Epoch 11/70
Epoch 1/70

Epoch 00011: val_loss improved from 0.63759 to 0.55919, saving model to keras_model/cifar10/miniGoogleNet/best_chkpt.hdf5
714/714 - 47s - loss: 0.4312 - acc: 0.8521 - val_loss: 0.5592 - val_acc: 0.8157
Epoch 12/70
Epoch 1/70

Epoch 00012: val_loss improved from 0.55919 to 0.48274, saving model to keras_model/cifar10/miniGoogleNet/best_chkpt.hdf5
714/714 - 47s - loss: 0.4107 - acc: 0.8598 - val_loss: 0.4827 - val_acc: 0.8428
Epoch 13/70
Epoch 1/70

Epoch 00013: val_loss did not improve from 0.48274
714/714 - 46s - loss: 0.3895 - acc: 0.8665 - val_loss: 0.5109 - val_acc: 0.8282
Epoch 14/70
Epoch 1/70

Epoch 00014: val_loss did not improve from 0.48274
714/714 - 47s - loss: 0.3735 - acc: 0.8722 - val_loss: 0.7203 - val_acc: 0.7841
Epoch 15/70
Epoch 1/70

Epoch 00015: val_loss improved from 0.48274 to 0.44376, saving model to keras_model/cifar10/miniGoogleNet/best_chkpt.hdf5
714/714 - 48s - loss: 0.3547 - acc: 0.8794 - val_loss: 0.4438 - val_acc: 0.8506
Epoch 16/70
Epoch 1/70

Epoch 00016: val_loss did not improve from 0.44376
714/714 - 46s - loss: 0.3401 - acc: 0.8826 - val_loss: 0.6420 - val_acc: 0.8049
Epoch 17/70
Epoch 1/70

Epoch 00017: val_loss did not improve from 0.44376
714/714 - 47s - loss: 0.3278 - acc: 0.8870 - val_loss: 0.5315 - val_acc: 0.8408
Epoch 18/70
Epoch 1/70

Epoch 00018: val_loss did not improve from 0.44376
714/714 - 47s - loss: 0.3160 - acc: 0.8919 - val_loss: 0.5773 - val_acc: 0.8208
Epoch 19/70
Epoch 1/70

Epoch 00019: val_loss did not improve from 0.44376
714/714 - 47s - loss: 0.3014 - acc: 0.8965 - val_loss: 0.4847 - val_acc: 0.8523
Epoch 20/70
Epoch 1/70

Epoch 00020: val_loss improved from 0.44376 to 0.41484, saving model to keras_model/cifar10/miniGoogleNet/best_chkpt.hdf5
714/714 - 48s - loss: 0.2904 - acc: 0.9003 - val_loss: 0.4148 - val_acc: 0.8659
Epoch 21/70
Epoch 1/70

Epoch 00021: val_loss did not improve from 0.41484
714/714 - 47s - loss: 0.2821 - acc: 0.9023 - val_loss: 0.5308 - val_acc: 0.8349
Epoch 22/70
Epoch 1/70

Epoch 00022: val_loss did not improve from 0.41484
714/714 - 46s - loss: 0.2730 - acc: 0.9064 - val_loss: 0.4193 - val_acc: 0.8700
Epoch 23/70
Epoch 1/70

Epoch 00023: val_loss did not improve from 0.41484
714/714 - 47s - loss: 0.2624 - acc: 0.9094 - val_loss: 0.5116 - val_acc: 0.8462
Epoch 24/70
Epoch 1/70

Epoch 00024: val_loss did not improve from 0.41484
714/714 - 46s - loss: 0.2547 - acc: 0.9121 - val_loss: 0.4581 - val_acc: 0.8472
Epoch 25/70
Epoch 1/70

Epoch 00025: val_loss did not improve from 0.41484
714/714 - 47s - loss: 0.2453 - acc: 0.9152 - val_loss: 0.5353 - val_acc: 0.8446
Epoch 26/70
Epoch 1/70

Epoch 00026: val_loss did not improve from 0.41484
714/714 - 47s - loss: 0.2353 - acc: 0.9188 - val_loss: 0.4280 - val_acc: 0.8645
Epoch 27/70
Epoch 1/70

Epoch 00027: val_loss did not improve from 0.41484
714/714 - 46s - loss: 0.2324 - acc: 0.9201 - val_loss: 0.4762 - val_acc: 0.8562
Epoch 28/70
Epoch 1/70

Epoch 00028: val_loss did not improve from 0.41484
714/714 - 47s - loss: 0.2238 - acc: 0.9233 - val_loss: 0.4285 - val_acc: 0.8621
Epoch 29/70
Epoch 1/70

Epoch 00029: val_loss did not improve from 0.41484
714/714 - 46s - loss: 0.2194 - acc: 0.9241 - val_loss: 0.5569 - val_acc: 0.8452
Epoch 30/70
Epoch 1/70

Epoch 00030: val_loss did not improve from 0.41484
714/714 - 47s - loss: 0.2097 - acc: 0.9273 - val_loss: 0.5001 - val_acc: 0.8544
Epoch 31/70
Epoch 1/70

Epoch 00031: val_loss did not improve from 0.41484
714/714 - 47s - loss: 0.2033 - acc: 0.9293 - val_loss: 0.4306 - val_acc: 0.8723
Epoch 32/70
Epoch 1/70

Epoch 00032: val_loss improved from 0.41484 to 0.36868, saving model to keras_model/cifar10/miniGoogleNet/best_chkpt.hdf5
714/714 - 47s - loss: 0.1971 - acc: 0.9314 - val_loss: 0.3687 - val_acc: 0.8862
Epoch 33/70
Epoch 1/70

Epoch 00033: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.1947 - acc: 0.9335 - val_loss: 0.4095 - val_acc: 0.8746
Epoch 34/70
Epoch 1/70

Epoch 00034: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.1879 - acc: 0.9349 - val_loss: 0.5233 - val_acc: 0.8534
Epoch 35/70
Epoch 1/70

Epoch 00035: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.1797 - acc: 0.9374 - val_loss: 0.4489 - val_acc: 0.8649
Epoch 36/70
Epoch 1/70

Epoch 00036: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1801 - acc: 0.9373 - val_loss: 0.4095 - val_acc: 0.8732
Epoch 37/70
Epoch 1/70

Epoch 00037: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.1731 - acc: 0.9398 - val_loss: 0.4430 - val_acc: 0.8739
Epoch 38/70
Epoch 1/70

Epoch 00038: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1667 - acc: 0.9426 - val_loss: 0.4903 - val_acc: 0.8606
Epoch 39/70
Epoch 1/70

Epoch 00039: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1649 - acc: 0.9431 - val_loss: 0.4188 - val_acc: 0.8742
Epoch 40/70
Epoch 1/70

Epoch 00040: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1596 - acc: 0.9448 - val_loss: 0.5483 - val_acc: 0.8560
Epoch 41/70
Epoch 1/70

Epoch 00041: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1566 - acc: 0.9458 - val_loss: 0.3721 - val_acc: 0.8816
Epoch 42/70
Epoch 1/70

Epoch 00042: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.1529 - acc: 0.9479 - val_loss: 0.4433 - val_acc: 0.8777
Epoch 43/70
Epoch 1/70

Epoch 00043: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1491 - acc: 0.9476 - val_loss: 0.4027 - val_acc: 0.8771
Epoch 44/70
Epoch 1/70

Epoch 00044: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1443 - acc: 0.9497 - val_loss: 0.3946 - val_acc: 0.8827
Epoch 45/70
Epoch 1/70

Epoch 00045: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1413 - acc: 0.9516 - val_loss: 0.4870 - val_acc: 0.8683
Epoch 46/70
Epoch 1/70

Epoch 00046: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1389 - acc: 0.9520 - val_loss: 0.3789 - val_acc: 0.8869
Epoch 47/70
Epoch 1/70

Epoch 00047: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.1350 - acc: 0.9526 - val_loss: 0.4592 - val_acc: 0.8740
Epoch 48/70
Epoch 1/70

Epoch 00048: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1325 - acc: 0.9537 - val_loss: 0.4469 - val_acc: 0.8746
Epoch 49/70
Epoch 1/70

Epoch 00049: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1309 - acc: 0.9549 - val_loss: 0.4396 - val_acc: 0.8763
Epoch 50/70
Epoch 1/70

Epoch 00050: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.1292 - acc: 0.9560 - val_loss: 0.4246 - val_acc: 0.8825
Epoch 51/70
Epoch 1/70

Epoch 00051: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1232 - acc: 0.9575 - val_loss: 0.5231 - val_acc: 0.8689
Epoch 52/70
Epoch 1/70

Epoch 00052: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.1206 - acc: 0.9584 - val_loss: 0.3987 - val_acc: 0.8825
Epoch 53/70
Epoch 1/70

Epoch 00053: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1180 - acc: 0.9596 - val_loss: 0.4768 - val_acc: 0.8766
Epoch 54/70
Epoch 1/70

Epoch 00054: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.1209 - acc: 0.9584 - val_loss: 0.4258 - val_acc: 0.8863
Epoch 55/70
Epoch 1/70

Epoch 00055: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1150 - acc: 0.9600 - val_loss: 0.4851 - val_acc: 0.8689
Epoch 56/70
Epoch 1/70

Epoch 00056: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1117 - acc: 0.9612 - val_loss: 0.4465 - val_acc: 0.8847
Epoch 57/70
Epoch 1/70

Epoch 00057: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.1109 - acc: 0.9616 - val_loss: 0.4551 - val_acc: 0.8743
Epoch 58/70
Epoch 1/70

Epoch 00058: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1098 - acc: 0.9624 - val_loss: 0.5043 - val_acc: 0.8676
Epoch 59/70
Epoch 1/70

Epoch 00059: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1067 - acc: 0.9634 - val_loss: 0.4420 - val_acc: 0.8800
Epoch 60/70
Epoch 1/70

Epoch 00060: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.1059 - acc: 0.9631 - val_loss: 0.4671 - val_acc: 0.8790
Epoch 61/70
Epoch 1/70

Epoch 00061: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.1021 - acc: 0.9650 - val_loss: 0.4803 - val_acc: 0.8783
Epoch 62/70
Epoch 1/70

Epoch 00062: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.0997 - acc: 0.9660 - val_loss: 0.4018 - val_acc: 0.8913
Epoch 63/70
Epoch 1/70

Epoch 00063: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.0995 - acc: 0.9667 - val_loss: 0.4876 - val_acc: 0.8746
Epoch 64/70
Epoch 1/70

Epoch 00064: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.0968 - acc: 0.9672 - val_loss: 0.5087 - val_acc: 0.8717
Epoch 65/70
Epoch 1/70

Epoch 00065: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.0961 - acc: 0.9670 - val_loss: 0.4179 - val_acc: 0.8833
Epoch 66/70
Epoch 1/70

Epoch 00066: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.0939 - acc: 0.9681 - val_loss: 0.5288 - val_acc: 0.8743
Epoch 67/70
Epoch 1/70

Epoch 00067: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.0920 - acc: 0.9690 - val_loss: 0.4865 - val_acc: 0.8750
Epoch 68/70
Epoch 1/70

Epoch 00068: val_loss did not improve from 0.36868
714/714 - 47s - loss: 0.0909 - acc: 0.9693 - val_loss: 0.4550 - val_acc: 0.8800
Epoch 69/70
Epoch 1/70

Epoch 00069: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.0894 - acc: 0.9696 - val_loss: 0.5781 - val_acc: 0.8642
Epoch 70/70
Epoch 1/70

Epoch 00070: val_loss did not improve from 0.36868
714/714 - 46s - loss: 0.0881 - acc: 0.9695 - val_loss: 0.5160 - val_acc: 0.8712


Elapsed time for Keras training (s):  3280.461596


[INFO] evaluating network on Test and Validation datasets...
 128/5000 [..............................] - ETA: 0s - loss: 0.5507 - acc: 0.8750 512/5000 [==>...........................] - ETA: 0s - loss: 0.5137 - acc: 0.8887 896/5000 [====>.........................] - ETA: 0s - loss: 0.5254 - acc: 0.87611280/5000 [======>.......................] - ETA: 0s - loss: 0.5363 - acc: 0.87031664/5000 [========>.....................] - ETA: 0s - loss: 0.5166 - acc: 0.87322048/5000 [===========>..................] - ETA: 0s - loss: 0.5034 - acc: 0.87652432/5000 [=============>................] - ETA: 0s - loss: 0.5074 - acc: 0.87542816/5000 [===============>..............] - ETA: 0s - loss: 0.4962 - acc: 0.87463200/5000 [==================>...........] - ETA: 0s - loss: 0.4967 - acc: 0.87633584/5000 [====================>.........] - ETA: 0s - loss: 0.4906 - acc: 0.87753968/5000 [======================>.......] - ETA: 0s - loss: 0.5132 - acc: 0.87474352/5000 [=========================>....] - ETA: 0s - loss: 0.5196 - acc: 0.87204736/5000 [===========================>..] - ETA: 0s - loss: 0.5190 - acc: 0.87205000/5000 [==============================] - 1s 142us/sample - loss: 0.5200 - acc: 0.8724
Validation Loss: 0.520
validation Accuracy: 0.872
 128/5000 [..............................] - ETA: 0s - loss: 0.4113 - acc: 0.8828 512/5000 [==>...........................] - ETA: 0s - loss: 0.5844 - acc: 0.8496 896/5000 [====>.........................] - ETA: 0s - loss: 0.6301 - acc: 0.85271280/5000 [======>.......................] - ETA: 0s - loss: 0.5987 - acc: 0.85701664/5000 [========>.....................] - ETA: 0s - loss: 0.5565 - acc: 0.86422048/5000 [===========>..................] - ETA: 0s - loss: 0.5189 - acc: 0.86912432/5000 [=============>................] - ETA: 0s - loss: 0.5057 - acc: 0.87212816/5000 [===============>..............] - ETA: 0s - loss: 0.5018 - acc: 0.87543200/5000 [==================>...........] - ETA: 0s - loss: 0.5039 - acc: 0.87633584/5000 [====================>.........] - ETA: 0s - loss: 0.5022 - acc: 0.87613968/5000 [======================>.......] - ETA: 0s - loss: 0.4977 - acc: 0.87654352/5000 [=========================>....] - ETA: 0s - loss: 0.5043 - acc: 0.87414736/5000 [===========================>..] - ETA: 0s - loss: 0.5059 - acc: 0.87545000/5000 [==============================] - 1s 142us/sample - loss: 0.5237 - acc: 0.8738
Test Loss: 0.524
Test Accuracy: 0.874
              precision    recall  f1-score   support

    airplane       0.93      0.87      0.90       500
  automobile       0.97      0.93      0.95       500
        bird       0.84      0.81      0.83       500
         cat       0.76      0.81      0.79       500
        deer       0.92      0.81      0.86       500
         dog       0.95      0.70      0.81       500
        frog       0.73      0.98      0.84       500
       horse       0.92      0.93      0.92       500
        ship       0.93      0.95      0.94       500
       truck       0.88      0.96      0.92       500

    accuracy                           0.87      5000
   macro avg       0.88      0.87      0.87      5000
weighted avg       0.88      0.87      0.87      5000


TRAINING miniGoogleNet FINISHED

 
##########################################################################
TRAIN & EVAL miniResNet  on CIFAR10
##########################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2022-06-09 07:33:11.130066: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 07:33:11.176040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 07:33:11.176302: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 07:33:11.177629: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 07:33:11.178845: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 07:33:11.179194: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 07:33:11.180852: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 07:33:11.182095: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 07:33:11.185539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 07:33:11.187315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 07:33:11.187738: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 07:33:11.209650: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 07:33:11.212207: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5635e9d190c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 07:33:11.212240: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 07:33:11.311489: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5635ea25fb90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 07:33:11.311531: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 07:33:11.314030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 07:33:11.314150: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 07:33:11.314197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 07:33:11.314234: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 07:33:11.314278: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 07:33:11.314316: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 07:33:11.314371: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 07:33:11.314421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 07:33:11.318723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 07:33:11.318821: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 07:33:11.322867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 07:33:11.322892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 07:33:11.322902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 07:33:11.326448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22522 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 07:33:26.181043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 07:33:26.285151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 07:33:26.822417: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
[INFO] compiling model...
[INFO] training model...
Epoch 1/100
Epoch 1/100

Epoch 00001: val_loss improved from inf to 1.83148, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 128s - loss: 2.1092 - acc: 0.4395 - val_loss: 1.8315 - val_acc: 0.5482
Epoch 2/100
Epoch 1/100

Epoch 00002: val_loss improved from 1.83148 to 1.33581, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 80s - loss: 1.5341 - acc: 0.6256 - val_loss: 1.3358 - val_acc: 0.6998
Epoch 3/100
Epoch 1/100

Epoch 00003: val_loss did not improve from 1.33581
500/500 - 80s - loss: 1.2552 - acc: 0.7127 - val_loss: 1.3659 - val_acc: 0.6850
Epoch 4/100
Epoch 1/100

Epoch 00004: val_loss improved from 1.33581 to 1.12491, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 1.1087 - acc: 0.7540 - val_loss: 1.1249 - val_acc: 0.7441
Epoch 5/100
Epoch 1/100

Epoch 00005: val_loss did not improve from 1.12491
500/500 - 79s - loss: 1.0052 - acc: 0.7810 - val_loss: 1.2539 - val_acc: 0.7018
Epoch 6/100
Epoch 1/100

Epoch 00006: val_loss improved from 1.12491 to 1.04782, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 80s - loss: 0.9395 - acc: 0.7981 - val_loss: 1.0478 - val_acc: 0.7691
Epoch 7/100
Epoch 1/100

Epoch 00007: val_loss improved from 1.04782 to 0.84692, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.8858 - acc: 0.8110 - val_loss: 0.8469 - val_acc: 0.8239
Epoch 8/100
Epoch 1/100

Epoch 00008: val_loss did not improve from 0.84692
500/500 - 79s - loss: 0.8448 - acc: 0.8208 - val_loss: 0.9870 - val_acc: 0.7779
Epoch 9/100
Epoch 1/100

Epoch 00009: val_loss did not improve from 0.84692
500/500 - 79s - loss: 0.8047 - acc: 0.8305 - val_loss: 0.8919 - val_acc: 0.8104
Epoch 10/100
Epoch 1/100

Epoch 00010: val_loss did not improve from 0.84692
500/500 - 80s - loss: 0.7829 - acc: 0.8349 - val_loss: 0.8547 - val_acc: 0.8119
Epoch 11/100
Epoch 1/100

Epoch 00011: val_loss did not improve from 0.84692
500/500 - 80s - loss: 0.7591 - acc: 0.8407 - val_loss: 0.9209 - val_acc: 0.7914
Epoch 12/100
Epoch 1/100

Epoch 00012: val_loss improved from 0.84692 to 0.77264, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 80s - loss: 0.7326 - acc: 0.8493 - val_loss: 0.7726 - val_acc: 0.8354
Epoch 13/100
Epoch 1/100

Epoch 00013: val_loss did not improve from 0.77264
500/500 - 79s - loss: 0.7184 - acc: 0.8520 - val_loss: 0.8710 - val_acc: 0.7941
Epoch 14/100
Epoch 1/100

Epoch 00014: val_loss did not improve from 0.77264
500/500 - 79s - loss: 0.7054 - acc: 0.8550 - val_loss: 0.8034 - val_acc: 0.8317
Epoch 15/100
Epoch 1/100

Epoch 00015: val_loss did not improve from 0.77264
500/500 - 79s - loss: 0.6875 - acc: 0.8611 - val_loss: 0.8924 - val_acc: 0.8140
Epoch 16/100
Epoch 1/100

Epoch 00016: val_loss improved from 0.77264 to 0.71888, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.6727 - acc: 0.8663 - val_loss: 0.7189 - val_acc: 0.8596
Epoch 17/100
Epoch 1/100

Epoch 00017: val_loss did not improve from 0.71888
500/500 - 79s - loss: 0.6595 - acc: 0.8674 - val_loss: 0.8416 - val_acc: 0.8105
Epoch 18/100
Epoch 1/100

Epoch 00018: val_loss did not improve from 0.71888
500/500 - 79s - loss: 0.6507 - acc: 0.8705 - val_loss: 0.9722 - val_acc: 0.7847
Epoch 19/100
Epoch 1/100

Epoch 00019: val_loss did not improve from 0.71888
500/500 - 80s - loss: 0.6357 - acc: 0.8742 - val_loss: 0.7989 - val_acc: 0.8293
Epoch 20/100
Epoch 1/100

Epoch 00020: val_loss did not improve from 0.71888
500/500 - 79s - loss: 0.6404 - acc: 0.8730 - val_loss: 0.7308 - val_acc: 0.8568
Epoch 21/100
Epoch 1/100

Epoch 00021: val_loss improved from 0.71888 to 0.66635, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 83s - loss: 0.6230 - acc: 0.8790 - val_loss: 0.6663 - val_acc: 0.8701
Epoch 22/100
Epoch 1/100

Epoch 00022: val_loss did not improve from 0.66635
500/500 - 79s - loss: 0.6189 - acc: 0.8799 - val_loss: 0.6728 - val_acc: 0.8591
Epoch 23/100
Epoch 1/100

Epoch 00023: val_loss did not improve from 0.66635
500/500 - 80s - loss: 0.6054 - acc: 0.8836 - val_loss: 0.6968 - val_acc: 0.8604
Epoch 24/100
Epoch 1/100

Epoch 00024: val_loss improved from 0.66635 to 0.65870, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 80s - loss: 0.6059 - acc: 0.8837 - val_loss: 0.6587 - val_acc: 0.8667
Epoch 25/100
Epoch 1/100

Epoch 00025: val_loss did not improve from 0.65870
500/500 - 79s - loss: 0.5910 - acc: 0.8879 - val_loss: 0.7296 - val_acc: 0.8455
Epoch 26/100
Epoch 1/100

Epoch 00026: val_loss did not improve from 0.65870
500/500 - 80s - loss: 0.5923 - acc: 0.8870 - val_loss: 0.8316 - val_acc: 0.8282
Epoch 27/100
Epoch 1/100

Epoch 00027: val_loss did not improve from 0.65870
500/500 - 79s - loss: 0.5808 - acc: 0.8911 - val_loss: 0.6716 - val_acc: 0.8658
Epoch 28/100
Epoch 1/100

Epoch 00028: val_loss did not improve from 0.65870
500/500 - 79s - loss: 0.5786 - acc: 0.8933 - val_loss: 0.7215 - val_acc: 0.8545
Epoch 29/100
Epoch 1/100

Epoch 00029: val_loss improved from 0.65870 to 0.65782, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.5728 - acc: 0.8932 - val_loss: 0.6578 - val_acc: 0.8763
Epoch 30/100
Epoch 1/100

Epoch 00030: val_loss did not improve from 0.65782
500/500 - 80s - loss: 0.5609 - acc: 0.8980 - val_loss: 0.7588 - val_acc: 0.8486
Epoch 31/100
Epoch 1/100

Epoch 00031: val_loss did not improve from 0.65782
500/500 - 79s - loss: 0.5582 - acc: 0.8985 - val_loss: 0.7270 - val_acc: 0.8616
Epoch 32/100
Epoch 1/100

Epoch 00032: val_loss did not improve from 0.65782
500/500 - 78s - loss: 0.5597 - acc: 0.8985 - val_loss: 0.7505 - val_acc: 0.8447
Epoch 33/100
Epoch 1/100

Epoch 00033: val_loss did not improve from 0.65782
500/500 - 78s - loss: 0.5470 - acc: 0.9015 - val_loss: 0.6923 - val_acc: 0.8615
Epoch 34/100
Epoch 1/100

Epoch 00034: val_loss did not improve from 0.65782
500/500 - 79s - loss: 0.5401 - acc: 0.9040 - val_loss: 0.7011 - val_acc: 0.8645
Epoch 35/100
Epoch 1/100

Epoch 00035: val_loss did not improve from 0.65782
500/500 - 81s - loss: 0.5448 - acc: 0.9025 - val_loss: 0.7640 - val_acc: 0.8401
Epoch 36/100
Epoch 1/100

Epoch 00036: val_loss improved from 0.65782 to 0.62804, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 80s - loss: 0.5363 - acc: 0.9059 - val_loss: 0.6280 - val_acc: 0.8761
Epoch 37/100
Epoch 1/100

Epoch 00037: val_loss did not improve from 0.62804
500/500 - 79s - loss: 0.5313 - acc: 0.9065 - val_loss: 0.6396 - val_acc: 0.8742
Epoch 38/100
Epoch 1/100

Epoch 00038: val_loss did not improve from 0.62804
500/500 - 79s - loss: 0.5252 - acc: 0.9087 - val_loss: 0.7289 - val_acc: 0.8537
Epoch 39/100
Epoch 1/100

Epoch 00039: val_loss did not improve from 0.62804
500/500 - 78s - loss: 0.5280 - acc: 0.9070 - val_loss: 0.6333 - val_acc: 0.8809
Epoch 40/100
Epoch 1/100

Epoch 00040: val_loss improved from 0.62804 to 0.59400, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.5141 - acc: 0.9112 - val_loss: 0.5940 - val_acc: 0.8863
Epoch 41/100
Epoch 1/100

Epoch 00041: val_loss did not improve from 0.59400
500/500 - 80s - loss: 0.5101 - acc: 0.9129 - val_loss: 0.6720 - val_acc: 0.8664
Epoch 42/100
Epoch 1/100

Epoch 00042: val_loss did not improve from 0.59400
500/500 - 79s - loss: 0.5084 - acc: 0.9128 - val_loss: 0.6209 - val_acc: 0.8783
Epoch 43/100
Epoch 1/100

Epoch 00043: val_loss did not improve from 0.59400
500/500 - 79s - loss: 0.5087 - acc: 0.9124 - val_loss: 0.6047 - val_acc: 0.8890
Epoch 44/100
Epoch 1/100

Epoch 00044: val_loss did not improve from 0.59400
500/500 - 79s - loss: 0.4913 - acc: 0.9191 - val_loss: 0.6070 - val_acc: 0.8838
Epoch 45/100
Epoch 1/100

Epoch 00045: val_loss did not improve from 0.59400
500/500 - 78s - loss: 0.4927 - acc: 0.9169 - val_loss: 0.6173 - val_acc: 0.8863
Epoch 46/100
Epoch 1/100

Epoch 00046: val_loss did not improve from 0.59400
500/500 - 79s - loss: 0.4913 - acc: 0.9165 - val_loss: 0.7120 - val_acc: 0.8653
Epoch 47/100
Epoch 1/100

Epoch 00047: val_loss did not improve from 0.59400
500/500 - 78s - loss: 0.4794 - acc: 0.9206 - val_loss: 0.6151 - val_acc: 0.8847
Epoch 48/100
Epoch 1/100

Epoch 00048: val_loss improved from 0.59400 to 0.59318, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.4727 - acc: 0.9229 - val_loss: 0.5932 - val_acc: 0.8935
Epoch 49/100
Epoch 1/100

Epoch 00049: val_loss improved from 0.59318 to 0.56857, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 79s - loss: 0.4684 - acc: 0.9239 - val_loss: 0.5686 - val_acc: 0.8911
Epoch 50/100
Epoch 1/100

Epoch 00050: val_loss did not improve from 0.56857
500/500 - 79s - loss: 0.4679 - acc: 0.9244 - val_loss: 0.6102 - val_acc: 0.8804
Epoch 51/100
Epoch 1/100

Epoch 00051: val_loss improved from 0.56857 to 0.55886, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.4603 - acc: 0.9260 - val_loss: 0.5589 - val_acc: 0.9002
Epoch 52/100
Epoch 1/100

Epoch 00052: val_loss did not improve from 0.55886
500/500 - 81s - loss: 0.4518 - acc: 0.9275 - val_loss: 0.6406 - val_acc: 0.8809
Epoch 53/100
Epoch 1/100

Epoch 00053: val_loss did not improve from 0.55886
500/500 - 81s - loss: 0.4518 - acc: 0.9293 - val_loss: 0.6122 - val_acc: 0.8861
Epoch 54/100
Epoch 1/100

Epoch 00054: val_loss did not improve from 0.55886
500/500 - 82s - loss: 0.4474 - acc: 0.9288 - val_loss: 0.5760 - val_acc: 0.8860
Epoch 55/100
Epoch 1/100

Epoch 00055: val_loss did not improve from 0.55886
500/500 - 81s - loss: 0.4434 - acc: 0.9299 - val_loss: 0.6177 - val_acc: 0.8814
Epoch 56/100
Epoch 1/100

Epoch 00056: val_loss did not improve from 0.55886
500/500 - 81s - loss: 0.4377 - acc: 0.9315 - val_loss: 0.5763 - val_acc: 0.8876
Epoch 57/100
Epoch 1/100

Epoch 00057: val_loss did not improve from 0.55886
500/500 - 80s - loss: 0.4314 - acc: 0.9332 - val_loss: 0.6165 - val_acc: 0.8817
Epoch 58/100
Epoch 1/100

Epoch 00058: val_loss did not improve from 0.55886
500/500 - 83s - loss: 0.4229 - acc: 0.9355 - val_loss: 0.5658 - val_acc: 0.8981
Epoch 59/100
Epoch 1/100

Epoch 00059: val_loss improved from 0.55886 to 0.52625, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.4241 - acc: 0.9347 - val_loss: 0.5262 - val_acc: 0.9054
Epoch 60/100
Epoch 1/100

Epoch 00060: val_loss did not improve from 0.52625
500/500 - 80s - loss: 0.4132 - acc: 0.9393 - val_loss: 0.5438 - val_acc: 0.9005
Epoch 61/100
Epoch 1/100

Epoch 00061: val_loss improved from 0.52625 to 0.50442, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.4106 - acc: 0.9380 - val_loss: 0.5044 - val_acc: 0.9089
Epoch 62/100
Epoch 1/100

Epoch 00062: val_loss did not improve from 0.50442
500/500 - 80s - loss: 0.4012 - acc: 0.9412 - val_loss: 0.5271 - val_acc: 0.9043
Epoch 63/100
Epoch 1/100

Epoch 00063: val_loss did not improve from 0.50442
500/500 - 80s - loss: 0.3979 - acc: 0.9409 - val_loss: 0.5942 - val_acc: 0.8901
Epoch 64/100
Epoch 1/100

Epoch 00064: val_loss did not improve from 0.50442
500/500 - 80s - loss: 0.3913 - acc: 0.9430 - val_loss: 0.5467 - val_acc: 0.9002
Epoch 65/100
Epoch 1/100

Epoch 00065: val_loss did not improve from 0.50442
500/500 - 80s - loss: 0.3876 - acc: 0.9441 - val_loss: 0.5639 - val_acc: 0.8959
Epoch 66/100
Epoch 1/100

Epoch 00066: val_loss did not improve from 0.50442
500/500 - 80s - loss: 0.3786 - acc: 0.9470 - val_loss: 0.5737 - val_acc: 0.8916
Epoch 67/100
Epoch 1/100

Epoch 00067: val_loss did not improve from 0.50442
500/500 - 80s - loss: 0.3713 - acc: 0.9476 - val_loss: 0.5335 - val_acc: 0.9072
Epoch 68/100
Epoch 1/100

Epoch 00068: val_loss did not improve from 0.50442
500/500 - 79s - loss: 0.3750 - acc: 0.9467 - val_loss: 0.5655 - val_acc: 0.8979
Epoch 69/100
Epoch 1/100

Epoch 00069: val_loss did not improve from 0.50442
500/500 - 80s - loss: 0.3610 - acc: 0.9502 - val_loss: 0.5285 - val_acc: 0.9061
Epoch 70/100
Epoch 1/100

Epoch 00070: val_loss did not improve from 0.50442
500/500 - 80s - loss: 0.3592 - acc: 0.9495 - val_loss: 0.5058 - val_acc: 0.9102
Epoch 71/100
Epoch 1/100

Epoch 00071: val_loss did not improve from 0.50442
500/500 - 79s - loss: 0.3521 - acc: 0.9527 - val_loss: 0.5615 - val_acc: 0.8959
Epoch 72/100
Epoch 1/100

Epoch 00072: val_loss improved from 0.50442 to 0.49195, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.3524 - acc: 0.9512 - val_loss: 0.4920 - val_acc: 0.9134
Epoch 73/100
Epoch 1/100

Epoch 00073: val_loss improved from 0.49195 to 0.49121, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.3347 - acc: 0.9566 - val_loss: 0.4912 - val_acc: 0.9135
Epoch 74/100
Epoch 1/100

Epoch 00074: val_loss did not improve from 0.49121
500/500 - 80s - loss: 0.3369 - acc: 0.9552 - val_loss: 0.5047 - val_acc: 0.9094
Epoch 75/100
Epoch 1/100

Epoch 00075: val_loss did not improve from 0.49121
500/500 - 80s - loss: 0.3271 - acc: 0.9575 - val_loss: 0.4984 - val_acc: 0.9045
Epoch 76/100
Epoch 1/100

Epoch 00076: val_loss did not improve from 0.49121
500/500 - 80s - loss: 0.3167 - acc: 0.9609 - val_loss: 0.4945 - val_acc: 0.9137
Epoch 77/100
Epoch 1/100

Epoch 00077: val_loss improved from 0.49121 to 0.46753, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 80s - loss: 0.3138 - acc: 0.9608 - val_loss: 0.4675 - val_acc: 0.9167
Epoch 78/100
Epoch 1/100

Epoch 00078: val_loss did not improve from 0.46753
500/500 - 80s - loss: 0.3070 - acc: 0.9630 - val_loss: 0.4823 - val_acc: 0.9188
Epoch 79/100
Epoch 1/100

Epoch 00079: val_loss did not improve from 0.46753
500/500 - 80s - loss: 0.3006 - acc: 0.9641 - val_loss: 0.4944 - val_acc: 0.9086
Epoch 80/100
Epoch 1/100

Epoch 00080: val_loss improved from 0.46753 to 0.46461, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.2908 - acc: 0.9669 - val_loss: 0.4646 - val_acc: 0.9210
Epoch 81/100
Epoch 1/100

Epoch 00081: val_loss did not improve from 0.46461
500/500 - 80s - loss: 0.2824 - acc: 0.9689 - val_loss: 0.5210 - val_acc: 0.9041
Epoch 82/100
Epoch 1/100

Epoch 00082: val_loss improved from 0.46461 to 0.41719, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 82s - loss: 0.2818 - acc: 0.9686 - val_loss: 0.4172 - val_acc: 0.9287
Epoch 83/100
Epoch 1/100

Epoch 00083: val_loss did not improve from 0.41719
500/500 - 80s - loss: 0.2678 - acc: 0.9725 - val_loss: 0.4534 - val_acc: 0.9218
Epoch 84/100
Epoch 1/100

Epoch 00084: val_loss did not improve from 0.41719
500/500 - 80s - loss: 0.2643 - acc: 0.9726 - val_loss: 0.4350 - val_acc: 0.9247
Epoch 85/100
Epoch 1/100

Epoch 00085: val_loss did not improve from 0.41719
500/500 - 80s - loss: 0.2575 - acc: 0.9743 - val_loss: 0.4212 - val_acc: 0.9279
Epoch 86/100
Epoch 1/100

Epoch 00086: val_loss improved from 0.41719 to 0.41679, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.2477 - acc: 0.9778 - val_loss: 0.4168 - val_acc: 0.9275
Epoch 87/100
Epoch 1/100

Epoch 00087: val_loss did not improve from 0.41679
500/500 - 80s - loss: 0.2409 - acc: 0.9789 - val_loss: 0.4324 - val_acc: 0.9252
Epoch 88/100
Epoch 1/100

Epoch 00088: val_loss improved from 0.41679 to 0.39685, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.2342 - acc: 0.9795 - val_loss: 0.3969 - val_acc: 0.9331
Epoch 89/100
Epoch 1/100

Epoch 00089: val_loss did not improve from 0.39685
500/500 - 80s - loss: 0.2257 - acc: 0.9820 - val_loss: 0.4172 - val_acc: 0.9291
Epoch 90/100
Epoch 1/100

Epoch 00090: val_loss did not improve from 0.39685
500/500 - 81s - loss: 0.2174 - acc: 0.9845 - val_loss: 0.4200 - val_acc: 0.9272
Epoch 91/100
Epoch 1/100

Epoch 00091: val_loss did not improve from 0.39685
500/500 - 79s - loss: 0.2081 - acc: 0.9870 - val_loss: 0.4449 - val_acc: 0.9231
Epoch 92/100
Epoch 1/100

Epoch 00092: val_loss did not improve from 0.39685
500/500 - 80s - loss: 0.2042 - acc: 0.9872 - val_loss: 0.4086 - val_acc: 0.9334
Epoch 93/100
Epoch 1/100

Epoch 00093: val_loss did not improve from 0.39685
500/500 - 80s - loss: 0.1976 - acc: 0.9886 - val_loss: 0.4449 - val_acc: 0.9320
Epoch 94/100
Epoch 1/100

Epoch 00094: val_loss did not improve from 0.39685
500/500 - 80s - loss: 0.1950 - acc: 0.9890 - val_loss: 0.3988 - val_acc: 0.9326
Epoch 95/100
Epoch 1/100

Epoch 00095: val_loss did not improve from 0.39685
500/500 - 80s - loss: 0.1878 - acc: 0.9917 - val_loss: 0.4107 - val_acc: 0.9376
Epoch 96/100
Epoch 1/100

Epoch 00096: val_loss improved from 0.39685 to 0.38616, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.1830 - acc: 0.9925 - val_loss: 0.3862 - val_acc: 0.9384
Epoch 97/100
Epoch 1/100

Epoch 00097: val_loss did not improve from 0.38616
500/500 - 80s - loss: 0.1769 - acc: 0.9940 - val_loss: 0.3870 - val_acc: 0.9398
Epoch 98/100
Epoch 1/100

Epoch 00098: val_loss improved from 0.38616 to 0.36759, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 82s - loss: 0.1750 - acc: 0.9941 - val_loss: 0.3676 - val_acc: 0.9439
Epoch 99/100
Epoch 1/100

Epoch 00099: val_loss did not improve from 0.36759
500/500 - 80s - loss: 0.1727 - acc: 0.9951 - val_loss: 0.3750 - val_acc: 0.9412
Epoch 100/100
Epoch 1/100

Epoch 00100: val_loss improved from 0.36759 to 0.36198, saving model to keras_model/cifar10/miniResNet/best_chkpt.hdf5
500/500 - 81s - loss: 0.1720 - acc: 0.9950 - val_loss: 0.3620 - val_acc: 0.9454


Elapsed time for Keras training (s):  8049.064174


[INFO] evaluating network on Test and Validation datasets...
 128/5000 [..............................] - ETA: 1s - loss: 0.3148 - acc: 0.9531 384/5000 [=>............................] - ETA: 1s - loss: 0.3522 - acc: 0.9453 640/5000 [==>...........................] - ETA: 1s - loss: 0.3620 - acc: 0.9406 896/5000 [====>.........................] - ETA: 1s - loss: 0.3675 - acc: 0.94421152/5000 [=====>........................] - ETA: 1s - loss: 0.3670 - acc: 0.94441408/5000 [=======>......................] - ETA: 1s - loss: 0.3866 - acc: 0.94321664/5000 [========>.....................] - ETA: 0s - loss: 0.3826 - acc: 0.94591920/5000 [==========>...................] - ETA: 0s - loss: 0.3785 - acc: 0.94322176/5000 [============>.................] - ETA: 0s - loss: 0.3771 - acc: 0.94212432/5000 [=============>................] - ETA: 0s - loss: 0.3738 - acc: 0.94372688/5000 [===============>..............] - ETA: 0s - loss: 0.3839 - acc: 0.94202944/5000 [================>.............] - ETA: 0s - loss: 0.3837 - acc: 0.94163200/5000 [==================>...........] - ETA: 0s - loss: 0.3797 - acc: 0.94253456/5000 [===================>..........] - ETA: 0s - loss: 0.3729 - acc: 0.94443712/5000 [=====================>........] - ETA: 0s - loss: 0.3689 - acc: 0.94533968/5000 [======================>.......] - ETA: 0s - loss: 0.3749 - acc: 0.94464224/5000 [========================>.....] - ETA: 0s - loss: 0.3801 - acc: 0.94324480/5000 [=========================>....] - ETA: 0s - loss: 0.3777 - acc: 0.94244736/5000 [===========================>..] - ETA: 0s - loss: 0.3775 - acc: 0.94304992/5000 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.94375000/5000 [==============================] - 2s 308us/sample - loss: 0.3736 - acc: 0.9436
Validation Loss: 0.374
validation Accuracy: 0.944
 128/5000 [..............................] - ETA: 1s - loss: 0.3364 - acc: 0.9375 384/5000 [=>............................] - ETA: 1s - loss: 0.3745 - acc: 0.9297 640/5000 [==>...........................] - ETA: 1s - loss: 0.4422 - acc: 0.9219 896/5000 [====>.........................] - ETA: 1s - loss: 0.4566 - acc: 0.92521152/5000 [=====>........................] - ETA: 1s - loss: 0.4367 - acc: 0.92801408/5000 [=======>......................] - ETA: 1s - loss: 0.4561 - acc: 0.92471664/5000 [========>.....................] - ETA: 0s - loss: 0.4473 - acc: 0.92731920/5000 [==========>...................] - ETA: 0s - loss: 0.4430 - acc: 0.92862176/5000 [============>.................] - ETA: 0s - loss: 0.4298 - acc: 0.92972432/5000 [=============>................] - ETA: 0s - loss: 0.4220 - acc: 0.93132688/5000 [===============>..............] - ETA: 0s - loss: 0.4098 - acc: 0.93342944/5000 [================>.............] - ETA: 0s - loss: 0.4099 - acc: 0.93383200/5000 [==================>...........] - ETA: 0s - loss: 0.4057 - acc: 0.93503456/5000 [===================>..........] - ETA: 0s - loss: 0.4079 - acc: 0.93553712/5000 [=====================>........] - ETA: 0s - loss: 0.4054 - acc: 0.93593968/5000 [======================>.......] - ETA: 0s - loss: 0.4029 - acc: 0.93624224/5000 [========================>.....] - ETA: 0s - loss: 0.4062 - acc: 0.93564480/5000 [=========================>....] - ETA: 0s - loss: 0.3991 - acc: 0.93774736/5000 [===========================>..] - ETA: 0s - loss: 0.3985 - acc: 0.93754992/5000 [============================>.] - ETA: 0s - loss: 0.4088 - acc: 0.93595000/5000 [==============================] - 2s 307us/sample - loss: 0.4084 - acc: 0.9360
Test Loss: 0.408
Test Accuracy: 0.936
              precision    recall  f1-score   support

    airplane       0.94      0.95      0.95       500
  automobile       0.94      0.97      0.96       500
        bird       0.92      0.94      0.93       500
         cat       0.87      0.85      0.86       500
        deer       0.94      0.95      0.95       500
         dog       0.93      0.86      0.89       500
        frog       0.90      0.98      0.94       500
       horse       0.98      0.95      0.96       500
        ship       0.98      0.95      0.96       500
       truck       0.95      0.96      0.96       500

    accuracy                           0.94      5000
   macro avg       0.94      0.94      0.94      5000
weighted avg       0.94      0.94      0.94      5000


TRAINING miniResNet FINISHED

 
############################################################################################
KERAS to TENSORFLOW GRAPH CONVERSION for LeNet on CIFAR10
############################################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From code/Keras2TF.py:73: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.

Model: "LeNet"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1_input (InputLayer)  [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 32, 50)        3800      
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 32, 50)        200       
_________________________________________________________________
activation_1 (Activation)    (None, 32, 32, 50)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 16, 16, 50)        0         
_________________________________________________________________
flatten (Flatten)            (None, 12800)             0         
_________________________________________________________________
dense (Dense)                (None, 500)               6400500   
_________________________________________________________________
activation_2 (Activation)    (None, 500)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                5010      
_________________________________________________________________
activation_3 (Activation)    (None, 10)                0         
=================================================================
Total params: 6,409,510
Trainable params: 6,409,410
Non-trainable params: 100
_________________________________________________________________

 TF input node name:
[<tf.Tensor 'conv2d_1_input:0' shape=(?, 32, 32, 3) dtype=float32>]

 TF output node name:
[<tf.Tensor 'activation_3/Softmax:0' shape=(?, 10) dtype=float32>]

FINISHED CREATING TF FILES

 
#################################################################################################
KERAS to TENSORFLOW GRAPH CONVERSION for miniVggNet  on CIFAR10
#################################################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From code/Keras2TF.py:73: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.

Model: "miniVggNet"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1_input (InputLayer)  [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 32, 32, 32)        864       
_________________________________________________________________
batch_normalization (BatchNo (None, 32, 32, 32)        128       
_________________________________________________________________
activation (Activation)      (None, 32, 32, 32)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 32, 32)        9216      
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
activation_1 (Activation)    (None, 32, 32, 32)        0         
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         
_________________________________________________________________
dropout (Dropout)            (None, 16, 16, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 16, 16, 64)        18432     
_________________________________________________________________
batch_normalization_2 (Batch (None, 16, 16, 64)        256       
_________________________________________________________________
activation_2 (Activation)    (None, 16, 16, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 16, 16, 64)        36864     
_________________________________________________________________
batch_normalization_3 (Batch (None, 16, 16, 64)        256       
_________________________________________________________________
activation_3 (Activation)    (None, 16, 16, 64)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 8, 8, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 4096)              0         
_________________________________________________________________
dense (Dense)                (None, 512)               2097664   
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_4 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                5130      
_________________________________________________________________
activation_5 (Activation)    (None, 10)                0         
=================================================================
Total params: 2,170,986
Trainable params: 2,169,578
Non-trainable params: 1,408
_________________________________________________________________

 TF input node name:
[<tf.Tensor 'conv2d_1_input:0' shape=(?, 32, 32, 3) dtype=float32>]

 TF output node name:
[<tf.Tensor 'activation_5/Softmax:0' shape=(?, 10) dtype=float32>]

FINISHED CREATING TF FILES

 
#################################################################################################
KERAS to TENSORFLOW GRAPH CONVERSION for miniGoogleNet  on CIFAR10
#################################################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Model: "miniGoogleNet"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
conv2d_1_input (InputLayer)     [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 32, 32, 96)   2688        conv2d_1_input[0][0]             
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 32, 32, 96)   384         conv2d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 32, 32, 96)   0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 32)   3104        activation[0][0]                 
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 32)   27680       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 32, 32, 64)   0           activation_1[0][0]               
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 32)   2080        concatenate[0][0]                
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 48)   27696       concatenate[0][0]                
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 48)   192         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 48)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 32, 32, 80)   0           activation_3[0][0]               
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 15, 15, 80)   57680       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 15, 15, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 15, 15, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 15, 15, 80)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 15, 15, 160)  0           activation_5[0][0]               
                                                                 max_pooling2d[0][0]              
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 15, 15, 112)  18032       concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 15, 15, 48)   69168       concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 15, 15, 112)  448         conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 15, 15, 48)   192         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 15, 15, 112)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 15, 15, 48)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 15, 15, 160)  0           activation_6[0][0]               
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 15, 15, 96)   15456       concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 15, 15, 64)   92224       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 15, 15, 96)   384         conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 15, 15, 64)   256         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 15, 15, 96)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 15, 15, 64)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 15, 15, 160)  0           activation_8[0][0]               
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 15, 15, 80)   12880       concatenate_4[0][0]              WARNING:tensorflow:From code/Keras2TF.py:73: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.


__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 15, 15, 80)   115280      concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 15, 15, 80)   320         conv2d_10[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 15, 15, 80)   320         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 15, 15, 80)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 15, 15, 80)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 15, 15, 160)  0           activation_10[0][0]              
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 15, 15, 48)   7728        concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 15, 15, 96)   138336      concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 15, 15, 48)   192         conv2d_12[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 15, 15, 96)   384         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 15, 15, 48)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 15, 15, 96)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 15, 15, 144)  0           activation_12[0][0]              
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 7, 7, 96)     124512      concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 7, 7, 96)     384         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 7, 7, 96)     0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 7, 7, 144)    0           concatenate_6[0][0]              
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 7, 7, 240)    0           activation_14[0][0]              
                                                                 max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 7, 7, 176)    42416       concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 7, 7, 160)    345760      concatenate_7[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 7, 7, 176)    704         conv2d_15[0][0]                  
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 7, 7, 160)    640         conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 7, 7, 176)    0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 7, 7, 160)    0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 7, 7, 336)    0           activation_15[0][0]              
                                                                 activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 7, 7, 176)    59312       concatenate_8[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 7, 7, 160)    484000      concatenate_8[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 7, 7, 176)    704         conv2d_17[0][0]                  
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 7, 7, 160)    640         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 7, 7, 176)    0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 7, 7, 160)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 7, 7, 336)    0           activation_17[0][0]              
                                                                 activation_18[0][0]              
__________________________________________________________________________________________________
average_pooling2d (AveragePooli (None, 1, 1, 336)    0           concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout (Dropout)               (None, 1, 1, 336)    0           average_pooling2d[0][0]          
__________________________________________________________________________________________________
flatten (Flatten)               (None, 336)          0           dropout[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 10)           3370        flatten[0][0]                    
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 10)           0           dense[0][0]                      
==================================================================================================
Total params: 1,656,250
Trainable params: 1,652,826
Non-trainable params: 3,424
__________________________________________________________________________________________________

 TF input node name:
[<tf.Tensor 'conv2d_1_input:0' shape=(?, 32, 32, 3) dtype=float32>]

 TF output node name:
[<tf.Tensor 'activation_19/Softmax:0' shape=(?, 10) dtype=float32>]

FINISHED CREATING TF FILES

 
#################################################################################################
KERAS to TENSORFLOW GRAPH CONVERSION for miniResNet  on CIFAR10
#################################################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Model: "resnet"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
conv2d_1_input (InputLayer)     [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 32, 32, 3)    12          conv2d_1_input[0][0]             
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 32, 32, 64)   1728        batch_normalization[0][0]        
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 32, 32, 64)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   1024        activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 16)   2304        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 64)   1024        activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 64)   4096        activation[0][0]                 
__________________________________________________________________________________________________
add (Add)                       (None, 32, 32, 64)   0           conv2d_3[0][0]                   
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         add[0][0]                        
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 16)   1024        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 16)   2304        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 64)   1024        activation_5[0][0]               
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 64)   0           conv2d_7[0][0]                   
                                                                 add[0][0]                        
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 16)   1024        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 16)   2304        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 64)   1024        activation_8[0][0]               
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 64)   0           conv2d_10[0][0]                  
                                                                 add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         add_2[0][0]                      
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 32, 32, 16)   1024        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 32, 32, 16)   64          conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 32, 32, 16)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 32, 32, 16)   2304        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 32, 32, 16)   64          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 32, 32, 16)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 32, 32, 64)   1024        activation_11[0][0]              
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 64)   0           conv2d_13[0][0]                  
                                                                 add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 32, 32, 64)   256         add_3[0][0]                      
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 32, 32, 64)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 32, 32, 16)   1024        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 32, 32, 16)   64          conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 32, 32, 16)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 32, 32, 16)   2304        activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 32, 32, 16)   64          conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 32, 32, 16)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 32, 32, 64)   1024        activation_14[0][0]              
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 64)   0           conv2d_16[0][0]                  
                                                                 add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 32, 32, 64)   256         add_4[0][0]                      
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 32, 32, 64)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 32, 32, 16)   1024        activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 32, 32, 16)   64          conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 32, 32, 16)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 32, 32, 16)   2304        activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 32, 32, 16)   64          conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 32, 32, 16)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 32, 32, 64)   1024        activation_17[0][0]              
__________________________________________________________________________________________________
add_5 (Add)                     (None, 32, 32, 64)   0           conv2d_19[0][0]                  
                                                                 add_4[0][0]                      
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 32, 32, 64)   256         add_5[0][0]                      
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 32, 32, 64)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 32, 32, 16)   1024        activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 32, 32, 16)   64          conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 32, 32, 16)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 32, 32, 16)   2304        activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 32, 32, 16)   64          conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 32, 32, 16)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 32, 32, 64)   1024        activation_20[0][0]              
__________________________________________________________________________________________________
add_6 (Add)                     (None, 32, 32, 64)   0           conv2d_22[0][0]                  
                                                                 add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 32, 32, 64)   256         add_6[0][0]                      
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 32, 32, 64)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 32, 32, 16)   1024        activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 32, 32, 16)   64          conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 32, 32, 16)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 32, 32, 16)   2304        activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 32, 32, 16)   64          conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 32, 32, 16)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 32, 32, 64)   1024        activation_23[0][0]              
__________________________________________________________________________________________________
add_7 (Add)                     (None, 32, 32, 64)   0           conv2d_25[0][0]                  
                                                                 add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 32, 32, 64)   256         add_7[0][0]                      
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 32, 32, 64)   0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 32, 32, 16)   1024        activation_24[0][0]              
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 32, 32, 16)   64          conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 32, 32, 16)   0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 32, 32, 16)   2304        activation_25[0][0]              
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 32, 32, 16)   64          conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 32, 32, 16)   0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 32, 32, 64)   1024        activation_26[0][0]              
__________________________________________________________________________________________________
add_8 (Add)                     (None, 32, 32, 64)   0           conv2d_28[0][0]                  
                                                                 add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 32, 32, 64)   256         add_8[0][0]                      
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 32, 32, 64)   0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 32, 32, 32)   2048        activation_27[0][0]              
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 32, 32, 32)   128         conv2d_29[0][0]                  
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 32, 32, 32)   0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 16, 16, 32)   9216        activation_28[0][0]              
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 16, 16, 32)   128         conv2d_30[0][0]                  
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 16, 16, 32)   0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 16, 16, 128)  4096        activation_29[0][0]              
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 16, 16, 128)  8192        activation_27[0][0]              
__________________________________________________________________________________________________
add_9 (Add)                     (None, 16, 16, 128)  0           conv2d_31[0][0]                  
                                                                 conv2d_32[0][0]                  
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 16, 16, 128)  512         add_9[0][0]                      
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 16, 16, 128)  0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 16, 16, 32)   4096        activation_30[0][0]              
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 16, 16, 32)   128         conv2d_33[0][0]                  
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 16, 16, 32)   0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 16, 16, 32)   9216        activation_31[0][0]              
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 16, 16, 32)   128         conv2d_34[0][0]                  
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 16, 16, 32)   0           batch_normalization_33[0][0]     
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 16, 16, 128)  4096        activation_32[0][0]              
__________________________________________________________________________________________________
add_10 (Add)                    (None, 16, 16, 128)  0           conv2d_35[0][0]                  
                                                                 add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 16, 16, 128)  512         add_10[0][0]                     
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 16, 16, 128)  0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 16, 16, 32)   4096        activation_33[0][0]              
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 16, 16, 32)   128         conv2d_36[0][0]                  
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 16, 16, 32)   0           batch_normalization_35[0][0]     
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 16, 16, 32)   9216        activation_34[0][0]              
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 16, 16, 32)   128         conv2d_37[0][0]                  
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 16, 16, 32)   0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 16, 16, 128)  4096        activation_35[0][0]              
__________________________________________________________________________________________________
add_11 (Add)                    (None, 16, 16, 128)  0           conv2d_38[0][0]                  
                                                                 add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 16, 16, 128)  512         add_11[0][0]                     
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 16, 16, 128)  0           batch_normalization_37[0][0]     
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 16, 16, 32)   4096        activation_36[0][0]              
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 16, 16, 32)   128         conv2d_39[0][0]                  
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 16, 16, 32)   0           batch_normalization_38[0][0]     
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 16, 16, 32)   9216        activation_37[0][0]              
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 16, 16, 32)   128         conv2d_40[0][0]                  
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 16, 16, 32)   0           batch_normalization_39[0][0]     
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 16, 16, 128)  4096        activation_38[0][0]              
__________________________________________________________________________________________________
add_12 (Add)                    (None, 16, 16, 128)  0           conv2d_41[0][0]                  
                                                                 add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 16, 16, 128)  512         add_12[0][0]                     
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 16, 16, 128)  0           batch_normalization_40[0][0]     
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 16, 16, 32)   4096        activation_39[0][0]              
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 16, 16, 32)   128         conv2d_42[0][0]                  
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 16, 16, 32)   0           batch_normalization_41[0][0]     
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 16, 16, 32)   9216        activation_40[0][0]              
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 16, 16, 32)   128         conv2d_43[0][0]                  
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 16, 16, 32)   0           batch_normalization_42[0][0]     
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 16, 16, 128)  4096        activation_41[0][0]              
__________________________________________________________________________________________________
add_13 (Add)                    (None, 16, 16, 128)  0           conv2d_44[0][0]                  
                                                                 add_12[0][0]                     
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 16, 16, 128)  512         add_13[0][0]                     
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 16, 16, 128)  0           batch_normalization_43[0][0]     
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 16, 16, 32)   4096        activation_42[0][0]              
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 16, 16, 32)   128         conv2d_45[0][0]                  
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 16, 16, 32)   0           batch_normalization_44[0][0]     
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 16, 16, 32)   9216        activation_43[0][0]              
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 16, 16, 32)   128         conv2d_46[0][0]                  
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 16, 16, 32)   0           batch_normalization_45[0][0]     
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 16, 16, 128)  4096        activation_44[0][0]              
__________________________________________________________________________________________________
add_14 (Add)                    (None, 16, 16, 128)  0           conv2d_47[0][0]                  
                                                                 add_13[0][0]                     
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 16, 16, 128)  512         add_14[0][0]                     
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 16, 16, 128)  0           batch_normalization_46[0][0]     
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 16, 16, 32)   4096        activation_45[0][0]              
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 16, 16, 32)   128         conv2d_48[0][0]                  
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 16, 16, 32)   0           batch_normalization_47[0][0]     
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 16, 16, 32)   9216        activation_46[0][0]              
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 16, 16, 32)   128         conv2d_49[0][0]                  
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 16, 16, 32)   0           batch_normalization_48[0][0]     
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 16, 16, 128)  4096        activation_47[0][0]              
__________________________________________________________________________________________________
add_15 (Add)                    (None, 16, 16, 128)  0           conv2d_50[0][0]                  
                                                                 add_14[0][0]                     
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 16, 16, 128)  512         add_15[0][0]                     
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 16, 16, 128)  0           batch_normalization_49[0][0]     
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 16, 16, 32)   4096        activation_48[0][0]              
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 16, 16, 32)   128         conv2d_51[0][0]                  
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 16, 16, 32)   0           batch_normalization_50[0][0]     
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 16, 16, 32)   9216        activation_49[0][0]              
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 16, 16, 32)   128         conv2d_52[0][0]                  
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 16, 16, 32)   0           batch_normalization_51[0][0]     
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 16, 16, 128)  4096        activation_50[0][0]              
__________________________________________________________________________________________________
add_16 (Add)                    (None, 16, 16, 128)  0           conv2d_53[0][0]                  
                                                                 add_15[0][0]                     
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 16, 16, 128)  512         add_16[0][0]                     
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 16, 16, 128)  0           batch_normalization_52[0][0]     
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 16, 16, 32)   4096        activation_51[0][0]              
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 16, 16, 32)   128         conv2d_54[0][0]                  
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 16, 16, 32)   0           batch_normalization_53[0][0]     
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 16, 16, 32)   9216        activation_52[0][0]              
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 16, 16, 32)   128         conv2d_55[0][0]                  
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 16, 16, 32)   0           batch_normalization_54[0][0]     
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 16, 16, 128)  4096        activation_53[0][0]              
__________________________________________________________________________________________________
add_17 (Add)                    (None, 16, 16, 128)  0           conv2d_56[0][0]                  
                                                                 add_16[0][0]                     
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 16, 16, 128)  512         add_17[0][0]                     
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 16, 16, 128)  0           batch_normalization_55[0][0]     
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 16, 16, 64)   8192        activation_54[0][0]              
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 16, 16, 64)   256         conv2d_57[0][0]                  
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 16, 16, 64)   0           batch_normalization_56[0][0]     
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 8, 8, 64)     36864       activation_55[0][0]              
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 8, 8, 64)     256         conv2d_58[0][0]                  
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 8, 8, 64)     0           batch_normalization_57[0][0]     
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 8, 8, 256)    16384       activation_56[0][0]              
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 8, 8, 256)    32768       activation_54[0][0]              
__________________________________________________________________________________________________
add_18 (Add)                    (None, 8, 8, 256)    0           conv2d_59[0][0]                  
                                                                 conv2d_60[0][0]                  
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 8, 8, 256)    1024        add_18[0][0]                     
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 8, 8, 256)    0           batch_normalization_58[0][0]     
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 8, 8, 64)     16384       activation_57[0][0]              
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 8, 8, 64)     256         conv2d_61[0][0]                  
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 8, 8, 64)     0           batch_normalization_59[0][0]     
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 8, 8, 64)     36864       activation_58[0][0]              
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 8, 8, 64)     256         conv2d_62[0][0]                  
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 8, 8, 64)     0           batch_normalization_60[0][0]     
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 8, 8, 256)    16384       activation_59[0][0]              
__________________________________________________________________________________________________
add_19 (Add)                    (None, 8, 8, 256)    0           conv2d_63[0][0]                  
                                                                 add_18[0][0]                     
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 8, 8, 256)    1024        add_19[0][0]                     
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 8, 8, 256)    0           batch_normalization_61[0][0]     
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 8, 8, 64)     16384       activation_60[0][0]              
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 8, 8, 64)     256         conv2d_64[0][0]                  
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 8, 8, 64)     0           batch_normalization_62[0][0]     
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 8, 8, 64)     36864       activation_61[0][0]              
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 8, 8, 64)     256         conv2d_65[0][0]                  
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 8, 8, 64)     0           batch_normalization_63[0][0]     
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 8, 8, 256)    16384       activation_62[0][0]              
__________________________________________________________________________________________________
add_20 (Add)                    (None, 8, 8, 256)    0           conv2d_66[0][0]                  
                                                                 add_19[0][0]                     
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 8, 8, 256)    1024        add_20[0][0]                     
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 8, 8, 256)    0           batch_normalization_64[0][0]     
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 8, 8, 64)     16384       activation_63[0][0]              
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 8, 8, 64)     256         conv2d_67[0][0]                  
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 8, 8, 64)     0           batch_normalization_65[0][0]     
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 8, 8, 64)     36864       activation_64[0][0]              
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 8, 8, 64)     256         conv2d_68[0][0]                  
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 8, 8, 64)     0           batch_normalization_66[0][0]     
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 8, 8, 256)    16384       activation_65[0][0]              
__________________________________________________________________________________________________
add_21 (Add)                    (None, 8, 8, 256)    0           conv2d_69[0][0]                  
                                                                 add_20[0][0]                     
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 8, 8, 256)    1024        add_21[0][0]                     
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 8, 8, 256)    0           batch_normalization_67[0][0]     
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 8, 8, 64)     16384       activation_66[0][0]              
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 8, 8, 64)     256         conv2d_70[0][0]                  
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 8, 8, 64)     0           batch_normalization_68[0][0]     
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 8, 8, 64)     36864       activation_67[0][0]              
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 8, 8, 64)     256         conv2d_71[0][0]                  
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 8, 8, 64)     0           batch_normalization_69[0][0]     
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 256)    16384       activation_68[0][0]              
__________________________________________________________________________________________________
add_22 (Add)                    (None, 8, 8, 256)    0           conv2d_72[0][0]                  
                                                                 add_21[0][0]                     
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 8, 8, 256)    1024        add_22[0][0]                     
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 8, 8, 256)    0           batch_normalization_70[0][0]     
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 8, 8, 64)     16384       activation_69[0][0]              
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 8, 8, 64)     256         conv2d_73[0][0]                  
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 8, 8, 64)     0           batch_normalization_71[0][0]     
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 8, 8, 64)     36864       activation_70[0][0]              
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 64)     256         conv2d_74[0][0]                  
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 8, 8, 64)     0           batch_normalization_72[0][0]     
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 8, 8, 256)    16384       activation_71[0][0]              
__________________________________________________________________________________________________
add_23 (Add)                    (None, 8, 8, 256)    0           conv2d_75[0][0]                  
                                                                 add_22[0][0]                     
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 8, 8, 256)    1024        add_23[0][0]                     
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 256)    0           batch_normalization_73[0][0]     
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 64)     16384       activation_72[0][0]              
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 8, 8, 64)     256         conv2d_76[0][0]                  
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 8, 8, 64)     0           batch_normalization_74[0][0]     
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 64)     36864       activation_73[0][0]              
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 8, 8, 64)     256         conv2d_77[0][0]                  
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 8, 8, 64)     0           batch_normalization_75[0][0]     
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 256)    16384       activation_74[0][0]              
__________________________________________________________________________________________________
add_24 (Add)                    (None, 8, 8, 256)    0           conv2d_78[0][0]                  
                                                                 add_23[0][0]                     
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 256)    1024        add_24[0][0]                     
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 8, 8, 256)    0           batch_normalization_76[0][0]     
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 64)     16384       activation_75[0][0]              
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 64)     256         conv2d_79[0][0]                  
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 64)     0           batch_normalization_77[0][0]     
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 64)     36864       activation_76[0][0]              
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 64)     256         conv2d_80[0][0]                  
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 64)     0           batch_normalization_78[0][0]     
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 256)    16384       activation_77[0][0]              
__________________________________________________________________________________________________
add_25 (Add)                    (None, 8, 8, 256)    0           conv2d_81[0][0]                  
                                                                 add_24[0][0]                     
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 256)    1024        add_25[0][0]                     
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 256)    0           batch_normalization_79[0][0]     
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 64)     16384       activation_78[0][0]              
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 64)     256         conv2d_82[0][0]                  
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 64)     0           batch_normalization_80[0][0]     
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 64)     36864       activation_79[0][0]              
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 64)     256         conv2d_83[0][0]                  
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 64)     0           batch_normalization_81[0][0]     
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 256)    16384       activation_80[0][0]              
__________________________________________________________________________________________________
add_26 (Add)                    (None, 8, 8, 256)    0           conv2d_84[0][0]                  
                                                                 add_25[0][0]                     WARNING:tensorflow:From code/Keras2TF.py:73: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.


__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 256)    1024        add_26[0][0]                     
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 256)    0           batch_normalization_82[0][0]     
__________________________________________________________________________________________________
average_pooling2d (AveragePooli (None, 1, 1, 256)    0           activation_81[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 256)          0           average_pooling2d[0][0]          
__________________________________________________________________________________________________
dense (Dense)                   (None, 10)           2570        flatten[0][0]                    
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 10)           0           dense[0][0]                      
==================================================================================================
Total params: 886,102
Trainable params: 873,872
Non-trainable params: 12,230
__________________________________________________________________________________________________

 TF input node name:
[<tf.Tensor 'conv2d_1_input:0' shape=(?, 32, 32, 3) dtype=float32>]

 TF output node name:
[<tf.Tensor 'activation_82/Softmax:0' shape=(?, 10) dtype=float32>]

FINISHED CREATING TF FILES

 
##########################################################################
FREEZE GRAPH of LeNet on CIFAR10
##########################################################################
rm: cannot remove './build/freeze/cifar10/Lenet/*': No such file or directory
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0609 09:48:09.319656 139901774796608 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2022-06-09 09:48:09.322571: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 09:48:09.379571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:48:09.379852: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:09.381478: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:09.382950: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:48:09.383321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:48:09.385317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:48:09.386835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:48:09.390894: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:09.392273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:48:09.392574: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 09:48:09.402340: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 09:48:09.404128: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5621f40723f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:48:09.404139: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 09:48:09.481498: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5621f3baa120 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:48:09.481540: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 09:48:09.484012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:48:09.484102: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:09.484131: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:09.484157: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:48:09.484183: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:48:09.484209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:48:09.484234: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:48:09.484261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:09.488451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:48:09.488527: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:09.491632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 09:48:09.491653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 09:48:09.491663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 09:48:09.495311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22401 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ./build/tf_chkpts/cifar10/LeNet/float_model.ckpt
I0609 09:48:09.566074 139901774796608 saver.py:1284] Restoring parameters from ./build/tf_chkpts/cifar10/LeNet/float_model.ckpt
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0609 09:48:10.548993 139901774796608 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0609 09:48:10.549408 139901774796608 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 10 variables.
I0609 09:48:10.587453 139901774796608 graph_util_impl.py:334] Froze 10 variables.
INFO:tensorflow:Converted 10 variables to const ops.
I0609 09:48:10.641408 139901774796608 graph_util_impl.py:394] Converted 10 variables to const ops.
Loaded meta graph file './build/tf_chkpts/cifar10/LeNet/float_model.ckpt.meta
 
##########################################################################
INSPECT FROZEN GRAPH of LeNet  on CIFAR10
##########################################################################
Op types used: 14 Const, 10 Identity, 3 BiasAdd, 2 MatMul, 2 Relu, 1 Conv2D, 1 FusedBatchNormV3, 1 MaxPool, 1 Pack, 1 Placeholder, 1 Reshape, 1 Shape, 1 Softmax, 1 StridedSlice

Found 1 possible inputs: (name=conv2d_1_input, type=float(1), shape=[?,32,32,3]) 
Found 1 possible outputs: (name=activation_3/Softmax, op=Softmax) 
 
##########################################################################
FREEZE GRAPH of miniVggNet  on CIFAR10
##########################################################################
rm: cannot remove './build/freeze/cifar10/miniVggNet/*': No such file or directory
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0609 09:48:16.251738 139801149122368 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2022-06-09 09:48:16.254982: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 09:48:16.294647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:48:16.294836: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:16.295944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:16.296966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:48:16.297235: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:48:16.298607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:48:16.299651: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:48:16.302710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:16.304081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:48:16.304377: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 09:48:16.327182: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 09:48:16.329665: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555cd756ca60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:48:16.329705: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 09:48:16.406750: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555cd7b610d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:48:16.406791: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 09:48:16.409285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:48:16.409365: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:16.409394: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:16.409421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:48:16.409447: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:48:16.409473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:48:16.409499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:48:16.409526: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:16.413817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:48:16.413892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:16.417200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 09:48:16.417223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 09:48:16.417234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 09:48:16.421571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22401 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ./build/tf_chkpts/cifar10/miniVggNet/float_model.ckpt
I0609 09:48:16.556413 139801149122368 saver.py:1284] Restoring parameters from ./build/tf_chkpts/cifar10/miniVggNet/float_model.ckpt
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0609 09:48:17.557917 139801149122368 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0609 09:48:17.558324 139801149122368 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 28 variables.
I0609 09:48:17.612207 139801149122368 graph_util_impl.py:334] Froze 28 variables.
INFO:tensorflow:Converted 28 variables to const ops.
I0609 09:48:17.626131 139801149122368 graph_util_impl.py:394] Converted 28 variables to const ops.
Loaded meta graph file './build/tf_chkpts/cifar10/miniVggNet/float_model.ckpt.meta
 
##########################################################################
INSPECT FROZEN GRAPH of miniVggNet on CIFAR10
##########################################################################
Op types used: 33 Const, 31 Identity, 5 Relu, 4 Conv2D, 4 FusedBatchNormV3, 3 Mul, 2 BiasAdd, 2 MatMul, 2 MaxPool, 2 AddV2, 1 Sub, 1 StridedSlice, 1 Softmax, 1 Shape, 1 Rsqrt, 1 Reshape, 1 Placeholder, 1 Pack

Found 1 possible inputs: (name=conv2d_1_input, type=float(1), shape=[?,32,32,3]) 
Found 1 possible outputs: (name=activation_5/Softmax, op=Softmax) 
##########################################################################
FREEZE GRAPH of miniGoogleNet  on CIFAR10
##########################################################################
rm: cannot remove './build/freeze/cifar10/miniGoogleNet/*': No such file or directory
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0609 09:48:22.517126 140329437529920 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2022-06-09 09:48:22.521125: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 09:48:22.562373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:48:22.562561: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:22.563622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:22.564731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:48:22.565012: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:48:22.566448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:48:22.567558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:48:22.570674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:22.572100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:48:22.572404: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 09:48:22.595606: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 09:48:22.597706: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e0587182c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:48:22.597729: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 09:48:22.673059: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e058bfea40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:48:22.673099: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 09:48:22.675560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:48:22.675628: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:22.675651: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:22.675671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:48:22.675690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:48:22.675710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:48:22.675729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:48:22.675749: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:22.679955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:48:22.680022: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:22.683320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 09:48:22.683344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 09:48:22.683355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 09:48:22.687704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22401 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ./build/tf_chkpts/cifar10/miniGoogleNet/float_model.ckpt
I0609 09:48:23.017267 140329437529920 saver.py:1284] Restoring parameters from ./build/tf_chkpts/cifar10/miniGoogleNet/float_model.ckpt
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0609 09:48:24.170088 140329437529920 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0609 09:48:24.170519 140329437529920 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 116 variables.
I0609 09:48:24.316587 140329437529920 graph_util_impl.py:334] Froze 116 variables.
INFO:tensorflow:Converted 116 variables to const ops.
I0609 09:48:24.332837 140329437529920 graph_util_impl.py:394] Converted 116 variables to const ops.
Loaded meta graph file './build/tf_chkpts/cifar10/miniGoogleNet/float_model.ckpt.meta
 
##########################################################################
INSPECT FROZEN GRAPH of miniGoogleNet  on CIFAR10
##########################################################################
Op types used: 130 Const, 117 Identity, 20 BiasAdd, 19 Conv2D, 19 FusedBatchNormV3, 19 Relu, 10 ConcatV2, 2 MaxPool, 1 AvgPool, 1 MatMul, 1 Pack, 1 Placeholder, 1 Reshape, 1 Shape, 1 Softmax, 1 StridedSlice

Found 1 possible inputs: (name=conv2d_1_input, type=float(1), shape=[?,32,32,3]) 
Found 1 possible outputs: (name=activation_19/Softmax, op=Softmax) 
##########################################################################
FREEZE GRAPH of miniResNet  on CIFAR10
##########################################################################
rm: cannot remove './build/freeze/cifar10/miniResNet/*': No such file or directory
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0609 09:48:29.259299 139817831970624 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2022-06-09 09:48:29.268124: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 09:48:29.317972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:48:29.318213: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:29.319700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:29.320957: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:48:29.321284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:48:29.323027: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:48:29.324096: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:48:29.327305: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:29.328722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:48:29.329034: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 09:48:29.351286: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 09:48:29.353520: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fe02186130 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:48:29.353537: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 09:48:29.427883: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fe01c4ff70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:48:29.427926: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 09:48:29.430337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:48:29.430416: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:29.430444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:29.430470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:48:29.430496: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:48:29.430521: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:48:29.430547: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:48:29.430574: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:29.434767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:48:29.434837: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:29.438123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 09:48:29.438146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 09:48:29.438156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 09:48:29.442516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22401 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ./build/tf_chkpts/cifar10/miniResNet/float_model.ckpt
I0609 09:48:30.771387 139817831970624 saver.py:1284] Restoring parameters from ./build/tf_chkpts/cifar10/miniResNet/float_model.ckpt
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0609 09:48:32.634315 139817831970624 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0609 09:48:32.634703 139817831970624 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 419 variables.
I0609 09:48:33.162660 139817831970624 graph_util_impl.py:334] Froze 419 variables.
INFO:tensorflow:Converted 419 variables to const ops.
I0609 09:48:33.202902 139817831970624 graph_util_impl.py:394] Converted 419 variables to const ops.
Loaded meta graph file './build/tf_chkpts/cifar10/miniResNet/float_model.ckpt.meta
 
##########################################################################
INSPECT FROZEN GRAPH of miniResNet
##########################################################################
Op types used: 423 Const, 419 Identity, 85 Conv2D, 83 FusedBatchNormV3, 82 Relu, 27 AddV2, 1 AvgPool, 1 BiasAdd, 1 MatMul, 1 Pack, 1 Placeholder, 1 Reshape, 1 Shape, 1 Softmax, 1 StridedSlice

Found 1 possible inputs: (name=conv2d_1_input, type=float(1), shape=[?,32,32,3]) 
Found 1 possible outputs: (name=activation_82/Softmax, op=Softmax) 
 
##########################################################################
FREEZE GRAPH COMPLETED
##########################################################################
 
 
##########################################################################
EVALUATE FROZEN GRAPH of LeNet on CIFAR10
##########################################################################
2022-06-09 09:48:42.748228: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 09:48:42.792066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:48:42.792311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:42.793465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:42.794487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:48:42.794800: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:48:42.796043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:48:42.796962: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:48:42.799655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:42.801102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:48:42.801387: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 09:48:42.822699: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 09:48:42.824573: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ddb5d2e250 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:48:42.824596: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 09:48:42.899096: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ddb5bc83d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:48:42.899148: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 09:48:42.901953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:48:42.902053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:42.902083: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:42.902123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:48:42.902166: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:48:42.902211: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:48:42.902254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:48:42.902300: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:42.907034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:48:42.907130: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:42.911685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 09:48:42.911722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 09:48:42.911740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 09:48:42.916053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22401 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 09:48:44.024722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:44.153789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:44.697618: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10
 Top 1 accuracy with validation set: 0.6838
 Top 5 accuracy with validation set: 0.9686
FINISHED!
 
##########################################################################
EVALUATE FROZEN GRAPH of miniVggNet  on CIFAR10
##########################################################################
2022-06-09 09:48:52.088836: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 09:48:52.141539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:48:52.141760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:52.142830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:52.143807: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:48:52.144114: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:48:52.145371: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:48:52.146323: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:48:52.149123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:52.150584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:48:52.150874: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 09:48:52.171909: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 09:48:52.173674: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557d426e0750 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:48:52.173697: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 09:48:52.246923: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557d42c7a480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:48:52.246979: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 09:48:52.249506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:48:52.249624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:52.249655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:52.249689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:48:52.249714: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:48:52.249743: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:48:52.249777: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:48:52.249811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:52.254070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:48:52.254184: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:48:52.258332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 09:48:52.258360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 09:48:52.258376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 09:48:52.262456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22401 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 09:48:53.320302: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:48:53.441192: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:48:53.977237: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10
 Top 1 accuracy with validation set: 0.8360
 Top 5 accuracy with validation set: 0.9918
FINISHED!
 
##########################################################################
EVALUATE FROZEN GRAPH of GoogleNet  on CIFAR10
##########################################################################
2022-06-09 09:49:01.640209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 09:49:01.676477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:49:01.676726: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:49:01.677827: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:49:01.678790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:49:01.679083: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:49:01.680317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:49:01.681271: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:49:01.684294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:49:01.685799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:49:01.686114: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 09:49:01.707329: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 09:49:01.709655: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5638310f2a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:49:01.709687: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 09:49:01.783747: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5638315a8c50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:49:01.783787: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 09:49:01.786278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:49:01.786373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:49:01.786401: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:49:01.786426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:49:01.786450: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:49:01.786474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:49:01.786498: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:49:01.786522: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:49:01.790831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:49:01.790923: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:49:01.795073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 09:49:01.795099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 09:49:01.795110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 09:49:01.798673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22401 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 09:49:02.855557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:49:02.973660: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:49:03.507848: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10
 Top 1 accuracy with validation set: 0.8778
 Top 5 accuracy with validation set: 0.9944
FINISHED!
 
##########################################################################
EVALUATE FROZEN GRAPH of ResNet  on CIFAR10
##########################################################################
2022-06-09 09:49:13.360364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 09:49:13.404725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:49:13.404954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:49:13.406333: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:49:13.407332: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:49:13.407620: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:49:13.408876: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:49:13.409819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:49:13.412462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:49:13.414307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:49:13.414618: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 09:49:13.436271: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 09:49:13.438358: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f001cf1830 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:49:13.438389: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 09:49:13.520617: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f001a734c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:49:13.520656: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 09:49:13.523085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:49:13.523172: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:49:13.523199: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:49:13.523224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:49:13.523248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:49:13.523273: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:49:13.523298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:49:13.523323: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:49:13.527437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:49:13.527515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:49:13.531270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 09:49:13.531290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 09:49:13.531299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 09:49:13.534668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22401 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 09:49:14.783645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:49:14.906041: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:49:15.444626: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10
 Top 1 accuracy with validation set: 0.9360
 Top 5 accuracy with validation set: 0.9978
FINISHED!
 
##########################################################################
EVALUATE FROZEN GRAPH COMPLETED  on CIFAR10
##########################################################################
 
 
##########################################################################
QUANTIZE LeNet on CIFAR10
##########################################################################
2022-06-09 09:49:22.626489: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
  0% (0 of 20) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--
  5% (1 of 20) |#                        | Elapsed Time: 0:00:00 ETA:   0:00:03
 10% (2 of 20) |##                       | Elapsed Time: 0:00:00 ETA:   0:00:02
 15% (3 of 20) |###                      | Elapsed Time: 0:00:00 ETA:   0:00:02
 20% (4 of 20) |#####                    | Elapsed Time: 0:00:00 ETA:   0:00:02
 25% (5 of 20) |######                   | Elapsed Time: 0:00:00 ETA:   0:00:02
 30% (6 of 20) |#######                  | Elapsed Time: 0:00:00 ETA:   0:00:02
 35% (7 of 20) |########                 | Elapsed Time: 0:00:01 ETA:   0:00:01
 40% (8 of 20) |##########               | Elapsed Time: 0:00:01 ETA:   0:00:01
 45% (9 of 20) |###########              | Elapsed Time: 0:00:01 ETA:   0:00:01
 50% (10 of 20) |############            | Elapsed Time: 0:00:01 ETA:   0:00:01
 55% (11 of 20) |#############           | Elapsed Time: 0:00:01 ETA:   0:00:01
 60% (12 of 20) |##############          | Elapsed Time: 0:00:01 ETA:   0:00:01
 65% (13 of 20) |###############         | Elapsed Time: 0:00:01 ETA:   0:00:01
 70% (14 of 20) |################        | Elapsed Time: 0:00:02 ETA:   0:00:00
 75% (15 of 20) |##################      | Elapsed Time: 0:00:02 ETA:   0:00:00
 80% (16 of 20) |###################     | Elapsed Time: 0:00:02 ETA:   0:00:00
 85% (17 of 20) |####################    | Elapsed Time: 0:00:02 ETA:   0:00:00
 90% (18 of 20) |#####################   | Elapsed Time: 0:00:02 ETA:   0:00:00
 95% (19 of 20) |######################  | Elapsed Time: 0:00:02 ETA:   0:00:00
100% (20 of 20) |########################| Elapsed Time: 0:00:02 Time:  0:00:02
script running on folder  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code
CALIB DIR  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code/../build/dataset/cifar10/calib
INFO: Checking Float Graph...
INFO: Float Graph Check Done.
INFO: Calibrating for 20 iterations...
INFO: Calibration Done.
********************* Quantization Summary *********************      
INFO: Output:       
  quantize_eval_model: ../build/quantized_results/cifar10/LeNet/ quantize_eval_model.pb 
 
##########################################################################
QUANTIZE miniVggNet  on CIFAR10
##########################################################################
2022-06-09 09:49:30.909588: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
  0% (0 of 20) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--
  5% (1 of 20) |#                        | Elapsed Time: 0:00:00 ETA:   0:00:03
 10% (2 of 20) |##                       | Elapsed Time: 0:00:00 ETA:   0:00:02
 15% (3 of 20) |###                      | Elapsed Time: 0:00:00 ETA:   0:00:02
 20% (4 of 20) |#####                    | Elapsed Time: 0:00:00 ETA:   0:00:02
 25% (5 of 20) |######                   | Elapsed Time: 0:00:00 ETA:   0:00:02
 30% (6 of 20) |#######                  | Elapsed Time: 0:00:00 ETA:   0:00:02
 35% (7 of 20) |########                 | Elapsed Time: 0:00:01 ETA:   0:00:02
 40% (8 of 20) |##########               | Elapsed Time: 0:00:01 ETA:   0:00:01
 45% (9 of 20) |###########              | Elapsed Time: 0:00:01 ETA:   0:00:01
 50% (10 of 20) |############            | Elapsed Time: 0:00:01 ETA:   0:00:01
 55% (11 of 20) |#############           | Elapsed Time: 0:00:01 ETA:   0:00:01
 60% (12 of 20) |##############          | Elapsed Time: 0:00:01 ETA:   0:00:01
 65% (13 of 20) |###############         | Elapsed Time: 0:00:01 ETA:   0:00:01
 70% (14 of 20) |################        | Elapsed Time: 0:00:02 ETA:   0:00:00
 75% (15 of 20) |##################      | Elapsed Time: 0:00:02 ETA:   0:00:00
 80% (16 of 20) |###################     | Elapsed Time: 0:00:02 ETA:   0:00:00
 85% (17 of 20) |####################    | Elapsed Time: 0:00:02 ETA:   0:00:00
 90% (18 of 20) |#####################   | Elapsed Time: 0:00:02 ETA:   0:00:00
 95% (19 of 20) |######################  | Elapsed Time: 0:00:02 ETA:   0:00:00
100% (20 of 20) |########################| Elapsed Time: 0:00:02 Time:  0:00:02
script running on folder  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code
CALIB DIR  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code/../build/dataset/cifar10/calib
INFO: Checking Float Graph...
INFO: Float Graph Check Done.
INFO: Calibrating for 20 iterations...
INFO: Calibration Done.
********************* Quantization Summary *********************      
INFO: Output:       
  quantize_eval_model: ../build/quantized_results/cifar10/miniVggNet/ quantize_eval_model.pb 
 
##########################################################################
QUANTIZE miniGoogleNet  on CIFAR10
##########################################################################
2022-06-09 09:49:39.084852: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2022-06-09 09:49:39.296663: W tensorflow/contrib/decent_q/utils/quantize_utils.cc:862] [DECENT_WARNING] Scale output of avg_pool node average_pooling2d/AvgPool to simulate DPU. Kernel size is 7 * 7
  0% (0 of 20) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--
  5% (1 of 20) |#                        | Elapsed Time: 0:00:00 ETA:   0:00:09
 10% (2 of 20) |##                       | Elapsed Time: 0:00:00 ETA:   0:00:08
 15% (3 of 20) |###                      | Elapsed Time: 0:00:01 ETA:   0:00:08
 20% (4 of 20) |#####                    | Elapsed Time: 0:00:01 ETA:   0:00:07
 25% (5 of 20) |######                   | Elapsed Time: 0:00:02 ETA:   0:00:07
 30% (6 of 20) |#######                  | Elapsed Time: 0:00:02 ETA:   0:00:06
 35% (7 of 20) |########                 | Elapsed Time: 0:00:03 ETA:   0:00:06
 40% (8 of 20) |##########               | Elapsed Time: 0:00:03 ETA:   0:00:05
 45% (9 of 20) |###########              | Elapsed Time: 0:00:04 ETA:   0:00:05
 50% (10 of 20) |############            | Elapsed Time: 0:00:04 ETA:   0:00:04
 55% (11 of 20) |#############           | Elapsed Time: 0:00:05 ETA:   0:00:04
 60% (12 of 20) |##############          | Elapsed Time: 0:00:05 ETA:   0:00:03
 65% (13 of 20) |###############         | Elapsed Time: 0:00:06 ETA:   0:00:03
 70% (14 of 20) |################        | Elapsed Time: 0:00:06 ETA:   0:00:02
 75% (15 of 20) |##################      | Elapsed Time: 0:00:07 ETA:   0:00:02
 80% (16 of 20) |###################     | Elapsed Time: 0:00:07 ETA:   0:00:01
 85% (17 of 20) |####################    | Elapsed Time: 0:00:08 ETA:   0:00:01
 90% (18 of 20) |#####################   | Elapsed Time: 0:00:08 ETA:   0:00:00
 95% (19 of 20) |######################  | Elapsed Time: 0:00:09 ETA:   0:00:00
100% (20 of 20) |########################| Elapsed Time: 0:00:09 Time:  0:00:09
script running on folder  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code
CALIB DIR  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code/../build/dataset/cifar10/calib
INFO: Checking Float Graph...
INFO: Float Graph Check Done.
INFO: Calibrating for 20 iterations...
INFO: Calibration Done.
********************* Quantization Summary *********************      
INFO: Output:       
  quantize_eval_model: ../build/quantized_results/cifar10/miniGoogleNet/ quantize_eval_model.pb 
 
##########################################################################
QUANTIZE miniResNet  on CIFAR10
##########################################################################
2022-06-09 09:49:54.104822: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2022-06-09 09:49:54.428007: W tensorflow/contrib/decent_q/utils/quantize_utils.cc:862] [DECENT_WARNING] Scale output of avg_pool node average_pooling2d/AvgPool to simulate DPU. Kernel size is 8 * 8
  0% (0 of 20) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--
  5% (1 of 20) |#                        | Elapsed Time: 0:00:02 ETA:   0:00:38
 10% (2 of 20) |##                       | Elapsed Time: 0:00:03 ETA:   0:00:34
 15% (3 of 20) |###                      | Elapsed Time: 0:00:05 ETA:   0:00:32
 20% (4 of 20) |#####                    | Elapsed Time: 0:00:07 ETA:   0:00:30
 25% (5 of 20) |######                   | Elapsed Time: 0:00:09 ETA:   0:00:28
 30% (6 of 20) |#######                  | Elapsed Time: 0:00:11 ETA:   0:00:26
 35% (7 of 20) |########                 | Elapsed Time: 0:00:13 ETA:   0:00:24
 40% (8 of 20) |##########               | Elapsed Time: 0:00:15 ETA:   0:00:22
 45% (9 of 20) |###########              | Elapsed Time: 0:00:17 ETA:   0:00:20
 50% (10 of 20) |############            | Elapsed Time: 0:00:18 ETA:   0:00:18
 55% (11 of 20) |#############           | Elapsed Time: 0:00:20 ETA:   0:00:16
 60% (12 of 20) |##############          | Elapsed Time: 0:00:22 ETA:   0:00:15
 65% (13 of 20) |###############         | Elapsed Time: 0:00:24 ETA:   0:00:13
 70% (14 of 20) |################        | Elapsed Time: 0:00:26 ETA:   0:00:11
 75% (15 of 20) |##################      | Elapsed Time: 0:00:28 ETA:   0:00:09
 80% (16 of 20) |###################     | Elapsed Time: 0:00:30 ETA:   0:00:07
 85% (17 of 20) |####################    | Elapsed Time: 0:00:32 ETA:   0:00:05
 90% (18 of 20) |#####################   | Elapsed Time: 0:00:33 ETA:   0:00:03
 95% (19 of 20) |######################  | Elapsed Time: 0:00:35 ETA:   0:00:01
100% (20 of 20) |########################| Elapsed Time: 0:00:37 Time:  0:00:37
script running on folder  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code
CALIB DIR  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code/../build/dataset/cifar10/calib
INFO: Checking Float Graph...
INFO: Float Graph Check Done.
INFO: Calibrating for 20 iterations...
INFO: Calibration Done.
********************* Quantization Summary *********************      
INFO: Output:       
  quantize_eval_model: ../build/quantized_results/cifar10/miniResNet/ quantize_eval_model.pb 
 
##########################################################################
QUANTIZATION COMPLETED  on CIFAR10
##########################################################################
 
 
##########################################################################
EVALUATE QUANTIZED GRAPH of LeNet on CIFAR10
##########################################################################
2022-06-09 09:50:39.748464: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 09:50:39.806377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:50:39.806587: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:50:39.807613: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:50:39.808543: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:50:39.808830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:50:39.810087: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:50:39.810998: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:50:39.813725: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:50:39.815379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:50:39.815649: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 09:50:39.837150: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 09:50:39.839139: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5599ab1d2680 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:50:39.839168: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 09:50:39.911728: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5599aadc0810 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:50:39.911771: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 09:50:39.914292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:50:39.914398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:50:39.914426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:50:39.914452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:50:39.914476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:50:39.914501: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:50:39.914526: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:50:39.914552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:50:39.918825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:50:39.918917: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:50:39.923757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 09:50:39.923776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 09:50:39.923784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 09:50:39.926784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22401 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 09:50:41.303302: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:50:41.432534: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:50:41.954759: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10
 Top 1 accuracy with validation set: 0.6886
 Top 5 accuracy with validation set: 0.9690
FINISHED!
 
##########################################################################
EVALUATE QUANTIZED GRAPH of miniVggNet  on CIFAR10
##########################################################################
2022-06-09 09:50:48.667780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 09:50:48.705791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:50:48.706002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:50:48.707038: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:50:48.707979: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:50:48.708264: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:50:48.709469: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:50:48.710412: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:50:48.713051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:50:48.714496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:50:48.714762: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 09:50:48.734987: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 09:50:48.737199: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556589aa6d40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:50:48.737231: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 09:50:48.812449: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556589828550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:50:48.812504: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 09:50:48.814995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:50:48.815094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:50:48.815124: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:50:48.815151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:50:48.815190: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:50:48.815222: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:50:48.815248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:50:48.815275: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:50:48.819518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:50:48.819605: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:50:48.823776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 09:50:48.823802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 09:50:48.823813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 09:50:48.827830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22401 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 09:50:49.959648: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:50:50.089565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:50:50.630176: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10
 Top 1 accuracy with validation set: 0.8428
 Top 5 accuracy with validation set: 0.9920
FINISHED!
 
##############################################################################
EVALUATE QUANTIZED GRAPH of miniGoogleNet  on CIFAR10
##############################################################################
2022-06-09 09:50:58.092804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 09:50:58.146198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:50:58.146432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:50:58.147610: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:50:58.148629: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:50:58.148959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:50:58.150293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:50:58.151529: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:50:58.154463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:50:58.155883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:50:58.156153: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 09:50:58.177377: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 09:50:58.179293: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5625d4a88d60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:50:58.179314: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 09:50:58.252905: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5625d48fab80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:50:58.252948: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 09:50:58.255434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:50:58.255537: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:50:58.255566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:50:58.255590: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:50:58.255615: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:50:58.255639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:50:58.255662: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:50:58.255687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:50:58.259987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:50:58.260074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:50:58.264275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 09:50:58.264301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 09:50:58.264311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 09:50:58.267473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22401 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 09:50:59.391821: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:50:59.522118: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:51:00.061423: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10
 Top 1 accuracy with validation set: 0.8820
 Top 5 accuracy with validation set: 0.9956
FINISHED!
 
##########################################################################
EVALUATE QUANTIZED GRAPH of miniResNet  on CIFAR10
##########################################################################
2022-06-09 09:51:10.111130: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 09:51:10.161936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:51:10.162178: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:51:10.163710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:51:10.164775: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:51:10.165091: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:51:10.166426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:51:10.167415: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:51:10.170381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:51:10.171912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:51:10.172241: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 09:51:10.194021: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 09:51:10.196303: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d3e34e0a10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:51:10.196333: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 09:51:10.271442: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d3e30d0d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 09:51:10.271486: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 09:51:10.274024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 09:51:10.274136: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:51:10.274176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:51:10.274213: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 09:51:10.274251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 09:51:10.274287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 09:51:10.274323: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 09:51:10.274360: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:51:10.278554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 09:51:10.278637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 09:51:10.282534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 09:51:10.282556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 09:51:10.282566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 09:51:10.286507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22400 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 09:51:11.604021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 09:51:11.733148: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 09:51:12.257358: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/cifar10
 Top 1 accuracy with validation set: 0.9304
 Top 5 accuracy with validation set: 0.9978
FINISHED!
 
##########################################################################
EVALUATE QUANTIZED GRAPH COMPLETED  on CIFAR10
##########################################################################
 
 
##########################################################################
COMPILE WITH Vitis AI on ZCU102: LeNet on CIFAR10
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/cifar10/LeNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/LeNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/cifar10/LeNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/18 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 18/18 [00:00<00:00, 13947.44it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/28 [00:00<?, ?it/s]                           [INFO] infer shape (NHWC)  :100%|| 28/28 [00:00<00:00, 828.36it/s]                 
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 833.69it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 788.28it/s]                   
[INFO] generate xmodel     :  0%|          | 0/20 [00:00<?, ?it/s]                           [INFO] generate xmodel     : 15%|        | 3/20 [00:00<00:02,  7.75it/s]                   [INFO] generate xmodel     :100%|| 20/20 [00:00<00:00, 50.72it/s]                  
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/LeNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 28
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/LeNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/LeNet/LeNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is b36472e556350d61b59e17f4cf80462f, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/LeNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on ZCU102: miniVggNet  on CIFAR10
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/cifar10/miniVggNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniVggNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/cifar10/miniVggNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/33 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 33/33 [00:00<00:00, 15862.03it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/45 [00:00<?, ?it/s]                           [INFO] infer shape (NHWC)  :100%|| 45/45 [00:00<00:00, 2511.96it/s]                
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 827.82it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 888.56it/s]                   
[INFO] generate xmodel     :  0%|          | 0/31 [00:00<?, ?it/s]                           [INFO] generate xmodel     : 10%|         | 3/31 [00:00<00:01, 22.64it/s]                   [INFO] generate xmodel     :100%|| 31/31 [00:00<00:00, 225.57it/s]                 
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniVggNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 51
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniVggNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniVggNet/miniVggNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 78690937d0e19301efb9b33ecde140c2, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniVggNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on ZCU102: miniGoogleNet  on CIFAR10
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/cifar10/miniGoogleNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniGoogleNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/cifar10/miniGoogleNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/94 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 94/94 [00:00<00:00, 19913.36it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/149 [00:00<?, ?it/s]                          [INFO] infer shape (NHWC)  :100%|| 149/149 [00:00<00:00, 13721.92it/s]             
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 274.33it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 395.97it/s]                   
[INFO] generate xmodel     :  0%|          | 0/95 [00:00<?, ?it/s]                           [INFO] generate xmodel     :100%|| 95/95 [00:00<00:00, 4834.38it/s]                
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniGoogleNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 173
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniGoogleNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniGoogleNet/miniGoogleNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 5140d34a57d86c908fc4aee192d7f91c, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniGoogleNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on ZCU102: miniResNet  on CIFAR10
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/cifar10/miniResNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniResNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/cifar10/miniResNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/458 [00:00<?, ?it/s]                          [INFO] parse raw model     :100%|| 458/458 [00:00<00:00, 15924.00it/s]             
[INFO] infer shape (NHWC)  :  0%|          | 0/692 [00:00<?, ?it/s]                          [INFO] infer shape (NHWC)  :100%|| 692/692 [00:00<00:00, 18323.14it/s]             
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 59.31it/s]                    
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 67.93it/s]                    
[INFO] generate xmodel     :  0%|          | 0/516 [00:00<?, ?it/s]                          [INFO] generate xmodel     :100%|| 516/516 [00:00<00:00, 6818.16it/s]              
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniResNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 798
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniResNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniResNet/miniResNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is f67049277e2254d4c692ecb7872f8714, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniResNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILATION COMPLETED  on CIFAR10 on ZCU102
##########################################################################
 
 
##########################################################################
COMPILE WITH Vitis AI on ZCU104: LeNet on CIFAR10
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/cifar10/LeNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/LeNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/cifar10/LeNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/18 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 18/18 [00:00<00:00, 14056.50it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/28 [00:00<?, ?it/s]                           [INFO] infer shape (NHWC)  :100%|| 28/28 [00:00<00:00, 873.15it/s]                 
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 1280.31it/s]                  
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 1338.96it/s]                  
[INFO] generate xmodel     :  0%|          | 0/20 [00:00<?, ?it/s]                           [INFO] generate xmodel     : 15%|        | 3/20 [00:00<00:02,  7.62it/s]                   [INFO] generate xmodel     :100%|| 20/20 [00:00<00:00, 50.47it/s]                  
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/LeNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 28
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/LeNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/LeNet/LeNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 801997cb36c53371b6bd218651c0fb46, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/LeNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on ZCU104: miniVggNet  on CIFAR10
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/cifar10/miniVggNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniVggNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/cifar10/miniVggNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/33 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 33/33 [00:00<00:00, 15997.69it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/45 [00:00<?, ?it/s]                           [INFO] infer shape (NHWC)  :100%|| 45/45 [00:00<00:00, 2755.02it/s]                
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 613.02it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 529.81it/s]                   
[INFO] generate xmodel     :  0%|          | 0/31 [00:00<?, ?it/s]                           [INFO] generate xmodel     : 10%|         | 3/31 [00:00<00:01, 19.29it/s]                   [INFO] generate xmodel     :100%|| 31/31 [00:00<00:00, 193.75it/s]                 
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniVggNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 51
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniVggNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniVggNet/miniVggNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 517936dd724df0c5508a1b2f42a73ae7, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniVggNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on ZCU104: miniGoogleNet  on CIFAR10
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/cifar10/miniGoogleNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniGoogleNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/cifar10/miniGoogleNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/94 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 94/94 [00:00<00:00, 19089.94it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/149 [00:00<?, ?it/s]                          [INFO] infer shape (NHWC)  :100%|| 149/149 [00:00<00:00, 13910.06it/s]             
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 280.44it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 394.00it/s]                   
[INFO] generate xmodel     :  0%|          | 0/95 [00:00<?, ?it/s]                           [INFO] generate xmodel     :100%|| 95/95 [00:00<00:00, 4867.80it/s]                
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniGoogleNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 173
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniGoogleNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniGoogleNet/miniGoogleNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 0318a0f983598929db91aaaf3a3e44a2, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniGoogleNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on ZCU104: miniResNet  on CIFAR10
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/cifar10/miniResNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniResNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/cifar10/miniResNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/458 [00:00<?, ?it/s]                          [INFO] parse raw model     :100%|| 458/458 [00:00<00:00, 16165.47it/s]             
[INFO] infer shape (NHWC)  :  0%|          | 0/692 [00:00<?, ?it/s]                          [INFO] infer shape (NHWC)  :100%|| 692/692 [00:00<00:00, 18446.46it/s]             
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 66.62it/s]                    
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 68.09it/s]                    
[INFO] generate xmodel     :  0%|          | 0/516 [00:00<?, ?it/s]                          [INFO] generate xmodel     :100%|| 516/516 [00:00<00:00, 6933.44it/s]              
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniResNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 798
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniResNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniResNet/miniResNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is eb1fd3442d8e6036bbd278eb7c4556d9, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniResNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILATION COMPLETED  on CIFAR10 on ZCU104
##########################################################################
 
 
##########################################################################
COMPILE WITH Vitis AI on VCK190: LeNet on CIFAR10
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/cifar10/LeNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/LeNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/cifar10/LeNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/18 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 18/18 [00:00<00:00, 13965.50it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/28 [00:00<?, ?it/s]                           [INFO] infer shape (NHWC)  :100%|| 28/28 [00:00<00:00, 862.21it/s]                 
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 1370.84it/s]                  
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 1371.14it/s]                  
[INFO] generate xmodel     :  0%|          | 0/20 [00:00<?, ?it/s]                           [INFO] generate xmodel     : 15%|        | 3/20 [00:00<00:02,  7.72it/s]                   [INFO] generate xmodel     :100%|| 20/20 [00:00<00:00, 51.15it/s]                  
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/LeNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA2_C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 28
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/LeNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/LeNet/LeNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is bf80d04740cdb74e512f1cd89d43aa2d, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/LeNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on VCK190: miniVggNet  on CIFAR10
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/cifar10/miniVggNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniVggNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/cifar10/miniVggNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/33 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 33/33 [00:00<00:00, 15957.12it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/45 [00:00<?, ?it/s]                           [INFO] infer shape (NHWC)  :100%|| 45/45 [00:00<00:00, 3555.16it/s]                
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 559.29it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 525.04it/s]                   
[INFO] generate xmodel     :  0%|          | 0/31 [00:00<?, ?it/s]                           [INFO] generate xmodel     : 10%|         | 3/31 [00:00<00:01, 18.94it/s]                   [INFO] generate xmodel     :100%|| 31/31 [00:00<00:00, 190.30it/s]                 
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniVggNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA2_C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 51
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniVggNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniVggNet/miniVggNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 6989884d88cd7ccb883dc35ac134d3f8, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniVggNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on VCK190: miniGoogleNet  on CIFAR10
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/cifar10/miniGoogleNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniGoogleNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/cifar10/miniGoogleNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/94 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 94/94 [00:00<00:00, 20033.77it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/149 [00:00<?, ?it/s]                          [INFO] infer shape (NHWC)  :100%|| 149/149 [00:00<00:00, 13984.14it/s]             
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 301.65it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 393.61it/s]                   
[INFO] generate xmodel     :  0%|          | 0/95 [00:00<?, ?it/s]                           [INFO] generate xmodel     :100%|| 95/95 [00:00<00:00, 4922.65it/s]                
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniGoogleNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA2_C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 173
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniGoogleNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniGoogleNet/miniGoogleNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 5452be6bcad066f196d19d2afcc60f6f, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniGoogleNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on VCK190: miniResNet  on CIFAR10
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/cifar10/miniResNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniResNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/cifar10/miniResNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/458 [00:00<?, ?it/s]                          [INFO] parse raw model     :100%|| 458/458 [00:00<00:00, 14691.42it/s]             
[INFO] infer shape (NHWC)  :  0%|          | 0/692 [00:00<?, ?it/s]                          [INFO] infer shape (NHWC)  :100%|| 692/692 [00:00<00:00, 17915.64it/s]             
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 69.25it/s]                    
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 68.13it/s]                    
[INFO] generate xmodel     :  0%|          | 0/516 [00:00<?, ?it/s]                          [INFO] generate xmodel     :100%|| 516/516 [00:00<00:00, 6668.29it/s]              
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniResNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA2_C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 798
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniResNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniResNet/miniResNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 53fdf936cf8a6b628df66b9f2e0c35d3, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/cifar10/miniResNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILATION COMPLETED  on CIFAR10 on VCK190
##########################################################################
 
 
 
##########################################################################
TRAIN & EVAL LeNet on FMNIST
##########################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2022-06-09 10:59:42.008979: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 10:59:42.053517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 10:59:42.053834: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 10:59:42.055307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 10:59:42.056605: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 10:59:42.057055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 10:59:42.058766: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 10:59:42.060067: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 10:59:42.063083: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 10:59:42.064564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 10:59:42.065499: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 10:59:42.087292: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 10:59:42.089721: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557827c74e70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 10:59:42.089758: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 10:59:42.177573: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5578268b98d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 10:59:42.177609: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 10:59:42.180228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 10:59:42.180325: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 10:59:42.180358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 10:59:42.180386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 10:59:42.180410: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 10:59:42.180436: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 10:59:42.180465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 10:59:42.180490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 10:59:42.184991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 10:59:42.185098: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 10:59:42.189125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 10:59:42.189151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 10:59:42.189162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 10:59:42.193205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22287 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 10:59:43.513010: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 10:59:43.606536: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 10:59:44.223430: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
[INFO] compiling model...
[INFO] training model...
Epoch 1/20
Epoch 1/20

Epoch 00001: val_loss improved from inf to 0.40678, saving model to keras_model/fmnist/LeNet/best_chkpt.hdf5
3000/3000 - 30s - loss: 0.4989 - acc: 0.8587 - val_loss: 0.4068 - val_acc: 0.8882
Epoch 2/20
Epoch 1/20

Epoch 00002: val_loss improved from 0.40678 to 0.36763, saving model to keras_model/fmnist/LeNet/best_chkpt.hdf5
3000/3000 - 29s - loss: 0.3480 - acc: 0.9095 - val_loss: 0.3676 - val_acc: 0.9068
Epoch 3/20
Epoch 1/20

Epoch 00003: val_loss improved from 0.36763 to 0.35356, saving model to keras_model/fmnist/LeNet/best_chkpt.hdf5
3000/3000 - 29s - loss: 0.3088 - acc: 0.9216 - val_loss: 0.3536 - val_acc: 0.9078
Epoch 4/20
Epoch 1/20

Epoch 00004: val_loss improved from 0.35356 to 0.33603, saving model to keras_model/fmnist/LeNet/best_chkpt.hdf5
3000/3000 - 29s - loss: 0.2787 - acc: 0.9339 - val_loss: 0.3360 - val_acc: 0.9178
Epoch 5/20
Epoch 1/20

Epoch 00005: val_loss improved from 0.33603 to 0.32815, saving model to keras_model/fmnist/LeNet/best_chkpt.hdf5
3000/3000 - 29s - loss: 0.2618 - acc: 0.9396 - val_loss: 0.3281 - val_acc: 0.9206
Epoch 6/20
Epoch 1/20

Epoch 00006: val_loss did not improve from 0.32815
3000/3000 - 28s - loss: 0.2423 - acc: 0.9474 - val_loss: 0.3335 - val_acc: 0.9169
Epoch 7/20
Epoch 1/20

Epoch 00007: val_loss did not improve from 0.32815
3000/3000 - 28s - loss: 0.2292 - acc: 0.9524 - val_loss: 0.3310 - val_acc: 0.9223
Epoch 8/20
Epoch 1/20

Epoch 00008: val_loss improved from 0.32815 to 0.32769, saving model to keras_model/fmnist/LeNet/best_chkpt.hdf5
3000/3000 - 29s - loss: 0.2157 - acc: 0.9573 - val_loss: 0.3277 - val_acc: 0.9240
Epoch 9/20
Epoch 1/20

Epoch 00009: val_loss did not improve from 0.32769
3000/3000 - 28s - loss: 0.2061 - acc: 0.9611 - val_loss: 0.3365 - val_acc: 0.9229
Epoch 10/20
Epoch 1/20

Epoch 00010: val_loss did not improve from 0.32769
3000/3000 - 28s - loss: 0.1983 - acc: 0.9637 - val_loss: 0.3355 - val_acc: 0.9249
Epoch 11/20
Epoch 1/20

Epoch 00011: val_loss did not improve from 0.32769
3000/3000 - 28s - loss: 0.1868 - acc: 0.9686 - val_loss: 0.3393 - val_acc: 0.9215
Epoch 12/20
Epoch 1/20

Epoch 00012: val_loss did not improve from 0.32769
3000/3000 - 28s - loss: 0.1825 - acc: 0.9697 - val_loss: 0.3455 - val_acc: 0.9219
Epoch 13/20
Epoch 1/20

Epoch 00013: val_loss did not improve from 0.32769
3000/3000 - 28s - loss: 0.1738 - acc: 0.9728 - val_loss: 0.3373 - val_acc: 0.9233
Epoch 14/20
Epoch 1/20

Epoch 00014: val_loss did not improve from 0.32769
3000/3000 - 28s - loss: 0.1671 - acc: 0.9760 - val_loss: 0.3490 - val_acc: 0.9220
Epoch 15/20
Epoch 1/20

Epoch 00015: val_loss did not improve from 0.32769
3000/3000 - 28s - loss: 0.1634 - acc: 0.9773 - val_loss: 0.3423 - val_acc: 0.9235
Epoch 16/20
Epoch 1/20

Epoch 00016: val_loss did not improve from 0.32769
3000/3000 - 28s - loss: 0.1564 - acc: 0.9800 - val_loss: 0.3492 - val_acc: 0.9236
Epoch 17/20
Epoch 1/20

Epoch 00017: val_loss did not improve from 0.32769
3000/3000 - 28s - loss: 0.1529 - acc: 0.9814 - val_loss: 0.3484 - val_acc: 0.9249
Epoch 18/20
Epoch 1/20

Epoch 00018: val_loss did not improve from 0.32769
3000/3000 - 28s - loss: 0.1478 - acc: 0.9833 - val_loss: 0.3569 - val_acc: 0.9238
Epoch 19/20
Epoch 1/20

Epoch 00019: val_loss did not improve from 0.32769
3000/3000 - 28s - loss: 0.1443 - acc: 0.9846 - val_loss: 0.3573 - val_acc: 0.9214
Epoch 20/20
Epoch 1/20

Epoch 00020: val_loss did not improve from 0.32769
3000/3000 - 28s - loss: 0.1411 - acc: 0.9858 - val_loss: 0.3610 - val_acc: 0.9194


Elapsed time for Keras training (s):  566.086133


[INFO] evaluating network on Test and Validation datasets...
  32/5000 [..............................] - ETA: 0s - loss: 0.5225 - acc: 0.9062 512/5000 [==>...........................] - ETA: 0s - loss: 0.4234 - acc: 0.8965 992/5000 [====>.........................] - ETA: 0s - loss: 0.3939 - acc: 0.91231504/5000 [========>.....................] - ETA: 0s - loss: 0.3959 - acc: 0.91221984/5000 [==========>...................] - ETA: 0s - loss: 0.3812 - acc: 0.91582464/5000 [=============>................] - ETA: 0s - loss: 0.3674 - acc: 0.91882912/5000 [================>.............] - ETA: 0s - loss: 0.3636 - acc: 0.91903456/5000 [===================>..........] - ETA: 0s - loss: 0.3653 - acc: 0.91843968/5000 [======================>.......] - ETA: 0s - loss: 0.3622 - acc: 0.91864416/5000 [=========================>....] - ETA: 0s - loss: 0.3590 - acc: 0.91894896/5000 [============================>.] - ETA: 0s - loss: 0.3589 - acc: 0.91935000/5000 [==============================] - 1s 107us/sample - loss: 0.3566 - acc: 0.9200
Validation Loss: 0.357
validation Accuracy: 0.920
  32/5000 [..............................] - ETA: 0s - loss: 0.3691 - acc: 0.9062 480/5000 [=>............................] - ETA: 0s - loss: 0.3659 - acc: 0.9208 960/5000 [====>.........................] - ETA: 0s - loss: 0.3761 - acc: 0.91671408/5000 [=======>......................] - ETA: 0s - loss: 0.4021 - acc: 0.90981856/5000 [==========>...................] - ETA: 0s - loss: 0.4004 - acc: 0.90732304/5000 [============>.................] - ETA: 0s - loss: 0.4045 - acc: 0.90842752/5000 [===============>..............] - ETA: 0s - loss: 0.4118 - acc: 0.90703200/5000 [==================>...........] - ETA: 0s - loss: 0.4120 - acc: 0.90723648/5000 [====================>.........] - ETA: 0s - loss: 0.4036 - acc: 0.90874096/5000 [=======================>......] - ETA: 0s - loss: 0.4060 - acc: 0.90874576/5000 [==========================>...] - ETA: 0s - loss: 0.4097 - acc: 0.90585000/5000 [==============================] - 1s 113us/sample - loss: 0.4084 - acc: 0.9070
Test Loss: 0.408
Test Accuracy: 0.907
              precision    recall  f1-score   support

         top       0.85      0.86      0.85       500
     trouser       0.99      0.99      0.99       500
    pullover       0.84      0.88      0.86       500
       dress       0.90      0.90      0.90       500
        coat       0.85      0.83      0.84       500
      sandal       0.98      0.98      0.98       500
       shirt       0.76      0.73      0.75       500
     sneaker       0.95      0.97      0.96       500
         bag       0.98      0.98      0.98       500
   ankleBoot       0.97      0.95      0.96       500

    accuracy                           0.91      5000
   macro avg       0.91      0.91      0.91      5000
weighted avg       0.91      0.91      0.91      5000


TRAINING LeNet FINISHED

 
##########################################################################
TRAIN & EVAL miniVggNet  on FMNIST
##########################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2022-06-09 11:10:06.013803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 11:10:06.058561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 11:10:06.058866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 11:10:06.060396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 11:10:06.061762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 11:10:06.062162: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 11:10:06.063549: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 11:10:06.064557: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 11:10:06.067580: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 11:10:06.069212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 11:10:06.069683: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 11:10:06.090852: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 11:10:06.093746: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a0f802a900 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 11:10:06.093778: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 11:10:06.184228: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a0f848c010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 11:10:06.184269: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 11:10:06.186668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 11:10:06.186771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 11:10:06.186812: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 11:10:06.186849: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 11:10:06.186885: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 11:10:06.186921: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 11:10:06.186964: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 11:10:06.187001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 11:10:06.191215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 11:10:06.191298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 11:10:06.195045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 11:10:06.195065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 11:10:06.195074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 11:10:06.198676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22375 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 11:10:08.012958: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 11:10:08.101794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 11:10:08.629372: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
[INFO] compiling model...
[INFO] training model...
Epoch 1/40
Epoch 1/40

Epoch 00001: val_loss improved from inf to 0.39013, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 27s - loss: 0.5358 - acc: 0.8457 - val_loss: 0.3901 - val_acc: 0.8997
Epoch 2/40
Epoch 1/40

Epoch 00002: val_loss improved from 0.39013 to 0.34482, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.3958 - acc: 0.8932 - val_loss: 0.3448 - val_acc: 0.9145
Epoch 3/40
Epoch 1/40

Epoch 00003: val_loss improved from 0.34482 to 0.31727, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.3592 - acc: 0.9081 - val_loss: 0.3173 - val_acc: 0.9271
Epoch 4/40
Epoch 1/40

Epoch 00004: val_loss did not improve from 0.31727
1500/1500 - 24s - loss: 0.3331 - acc: 0.9161 - val_loss: 0.3173 - val_acc: 0.9265
Epoch 5/40
Epoch 1/40

Epoch 00005: val_loss improved from 0.31727 to 0.30058, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.3169 - acc: 0.9229 - val_loss: 0.3006 - val_acc: 0.9313
Epoch 6/40
Epoch 1/40

Epoch 00006: val_loss improved from 0.30058 to 0.29531, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.3031 - acc: 0.9261 - val_loss: 0.2953 - val_acc: 0.9369
Epoch 7/40
Epoch 1/40

Epoch 00007: val_loss improved from 0.29531 to 0.28348, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.2921 - acc: 0.9301 - val_loss: 0.2835 - val_acc: 0.9377
Epoch 8/40
Epoch 1/40

Epoch 00008: val_loss improved from 0.28348 to 0.27989, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.2847 - acc: 0.9324 - val_loss: 0.2799 - val_acc: 0.9377
Epoch 9/40
Epoch 1/40

Epoch 00009: val_loss improved from 0.27989 to 0.27794, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.2765 - acc: 0.9346 - val_loss: 0.2779 - val_acc: 0.9352
Epoch 10/40
Epoch 1/40

Epoch 00010: val_loss improved from 0.27794 to 0.27753, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.2707 - acc: 0.9371 - val_loss: 0.2775 - val_acc: 0.9379
Epoch 11/40
Epoch 1/40

Epoch 00011: val_loss improved from 0.27753 to 0.27371, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.2635 - acc: 0.9390 - val_loss: 0.2737 - val_acc: 0.9389
Epoch 12/40
Epoch 1/40

Epoch 00012: val_loss improved from 0.27371 to 0.27046, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.2591 - acc: 0.9398 - val_loss: 0.2705 - val_acc: 0.9384
Epoch 13/40
Epoch 1/40

Epoch 00013: val_loss did not improve from 0.27046
1500/1500 - 24s - loss: 0.2534 - acc: 0.9419 - val_loss: 0.2789 - val_acc: 0.9389
Epoch 14/40
Epoch 1/40

Epoch 00014: val_loss did not improve from 0.27046
1500/1500 - 24s - loss: 0.2487 - acc: 0.9430 - val_loss: 0.2706 - val_acc: 0.9422
Epoch 15/40
Epoch 1/40

Epoch 00015: val_loss improved from 0.27046 to 0.26378, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.2455 - acc: 0.9442 - val_loss: 0.2638 - val_acc: 0.9403
Epoch 16/40
Epoch 1/40

Epoch 00016: val_loss did not improve from 0.26378
1500/1500 - 24s - loss: 0.2398 - acc: 0.9456 - val_loss: 0.2702 - val_acc: 0.9378
Epoch 17/40
Epoch 1/40

Epoch 00017: val_loss did not improve from 0.26378
1500/1500 - 24s - loss: 0.2371 - acc: 0.9471 - val_loss: 0.2641 - val_acc: 0.9451
Epoch 18/40
Epoch 1/40

Epoch 00018: val_loss did not improve from 0.26378
1500/1500 - 24s - loss: 0.2348 - acc: 0.9474 - val_loss: 0.2662 - val_acc: 0.9412
Epoch 19/40
Epoch 1/40

Epoch 00019: val_loss improved from 0.26378 to 0.25954, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.2304 - acc: 0.9483 - val_loss: 0.2595 - val_acc: 0.9441
Epoch 20/40
Epoch 1/40

Epoch 00020: val_loss improved from 0.25954 to 0.25235, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.2293 - acc: 0.9480 - val_loss: 0.2523 - val_acc: 0.9450
Epoch 21/40
Epoch 1/40

Epoch 00021: val_loss did not improve from 0.25235
1500/1500 - 24s - loss: 0.2235 - acc: 0.9507 - val_loss: 0.2565 - val_acc: 0.9444
Epoch 22/40
Epoch 1/40

Epoch 00022: val_loss did not improve from 0.25235
1500/1500 - 24s - loss: 0.2229 - acc: 0.9503 - val_loss: 0.2552 - val_acc: 0.9439
Epoch 23/40
Epoch 1/40

Epoch 00023: val_loss did not improve from 0.25235
1500/1500 - 24s - loss: 0.2203 - acc: 0.9523 - val_loss: 0.2528 - val_acc: 0.9459
Epoch 24/40
Epoch 1/40

Epoch 00024: val_loss did not improve from 0.25235
1500/1500 - 24s - loss: 0.2156 - acc: 0.9532 - val_loss: 0.2544 - val_acc: 0.9435
Epoch 25/40
Epoch 1/40

Epoch 00025: val_loss improved from 0.25235 to 0.25123, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.2155 - acc: 0.9527 - val_loss: 0.2512 - val_acc: 0.9452
Epoch 26/40
Epoch 1/40

Epoch 00026: val_loss did not improve from 0.25123
1500/1500 - 24s - loss: 0.2131 - acc: 0.9532 - val_loss: 0.2531 - val_acc: 0.9465
Epoch 27/40
Epoch 1/40

Epoch 00027: val_loss improved from 0.25123 to 0.24791, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.2102 - acc: 0.9556 - val_loss: 0.2479 - val_acc: 0.9498
Epoch 28/40
Epoch 1/40

Epoch 00028: val_loss did not improve from 0.24791
1500/1500 - 24s - loss: 0.2075 - acc: 0.9557 - val_loss: 0.2553 - val_acc: 0.9460
Epoch 29/40
Epoch 1/40

Epoch 00029: val_loss improved from 0.24791 to 0.24756, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.2068 - acc: 0.9562 - val_loss: 0.2476 - val_acc: 0.9468
Epoch 30/40
Epoch 1/40

Epoch 00030: val_loss did not improve from 0.24756
1500/1500 - 24s - loss: 0.2035 - acc: 0.9573 - val_loss: 0.2546 - val_acc: 0.9469
Epoch 31/40
Epoch 1/40

Epoch 00031: val_loss improved from 0.24756 to 0.24608, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.2018 - acc: 0.9578 - val_loss: 0.2461 - val_acc: 0.9471
Epoch 32/40
Epoch 1/40

Epoch 00032: val_loss did not improve from 0.24608
1500/1500 - 24s - loss: 0.1992 - acc: 0.9580 - val_loss: 0.2491 - val_acc: 0.9483
Epoch 33/40
Epoch 1/40

Epoch 00033: val_loss did not improve from 0.24608
1500/1500 - 24s - loss: 0.1992 - acc: 0.9584 - val_loss: 0.2531 - val_acc: 0.9461
Epoch 34/40
Epoch 1/40

Epoch 00034: val_loss improved from 0.24608 to 0.24488, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.1972 - acc: 0.9585 - val_loss: 0.2449 - val_acc: 0.9488
Epoch 35/40
Epoch 1/40

Epoch 00035: val_loss did not improve from 0.24488
1500/1500 - 24s - loss: 0.1943 - acc: 0.9600 - val_loss: 0.2503 - val_acc: 0.9457
Epoch 36/40
Epoch 1/40

Epoch 00036: val_loss did not improve from 0.24488
1500/1500 - 24s - loss: 0.1943 - acc: 0.9591 - val_loss: 0.2488 - val_acc: 0.9476
Epoch 37/40
Epoch 1/40

Epoch 00037: val_loss improved from 0.24488 to 0.24076, saving model to keras_model/fmnist/miniVggNet/best_chkpt.hdf5
1500/1500 - 25s - loss: 0.1926 - acc: 0.9602 - val_loss: 0.2408 - val_acc: 0.9489
Epoch 38/40
Epoch 1/40

Epoch 00038: val_loss did not improve from 0.24076
1500/1500 - 24s - loss: 0.1932 - acc: 0.9606 - val_loss: 0.2495 - val_acc: 0.9451
Epoch 39/40
Epoch 1/40

Epoch 00039: val_loss did not improve from 0.24076
1500/1500 - 25s - loss: 0.1907 - acc: 0.9607 - val_loss: 0.2436 - val_acc: 0.9471
Epoch 40/40
Epoch 1/40

Epoch 00040: val_loss did not improve from 0.24076
1500/1500 - 24s - loss: 0.1866 - acc: 0.9629 - val_loss: 0.2478 - val_acc: 0.9470


Elapsed time for Keras training (s):  988.848988


[INFO] evaluating network on Test and Validation datasets...
  64/5000 [..............................] - ETA: 0s - loss: 0.4006 - acc: 0.8906 768/5000 [===>..........................] - ETA: 0s - loss: 0.2778 - acc: 0.94011408/5000 [=======>......................] - ETA: 0s - loss: 0.2670 - acc: 0.94252048/5000 [===========>..................] - ETA: 0s - loss: 0.2676 - acc: 0.93992688/5000 [===============>..............] - ETA: 0s - loss: 0.2573 - acc: 0.94383328/5000 [==================>...........] - ETA: 0s - loss: 0.2565 - acc: 0.94413968/5000 [======================>.......] - ETA: 0s - loss: 0.2532 - acc: 0.94664672/5000 [===========================>..] - ETA: 0s - loss: 0.2482 - acc: 0.94695000/5000 [==============================] - 0s 79us/sample - loss: 0.2468 - acc: 0.9466
Validation Loss: 0.247
validation Accuracy: 0.947
  64/5000 [..............................] - ETA: 0s - loss: 0.2088 - acc: 0.9375 768/5000 [===>..........................] - ETA: 0s - loss: 0.2618 - acc: 0.94141408/5000 [=======>......................] - ETA: 0s - loss: 0.2671 - acc: 0.93182112/5000 [===========>..................] - ETA: 0s - loss: 0.2586 - acc: 0.93282816/5000 [===============>..............] - ETA: 0s - loss: 0.2633 - acc: 0.93293456/5000 [===================>..........] - ETA: 0s - loss: 0.2627 - acc: 0.93434096/5000 [=======================>......] - ETA: 0s - loss: 0.2626 - acc: 0.93604736/5000 [===========================>..] - ETA: 0s - loss: 0.2668 - acc: 0.93185000/5000 [==============================] - 0s 78us/sample - loss: 0.2629 - acc: 0.9328
Test Loss: 0.263
Test Accuracy: 0.933
              precision    recall  f1-score   support

         top       0.88      0.88      0.88       500
     trouser       1.00      0.99      0.99       500
    pullover       0.90      0.92      0.91       500
       dress       0.95      0.94      0.94       500
        coat       0.87      0.91      0.89       500
      sandal       0.99      0.98      0.99       500
       shirt       0.81      0.77      0.79       500
     sneaker       0.95      0.99      0.97       500
         bag       0.99      0.99      0.99       500
   ankleBoot       0.99      0.96      0.97       500

    accuracy                           0.93      5000
   macro avg       0.93      0.93      0.93      5000
weighted avg       0.93      0.93      0.93      5000


TRAINING miniVggNet FINISHED

 
##########################################################################
TRAIN & EVAL miniGoogleNet on FMNIST
##########################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2022-06-09 11:27:32.427826: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 11:27:32.471541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 11:27:32.471811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 11:27:32.473236: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 11:27:32.474500: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 11:27:32.474870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 11:27:32.476532: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 11:27:32.477702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 11:27:32.480534: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 11:27:32.482391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 11:27:32.482832: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 11:27:32.503756: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 11:27:32.506016: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55dab753f3b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 11:27:32.506048: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 11:27:32.590140: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55dab7986670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 11:27:32.590181: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 11:27:32.592605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 11:27:32.592731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 11:27:32.592773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 11:27:32.592797: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 11:27:32.592820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 11:27:32.592844: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 11:27:32.592866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 11:27:32.592890: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 11:27:32.596988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 11:27:32.597080: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 11:27:32.600888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 11:27:32.600910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 11:27:32.600919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 11:27:32.604847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22434 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 11:27:35.996387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 11:27:36.087144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 11:27:36.611222: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
[INFO] compiling model...
[INFO] training model...
Epoch 1/70
Epoch 1/70

Epoch 00001: val_loss improved from inf to 0.42197, saving model to keras_model/fmnist/miniGoogleNet/best_chkpt.hdf5
857/857 - 63s - loss: 0.4955 - acc: 0.8233 - val_loss: 0.4220 - val_acc: 0.8587
Epoch 2/70
Epoch 1/70

Epoch 00002: val_loss improved from 0.42197 to 0.30221, saving model to keras_model/fmnist/miniGoogleNet/best_chkpt.hdf5
857/857 - 58s - loss: 0.2733 - acc: 0.9025 - val_loss: 0.3022 - val_acc: 0.8981
Epoch 3/70
Epoch 1/70

Epoch 00003: val_loss did not improve from 0.30221
857/857 - 58s - loss: 0.2183 - acc: 0.9223 - val_loss: 0.3173 - val_acc: 0.8908
Epoch 4/70
Epoch 1/70

Epoch 00004: val_loss improved from 0.30221 to 0.25024, saving model to keras_model/fmnist/miniGoogleNet/best_chkpt.hdf5
857/857 - 59s - loss: 0.1799 - acc: 0.9364 - val_loss: 0.2502 - val_acc: 0.9158
Epoch 5/70
Epoch 1/70

Epoch 00005: val_loss improved from 0.25024 to 0.24028, saving model to keras_model/fmnist/miniGoogleNet/best_chkpt.hdf5
857/857 - 58s - loss: 0.1478 - acc: 0.9481 - val_loss: 0.2403 - val_acc: 0.9185
Epoch 6/70
Epoch 1/70

Epoch 00006: val_loss improved from 0.24028 to 0.24000, saving model to keras_model/fmnist/miniGoogleNet/best_chkpt.hdf5
857/857 - 59s - loss: 0.1202 - acc: 0.9582 - val_loss: 0.2400 - val_acc: 0.9238
Epoch 7/70
Epoch 1/70

Epoch 00007: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0877 - acc: 0.9699 - val_loss: 0.2667 - val_acc: 0.9160
Epoch 8/70
Epoch 1/70

Epoch 00008: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0649 - acc: 0.9788 - val_loss: 0.3266 - val_acc: 0.9119
Epoch 9/70
Epoch 1/70

Epoch 00009: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0497 - acc: 0.9841 - val_loss: 0.3113 - val_acc: 0.9127
Epoch 10/70
Epoch 1/70

Epoch 00010: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0333 - acc: 0.9898 - val_loss: 0.2505 - val_acc: 0.9303
Epoch 11/70
Epoch 1/70

Epoch 00011: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0211 - acc: 0.9945 - val_loss: 0.3256 - val_acc: 0.9226
Epoch 12/70
Epoch 1/70

Epoch 00012: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0154 - acc: 0.9966 - val_loss: 0.3182 - val_acc: 0.9204
Epoch 13/70
Epoch 1/70

Epoch 00013: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0092 - acc: 0.9984 - val_loss: 0.2946 - val_acc: 0.9338
Epoch 14/70
Epoch 1/70

Epoch 00014: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0065 - acc: 0.9992 - val_loss: 0.2964 - val_acc: 0.9282
Epoch 15/70
Epoch 1/70

Epoch 00015: val_loss did not improve from 0.24000
857/857 - 56s - loss: 0.0048 - acc: 0.9995 - val_loss: 0.2927 - val_acc: 0.9294
Epoch 16/70
Epoch 1/70

Epoch 00016: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0038 - acc: 0.9996 - val_loss: 0.3009 - val_acc: 0.9285
Epoch 17/70
Epoch 1/70

Epoch 00017: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0029 - acc: 0.9999 - val_loss: 0.3234 - val_acc: 0.9302
Epoch 18/70
Epoch 1/70

Epoch 00018: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0024 - acc: 0.9999 - val_loss: 0.2972 - val_acc: 0.9308
Epoch 19/70
Epoch 1/70

Epoch 00019: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0023 - acc: 0.9999 - val_loss: 0.2840 - val_acc: 0.9335
Epoch 20/70
Epoch 1/70

Epoch 00020: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0022 - acc: 0.9999 - val_loss: 0.2926 - val_acc: 0.9326
Epoch 21/70
Epoch 1/70

Epoch 00021: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0016 - acc: 1.0000 - val_loss: 0.2807 - val_acc: 0.9341
Epoch 22/70
Epoch 1/70

Epoch 00022: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0015 - acc: 1.0000 - val_loss: 0.2880 - val_acc: 0.9329
Epoch 23/70
Epoch 1/70

Epoch 00023: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0016 - acc: 0.9999 - val_loss: 0.2964 - val_acc: 0.9331
Epoch 24/70
Epoch 1/70

Epoch 00024: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.2867 - val_acc: 0.9374
Epoch 25/70
Epoch 1/70

Epoch 00025: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0013 - acc: 0.9999 - val_loss: 0.2957 - val_acc: 0.9311
Epoch 26/70
Epoch 1/70

Epoch 00026: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2969 - val_acc: 0.9344
Epoch 27/70
Epoch 1/70

Epoch 00027: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.3084 - val_acc: 0.9318
Epoch 28/70
Epoch 1/70

Epoch 00028: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0010 - acc: 1.0000 - val_loss: 0.3007 - val_acc: 0.9333
Epoch 29/70
Epoch 1/70

Epoch 00029: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.3122 - val_acc: 0.9295
Epoch 30/70
Epoch 1/70

Epoch 00030: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.3113 - val_acc: 0.9318
Epoch 31/70
Epoch 1/70

Epoch 00031: val_loss did not improve from 0.24000
857/857 - 57s - loss: 0.0010 - acc: 1.0000 - val_loss: 0.3035 - val_acc: 0.9323
Epoch 32/70
Epoch 1/70

Epoch 00032: val_loss did not improve from 0.24000
857/857 - 57s - loss: 9.2725e-04 - acc: 1.0000 - val_loss: 0.3043 - val_acc: 0.9316
Epoch 33/70
Epoch 1/70

Epoch 00033: val_loss did not improve from 0.24000
857/857 - 57s - loss: 9.1202e-04 - acc: 1.0000 - val_loss: 0.3055 - val_acc: 0.9342
Epoch 34/70
Epoch 1/70

Epoch 00034: val_loss did not improve from 0.24000
857/857 - 57s - loss: 8.2534e-04 - acc: 1.0000 - val_loss: 0.3291 - val_acc: 0.9347
Epoch 35/70
Epoch 1/70

Epoch 00035: val_loss did not improve from 0.24000
857/857 - 57s - loss: 8.0543e-04 - acc: 1.0000 - val_loss: 0.2939 - val_acc: 0.9345
Epoch 36/70
Epoch 1/70

Epoch 00036: val_loss did not improve from 0.24000
857/857 - 57s - loss: 7.3971e-04 - acc: 1.0000 - val_loss: 0.3055 - val_acc: 0.9338
Epoch 37/70
Epoch 1/70

Epoch 00037: val_loss did not improve from 0.24000
857/857 - 57s - loss: 7.9209e-04 - acc: 1.0000 - val_loss: 0.3110 - val_acc: 0.9311
Epoch 38/70
Epoch 1/70

Epoch 00038: val_loss did not improve from 0.24000
857/857 - 57s - loss: 7.7491e-04 - acc: 1.0000 - val_loss: 0.3092 - val_acc: 0.9358
Epoch 39/70
Epoch 1/70

Epoch 00039: val_loss did not improve from 0.24000
857/857 - 57s - loss: 7.7456e-04 - acc: 1.0000 - val_loss: 0.3025 - val_acc: 0.9333
Epoch 40/70
Epoch 1/70

Epoch 00040: val_loss did not improve from 0.24000
857/857 - 57s - loss: 7.1407e-04 - acc: 1.0000 - val_loss: 0.3082 - val_acc: 0.9312
Epoch 41/70
Epoch 1/70

Epoch 00041: val_loss did not improve from 0.24000
857/857 - 57s - loss: 6.8288e-04 - acc: 1.0000 - val_loss: 0.3112 - val_acc: 0.9333
Epoch 42/70
Epoch 1/70

Epoch 00042: val_loss did not improve from 0.24000
857/857 - 57s - loss: 6.5577e-04 - acc: 1.0000 - val_loss: 0.3095 - val_acc: 0.9333
Epoch 43/70
Epoch 1/70

Epoch 00043: val_loss did not improve from 0.24000
857/857 - 57s - loss: 6.7549e-04 - acc: 1.0000 - val_loss: 0.3091 - val_acc: 0.9331
Epoch 44/70
Epoch 1/70

Epoch 00044: val_loss did not improve from 0.24000
857/857 - 57s - loss: 6.5345e-04 - acc: 1.0000 - val_loss: 0.3116 - val_acc: 0.9345
Epoch 45/70
Epoch 1/70

Epoch 00045: val_loss did not improve from 0.24000
857/857 - 57s - loss: 6.1910e-04 - acc: 1.0000 - val_loss: 0.3081 - val_acc: 0.9349
Epoch 46/70
Epoch 1/70

Epoch 00046: val_loss did not improve from 0.24000
857/857 - 56s - loss: 6.0972e-04 - acc: 1.0000 - val_loss: 0.3261 - val_acc: 0.9339
Epoch 47/70
Epoch 1/70

Epoch 00047: val_loss did not improve from 0.24000
857/857 - 56s - loss: 6.0470e-04 - acc: 1.0000 - val_loss: 0.3030 - val_acc: 0.9324
Epoch 48/70
Epoch 1/70

Epoch 00048: val_loss did not improve from 0.24000
857/857 - 57s - loss: 6.2038e-04 - acc: 1.0000 - val_loss: 0.3068 - val_acc: 0.9328
Epoch 49/70
Epoch 1/70

Epoch 00049: val_loss did not improve from 0.24000
857/857 - 57s - loss: 5.9533e-04 - acc: 1.0000 - val_loss: 0.3405 - val_acc: 0.9334
Epoch 50/70
Epoch 1/70

Epoch 00050: val_loss did not improve from 0.24000
857/857 - 57s - loss: 5.4250e-04 - acc: 1.0000 - val_loss: 0.3074 - val_acc: 0.9344
Epoch 51/70
Epoch 1/70

Epoch 00051: val_loss did not improve from 0.24000
857/857 - 57s - loss: 5.3570e-04 - acc: 1.0000 - val_loss: 0.3028 - val_acc: 0.9333
Epoch 52/70
Epoch 1/70

Epoch 00052: val_loss did not improve from 0.24000
857/857 - 56s - loss: 5.6350e-04 - acc: 1.0000 - val_loss: 0.3142 - val_acc: 0.9341
Epoch 53/70
Epoch 1/70

Epoch 00053: val_loss did not improve from 0.24000
857/857 - 57s - loss: 5.3313e-04 - acc: 1.0000 - val_loss: 0.3255 - val_acc: 0.9334
Epoch 54/70
Epoch 1/70

Epoch 00054: val_loss did not improve from 0.24000
857/857 - 57s - loss: 4.9836e-04 - acc: 1.0000 - val_loss: 0.3375 - val_acc: 0.9332
Epoch 55/70
Epoch 1/70

Epoch 00055: val_loss did not improve from 0.24000
857/857 - 57s - loss: 5.3062e-04 - acc: 1.0000 - val_loss: 0.3158 - val_acc: 0.9319
Epoch 56/70
Epoch 1/70

Epoch 00056: val_loss did not improve from 0.24000
857/857 - 57s - loss: 5.0980e-04 - acc: 1.0000 - val_loss: 0.3190 - val_acc: 0.9314
Epoch 57/70
Epoch 1/70

Epoch 00057: val_loss did not improve from 0.24000
857/857 - 56s - loss: 4.9503e-04 - acc: 1.0000 - val_loss: 0.3121 - val_acc: 0.9331
Epoch 58/70
Epoch 1/70

Epoch 00058: val_loss did not improve from 0.24000
857/857 - 56s - loss: 4.8228e-04 - acc: 1.0000 - val_loss: 0.3155 - val_acc: 0.9333
Epoch 59/70
Epoch 1/70

Epoch 00059: val_loss did not improve from 0.24000
857/857 - 57s - loss: 5.2777e-04 - acc: 1.0000 - val_loss: 0.3103 - val_acc: 0.9345
Epoch 60/70
Epoch 1/70

Epoch 00060: val_loss did not improve from 0.24000
857/857 - 57s - loss: 5.2514e-04 - acc: 1.0000 - val_loss: 0.3074 - val_acc: 0.9340
Epoch 61/70
Epoch 1/70

Epoch 00061: val_loss did not improve from 0.24000
857/857 - 57s - loss: 4.7855e-04 - acc: 1.0000 - val_loss: 0.3247 - val_acc: 0.9326
Epoch 62/70
Epoch 1/70

Epoch 00062: val_loss did not improve from 0.24000
857/857 - 57s - loss: 5.0936e-04 - acc: 1.0000 - val_loss: 0.3109 - val_acc: 0.9338
Epoch 63/70
Epoch 1/70

Epoch 00063: val_loss did not improve from 0.24000
857/857 - 57s - loss: 4.8792e-04 - acc: 1.0000 - val_loss: 0.3225 - val_acc: 0.9351
Epoch 64/70
Epoch 1/70

Epoch 00064: val_loss did not improve from 0.24000
857/857 - 57s - loss: 4.4521e-04 - acc: 1.0000 - val_loss: 0.3204 - val_acc: 0.9332
Epoch 65/70
Epoch 1/70

Epoch 00065: val_loss did not improve from 0.24000
857/857 - 56s - loss: 4.3870e-04 - acc: 1.0000 - val_loss: 0.3128 - val_acc: 0.9344
Epoch 66/70
Epoch 1/70

Epoch 00066: val_loss did not improve from 0.24000
857/857 - 57s - loss: 4.3435e-04 - acc: 1.0000 - val_loss: 0.3221 - val_acc: 0.9319
Epoch 67/70
Epoch 1/70

Epoch 00067: val_loss did not improve from 0.24000
857/857 - 56s - loss: 4.5425e-04 - acc: 1.0000 - val_loss: 0.3146 - val_acc: 0.9335
Epoch 68/70
Epoch 1/70

Epoch 00068: val_loss did not improve from 0.24000
857/857 - 57s - loss: 4.3627e-04 - acc: 1.0000 - val_loss: 0.3229 - val_acc: 0.9316
Epoch 69/70
Epoch 1/70

Epoch 00069: val_loss did not improve from 0.24000
857/857 - 57s - loss: 4.5904e-04 - acc: 1.0000 - val_loss: 0.3221 - val_acc: 0.9330
Epoch 70/70
Epoch 1/70

Epoch 00070: val_loss did not improve from 0.24000
857/857 - 57s - loss: 4.4049e-04 - acc: 1.0000 - val_loss: 0.3104 - val_acc: 0.9330


Elapsed time for Keras training (s):  3999.417234


[INFO] evaluating network on Test and Validation datasets...
 128/5000 [..............................] - ETA: 0s - loss: 0.3937 - acc: 0.8984 512/5000 [==>...........................] - ETA: 0s - loss: 0.3399 - acc: 0.9219 896/5000 [====>.........................] - ETA: 0s - loss: 0.4073 - acc: 0.91521280/5000 [======>.......................] - ETA: 0s - loss: 0.3759 - acc: 0.92421664/5000 [========>.....................] - ETA: 0s - loss: 0.3999 - acc: 0.92072048/5000 [===========>..................] - ETA: 0s - loss: 0.3833 - acc: 0.92432432/5000 [=============>................] - ETA: 0s - loss: 0.3715 - acc: 0.92522816/5000 [===============>..............] - ETA: 0s - loss: 0.3661 - acc: 0.92583200/5000 [==================>...........] - ETA: 0s - loss: 0.3450 - acc: 0.92783584/5000 [====================>.........] - ETA: 0s - loss: 0.3431 - acc: 0.92913968/5000 [======================>.......] - ETA: 0s - loss: 0.3305 - acc: 0.92944352/5000 [=========================>....] - ETA: 0s - loss: 0.3198 - acc: 0.93014736/5000 [===========================>..] - ETA: 0s - loss: 0.3150 - acc: 0.93185000/5000 [==============================] - 1s 148us/sample - loss: 0.3190 - acc: 0.9324
Validation Loss: 0.319
validation Accuracy: 0.932
 128/5000 [..............................] - ETA: 0s - loss: 0.3572 - acc: 0.9453 512/5000 [==>...........................] - ETA: 0s - loss: 0.4078 - acc: 0.9297 896/5000 [====>.........................] - ETA: 0s - loss: 0.3810 - acc: 0.92861280/5000 [======>.......................] - ETA: 0s - loss: 0.3808 - acc: 0.92341664/5000 [========>.....................] - ETA: 0s - loss: 0.3804 - acc: 0.92432048/5000 [===========>..................] - ETA: 0s - loss: 0.3601 - acc: 0.92722432/5000 [=============>................] - ETA: 0s - loss: 0.3603 - acc: 0.92802816/5000 [===============>..............] - ETA: 0s - loss: 0.3661 - acc: 0.92653200/5000 [==================>...........] - ETA: 0s - loss: 0.3619 - acc: 0.92813584/5000 [====================>.........] - ETA: 0s - loss: 0.3530 - acc: 0.92943968/5000 [======================>.......] - ETA: 0s - loss: 0.3547 - acc: 0.92944352/5000 [=========================>....] - ETA: 0s - loss: 0.3609 - acc: 0.92834736/5000 [===========================>..] - ETA: 0s - loss: 0.3696 - acc: 0.92575000/5000 [==============================] - 1s 150us/sample - loss: 0.3609 - acc: 0.9270
Test Loss: 0.361
Test Accuracy: 0.927
              precision    recall  f1-score   support

         top       0.86      0.88      0.87       500
     trouser       1.00      0.99      0.99       500
    pullover       0.90      0.90      0.90       500
       dress       0.93      0.93      0.93       500
        coat       0.88      0.88      0.88       500
      sandal       0.99      0.98      0.98       500
       shirt       0.78      0.77      0.78       500
     sneaker       0.95      0.99      0.97       500
         bag       0.99      0.99      0.99       500
   ankleBoot       0.99      0.97      0.98       500

    accuracy                           0.93      5000
   macro avg       0.93      0.93      0.93      5000
weighted avg       0.93      0.93      0.93      5000


TRAINING miniGoogleNet FINISHED

 
##########################################################################
TRAIN & EVAL miniResNet  on FMNIST
##########################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2022-06-09 12:35:14.642086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 12:35:14.690114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 12:35:14.690398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 12:35:14.691784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 12:35:14.692977: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 12:35:14.693326: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 12:35:14.694752: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 12:35:14.695791: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 12:35:14.698994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 12:35:14.701092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 12:35:14.701645: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 12:35:14.722158: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 12:35:14.724495: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e7f887bc40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 12:35:14.724527: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 12:35:14.813606: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e7f8c74a30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 12:35:14.813648: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 12:35:14.816238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 12:35:14.816326: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 12:35:14.816354: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 12:35:14.816378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 12:35:14.816403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 12:35:14.816426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 12:35:14.816450: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 12:35:14.816475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 12:35:14.820747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 12:35:14.820850: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 12:35:14.824798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 12:35:14.824815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 12:35:14.824824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 12:35:14.830454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22485 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
2022-06-09 12:35:29.795213: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 12:35:29.909722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 12:35:30.422540: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
[INFO] compiling model...
[INFO] training model...
Epoch 1/100
Epoch 1/100

Epoch 00001: val_loss improved from inf to 0.94221, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 142s - loss: 1.2159 - acc: 0.7601 - val_loss: 0.9422 - val_acc: 0.8293
Epoch 2/100
Epoch 1/100

Epoch 00002: val_loss improved from 0.94221 to 0.70112, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 95s - loss: 0.8259 - acc: 0.8578 - val_loss: 0.7011 - val_acc: 0.8908
Epoch 3/100
Epoch 1/100

Epoch 00003: val_loss improved from 0.70112 to 0.69525, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.6953 - acc: 0.8760 - val_loss: 0.6953 - val_acc: 0.8791
Epoch 4/100
Epoch 1/100

Epoch 00004: val_loss improved from 0.69525 to 0.57864, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.6144 - acc: 0.8863 - val_loss: 0.5786 - val_acc: 0.8949
Epoch 5/100
Epoch 1/100

Epoch 00005: val_loss improved from 0.57864 to 0.52008, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.5597 - acc: 0.8911 - val_loss: 0.5201 - val_acc: 0.8917
Epoch 6/100
Epoch 1/100

Epoch 00006: val_loss did not improve from 0.52008
600/600 - 93s - loss: 0.5198 - acc: 0.8934 - val_loss: 0.5550 - val_acc: 0.8760
Epoch 7/100
Epoch 1/100

Epoch 00007: val_loss improved from 0.52008 to 0.48455, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 95s - loss: 0.4863 - acc: 0.8984 - val_loss: 0.4845 - val_acc: 0.8976
Epoch 8/100
Epoch 1/100

Epoch 00008: val_loss did not improve from 0.48455
600/600 - 93s - loss: 0.4601 - acc: 0.9008 - val_loss: 0.5146 - val_acc: 0.8809
Epoch 9/100
Epoch 1/100

Epoch 00009: val_loss did not improve from 0.48455
600/600 - 94s - loss: 0.4427 - acc: 0.9011 - val_loss: 0.5101 - val_acc: 0.8839
Epoch 10/100
Epoch 1/100

Epoch 00010: val_loss improved from 0.48455 to 0.45632, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 95s - loss: 0.4226 - acc: 0.9030 - val_loss: 0.4563 - val_acc: 0.8849
Epoch 11/100
Epoch 1/100

Epoch 00011: val_loss improved from 0.45632 to 0.40823, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.4062 - acc: 0.9061 - val_loss: 0.4082 - val_acc: 0.9051
Epoch 12/100
Epoch 1/100

Epoch 00012: val_loss improved from 0.40823 to 0.37714, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 95s - loss: 0.3978 - acc: 0.9057 - val_loss: 0.3771 - val_acc: 0.9116
Epoch 13/100
Epoch 1/100

Epoch 00013: val_loss did not improve from 0.37714
600/600 - 93s - loss: 0.3917 - acc: 0.9069 - val_loss: 0.4378 - val_acc: 0.8916
Epoch 14/100
Epoch 1/100

Epoch 00014: val_loss improved from 0.37714 to 0.36500, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 95s - loss: 0.3849 - acc: 0.9083 - val_loss: 0.3650 - val_acc: 0.9137
Epoch 15/100
Epoch 1/100

Epoch 00015: val_loss did not improve from 0.36500
600/600 - 93s - loss: 0.3770 - acc: 0.9094 - val_loss: 0.3667 - val_acc: 0.9105
Epoch 16/100
Epoch 1/100

Epoch 00016: val_loss did not improve from 0.36500
600/600 - 93s - loss: 0.3685 - acc: 0.9117 - val_loss: 0.3749 - val_acc: 0.9048
Epoch 17/100
Epoch 1/100

Epoch 00017: val_loss did not improve from 0.36500
600/600 - 93s - loss: 0.3642 - acc: 0.9128 - val_loss: 0.3794 - val_acc: 0.9091
Epoch 18/100
Epoch 1/100

Epoch 00018: val_loss did not improve from 0.36500
600/600 - 94s - loss: 0.3597 - acc: 0.9117 - val_loss: 0.3749 - val_acc: 0.9081
Epoch 19/100
Epoch 1/100

Epoch 00019: val_loss improved from 0.36500 to 0.34187, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.3527 - acc: 0.9140 - val_loss: 0.3419 - val_acc: 0.9193
Epoch 20/100
Epoch 1/100

Epoch 00020: val_loss improved from 0.34187 to 0.34095, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.3487 - acc: 0.9158 - val_loss: 0.3409 - val_acc: 0.9159
Epoch 21/100
Epoch 1/100

Epoch 00021: val_loss did not improve from 0.34095
600/600 - 93s - loss: 0.3453 - acc: 0.9158 - val_loss: 0.3654 - val_acc: 0.9129
Epoch 22/100
Epoch 1/100

Epoch 00022: val_loss did not improve from 0.34095
600/600 - 93s - loss: 0.3431 - acc: 0.9164 - val_loss: 0.4222 - val_acc: 0.8930
Epoch 23/100
Epoch 1/100

Epoch 00023: val_loss did not improve from 0.34095
600/600 - 94s - loss: 0.3365 - acc: 0.9186 - val_loss: 0.3688 - val_acc: 0.9124
Epoch 24/100
Epoch 1/100

Epoch 00024: val_loss improved from 0.34095 to 0.33216, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.3338 - acc: 0.9187 - val_loss: 0.3322 - val_acc: 0.9169
Epoch 25/100
Epoch 1/100

Epoch 00025: val_loss improved from 0.33216 to 0.32007, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.3370 - acc: 0.9179 - val_loss: 0.3201 - val_acc: 0.9279
Epoch 26/100
Epoch 1/100

Epoch 00026: val_loss did not improve from 0.32007
600/600 - 93s - loss: 0.3316 - acc: 0.9194 - val_loss: 0.3456 - val_acc: 0.9142
Epoch 27/100
Epoch 1/100

Epoch 00027: val_loss did not improve from 0.32007
600/600 - 93s - loss: 0.3260 - acc: 0.9209 - val_loss: 0.3403 - val_acc: 0.9255
Epoch 28/100
Epoch 1/100

Epoch 00028: val_loss did not improve from 0.32007
600/600 - 94s - loss: 0.3258 - acc: 0.9228 - val_loss: 0.3524 - val_acc: 0.9180
Epoch 29/100
Epoch 1/100

Epoch 00029: val_loss improved from 0.32007 to 0.30726, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.3212 - acc: 0.9227 - val_loss: 0.3073 - val_acc: 0.9336
Epoch 30/100
Epoch 1/100

Epoch 00030: val_loss did not improve from 0.30726
600/600 - 94s - loss: 0.3207 - acc: 0.9227 - val_loss: 0.3785 - val_acc: 0.9084
Epoch 31/100
Epoch 1/100

Epoch 00031: val_loss did not improve from 0.30726
600/600 - 93s - loss: 0.3147 - acc: 0.9241 - val_loss: 0.3602 - val_acc: 0.9035
Epoch 32/100
Epoch 1/100

Epoch 00032: val_loss did not improve from 0.30726
600/600 - 92s - loss: 0.3187 - acc: 0.9238 - val_loss: 0.4016 - val_acc: 0.8854
Epoch 33/100
Epoch 1/100

Epoch 00033: val_loss did not improve from 0.30726
600/600 - 93s - loss: 0.3108 - acc: 0.9260 - val_loss: 0.3404 - val_acc: 0.9193
Epoch 34/100
Epoch 1/100

Epoch 00034: val_loss did not improve from 0.30726
600/600 - 94s - loss: 0.3121 - acc: 0.9263 - val_loss: 0.3511 - val_acc: 0.9075
Epoch 35/100
Epoch 1/100

Epoch 00035: val_loss did not improve from 0.30726
600/600 - 94s - loss: 0.3062 - acc: 0.9275 - val_loss: 0.3484 - val_acc: 0.9123
Epoch 36/100
Epoch 1/100

Epoch 00036: val_loss did not improve from 0.30726
600/600 - 96s - loss: 0.3049 - acc: 0.9282 - val_loss: 0.3170 - val_acc: 0.9204
Epoch 37/100
Epoch 1/100

Epoch 00037: val_loss did not improve from 0.30726
600/600 - 95s - loss: 0.3052 - acc: 0.9273 - val_loss: 0.3273 - val_acc: 0.9164
Epoch 38/100
Epoch 1/100

Epoch 00038: val_loss improved from 0.30726 to 0.29405, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 97s - loss: 0.3018 - acc: 0.9285 - val_loss: 0.2940 - val_acc: 0.9339
Epoch 39/100
Epoch 1/100

Epoch 00039: val_loss did not improve from 0.29405
600/600 - 97s - loss: 0.2991 - acc: 0.9296 - val_loss: 0.3188 - val_acc: 0.9264
Epoch 40/100
Epoch 1/100

Epoch 00040: val_loss did not improve from 0.29405
600/600 - 97s - loss: 0.2969 - acc: 0.9305 - val_loss: 0.3239 - val_acc: 0.9193
Epoch 41/100
Epoch 1/100

Epoch 00041: val_loss did not improve from 0.29405
600/600 - 96s - loss: 0.2959 - acc: 0.9308 - val_loss: 0.4056 - val_acc: 0.8895
Epoch 42/100
Epoch 1/100

Epoch 00042: val_loss did not improve from 0.29405
600/600 - 96s - loss: 0.2950 - acc: 0.9302 - val_loss: 0.3088 - val_acc: 0.9282
Epoch 43/100
Epoch 1/100

Epoch 00043: val_loss did not improve from 0.29405
600/600 - 93s - loss: 0.2922 - acc: 0.9309 - val_loss: 0.3275 - val_acc: 0.9175
Epoch 44/100
Epoch 1/100

Epoch 00044: val_loss did not improve from 0.29405
600/600 - 93s - loss: 0.2917 - acc: 0.9309 - val_loss: 0.3164 - val_acc: 0.9275
Epoch 45/100
Epoch 1/100

Epoch 00045: val_loss did not improve from 0.29405
600/600 - 93s - loss: 0.2881 - acc: 0.9324 - val_loss: 0.3008 - val_acc: 0.9298
Epoch 46/100
Epoch 1/100

Epoch 00046: val_loss improved from 0.29405 to 0.29265, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.2885 - acc: 0.9321 - val_loss: 0.2927 - val_acc: 0.9373
Epoch 47/100
Epoch 1/100

Epoch 00047: val_loss did not improve from 0.29265
600/600 - 93s - loss: 0.2842 - acc: 0.9341 - val_loss: 0.3064 - val_acc: 0.9240
Epoch 48/100
Epoch 1/100

Epoch 00048: val_loss improved from 0.29265 to 0.27609, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.2817 - acc: 0.9343 - val_loss: 0.2761 - val_acc: 0.9373
Epoch 49/100
Epoch 1/100

Epoch 00049: val_loss did not improve from 0.27609
600/600 - 93s - loss: 0.2788 - acc: 0.9354 - val_loss: 0.3600 - val_acc: 0.9142
Epoch 50/100
Epoch 1/100

Epoch 00050: val_loss did not improve from 0.27609
600/600 - 94s - loss: 0.2756 - acc: 0.9373 - val_loss: 0.2867 - val_acc: 0.9331
Epoch 51/100
Epoch 1/100

Epoch 00051: val_loss did not improve from 0.27609
600/600 - 94s - loss: 0.2764 - acc: 0.9362 - val_loss: 0.2969 - val_acc: 0.9304
Epoch 52/100
Epoch 1/100

Epoch 00052: val_loss did not improve from 0.27609
600/600 - 94s - loss: 0.2755 - acc: 0.9358 - val_loss: 0.2961 - val_acc: 0.9290
Epoch 53/100
Epoch 1/100

Epoch 00053: val_loss did not improve from 0.27609
600/600 - 93s - loss: 0.2709 - acc: 0.9369 - val_loss: 0.2916 - val_acc: 0.9354
Epoch 54/100
Epoch 1/100

Epoch 00054: val_loss did not improve from 0.27609
600/600 - 93s - loss: 0.2690 - acc: 0.9377 - val_loss: 0.2785 - val_acc: 0.9363
Epoch 55/100
Epoch 1/100

Epoch 00055: val_loss did not improve from 0.27609
600/600 - 93s - loss: 0.2642 - acc: 0.9402 - val_loss: 0.3215 - val_acc: 0.9282
Epoch 56/100
Epoch 1/100

Epoch 00056: val_loss did not improve from 0.27609
600/600 - 93s - loss: 0.2709 - acc: 0.9369 - val_loss: 0.2808 - val_acc: 0.9354
Epoch 57/100
Epoch 1/100

Epoch 00057: val_loss did not improve from 0.27609
600/600 - 93s - loss: 0.2635 - acc: 0.9388 - val_loss: 0.2999 - val_acc: 0.9299
Epoch 58/100
Epoch 1/100

Epoch 00058: val_loss did not improve from 0.27609
600/600 - 94s - loss: 0.2596 - acc: 0.9406 - val_loss: 0.2956 - val_acc: 0.9318
Epoch 59/100
Epoch 1/100

Epoch 00059: val_loss did not improve from 0.27609
600/600 - 94s - loss: 0.2598 - acc: 0.9400 - val_loss: 0.3147 - val_acc: 0.9229
Epoch 60/100
Epoch 1/100

Epoch 00060: val_loss improved from 0.27609 to 0.27406, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.2578 - acc: 0.9405 - val_loss: 0.2741 - val_acc: 0.9389
Epoch 61/100
Epoch 1/100

Epoch 00061: val_loss did not improve from 0.27406
600/600 - 92s - loss: 0.2552 - acc: 0.9415 - val_loss: 0.3265 - val_acc: 0.9232
Epoch 62/100
Epoch 1/100

Epoch 00062: val_loss did not improve from 0.27406
600/600 - 93s - loss: 0.2511 - acc: 0.9438 - val_loss: 0.2743 - val_acc: 0.9334
Epoch 63/100
Epoch 1/100

Epoch 00063: val_loss did not improve from 0.27406
600/600 - 93s - loss: 0.2501 - acc: 0.9430 - val_loss: 0.2763 - val_acc: 0.9392
Epoch 64/100
Epoch 1/100

Epoch 00064: val_loss did not improve from 0.27406
600/600 - 93s - loss: 0.2508 - acc: 0.9431 - val_loss: 0.2950 - val_acc: 0.9295
Epoch 65/100
Epoch 1/100

Epoch 00065: val_loss did not improve from 0.27406
600/600 - 93s - loss: 0.2430 - acc: 0.9455 - val_loss: 0.3036 - val_acc: 0.9296
Epoch 66/100
Epoch 1/100

Epoch 00066: val_loss improved from 0.27406 to 0.27159, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.2426 - acc: 0.9450 - val_loss: 0.2716 - val_acc: 0.9369
Epoch 67/100
Epoch 1/100

Epoch 00067: val_loss improved from 0.27159 to 0.26382, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 94s - loss: 0.2416 - acc: 0.9447 - val_loss: 0.2638 - val_acc: 0.9369
Epoch 68/100
Epoch 1/100

Epoch 00068: val_loss did not improve from 0.26382
600/600 - 93s - loss: 0.2401 - acc: 0.9461 - val_loss: 0.2807 - val_acc: 0.9379
Epoch 69/100
Epoch 1/100

Epoch 00069: val_loss did not improve from 0.26382
600/600 - 94s - loss: 0.2346 - acc: 0.9467 - val_loss: 0.2695 - val_acc: 0.9416
Epoch 70/100
Epoch 1/100

Epoch 00070: val_loss improved from 0.26382 to 0.26345, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 95s - loss: 0.2329 - acc: 0.9481 - val_loss: 0.2635 - val_acc: 0.9368
Epoch 71/100
Epoch 1/100

Epoch 00071: val_loss did not improve from 0.26345
600/600 - 93s - loss: 0.2325 - acc: 0.9483 - val_loss: 0.3286 - val_acc: 0.9212
Epoch 72/100
Epoch 1/100

Epoch 00072: val_loss did not improve from 0.26345
600/600 - 93s - loss: 0.2288 - acc: 0.9494 - val_loss: 0.2707 - val_acc: 0.9400
Epoch 73/100
Epoch 1/100

Epoch 00073: val_loss did not improve from 0.26345
600/600 - 94s - loss: 0.2251 - acc: 0.9502 - val_loss: 0.2704 - val_acc: 0.9381
Epoch 74/100
Epoch 1/100

Epoch 00074: val_loss did not improve from 0.26345
600/600 - 92s - loss: 0.2227 - acc: 0.9510 - val_loss: 0.2784 - val_acc: 0.9376
Epoch 75/100
Epoch 1/100

Epoch 00075: val_loss did not improve from 0.26345
600/600 - 95s - loss: 0.2230 - acc: 0.9508 - val_loss: 0.2832 - val_acc: 0.9326
Epoch 76/100
Epoch 1/100

Epoch 00076: val_loss did not improve from 0.26345
600/600 - 96s - loss: 0.2148 - acc: 0.9536 - val_loss: 0.3067 - val_acc: 0.9282
Epoch 77/100
Epoch 1/100

Epoch 00077: val_loss improved from 0.26345 to 0.25267, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 97s - loss: 0.2135 - acc: 0.9535 - val_loss: 0.2527 - val_acc: 0.9467
Epoch 78/100
Epoch 1/100

Epoch 00078: val_loss did not improve from 0.25267
600/600 - 94s - loss: 0.2141 - acc: 0.9534 - val_loss: 0.2556 - val_acc: 0.9384
Epoch 79/100
Epoch 1/100

Epoch 00079: val_loss did not improve from 0.25267
600/600 - 94s - loss: 0.2108 - acc: 0.9541 - val_loss: 0.2800 - val_acc: 0.9326
Epoch 80/100
Epoch 1/100

Epoch 00080: val_loss did not improve from 0.25267
600/600 - 94s - loss: 0.2043 - acc: 0.9564 - val_loss: 0.2848 - val_acc: 0.9315
Epoch 81/100
Epoch 1/100

Epoch 00081: val_loss improved from 0.25267 to 0.24560, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 96s - loss: 0.2026 - acc: 0.9567 - val_loss: 0.2456 - val_acc: 0.9471
Epoch 82/100
Epoch 1/100

Epoch 00082: val_loss did not improve from 0.24560
600/600 - 94s - loss: 0.2005 - acc: 0.9575 - val_loss: 0.2873 - val_acc: 0.9298
Epoch 83/100
Epoch 1/100

Epoch 00083: val_loss did not improve from 0.24560
600/600 - 94s - loss: 0.1943 - acc: 0.9589 - val_loss: 0.2656 - val_acc: 0.9379
Epoch 84/100
Epoch 1/100

Epoch 00084: val_loss did not improve from 0.24560
600/600 - 94s - loss: 0.1926 - acc: 0.9599 - val_loss: 0.2613 - val_acc: 0.9422
Epoch 85/100
Epoch 1/100

Epoch 00085: val_loss improved from 0.24560 to 0.23927, saving model to keras_model/fmnist/miniResNet/best_chkpt.hdf5
600/600 - 95s - loss: 0.1859 - acc: 0.9622 - val_loss: 0.2393 - val_acc: 0.9468
Epoch 86/100
Epoch 1/100

Epoch 00086: val_loss did not improve from 0.23927
600/600 - 95s - loss: 0.1862 - acc: 0.9612 - val_loss: 0.2866 - val_acc: 0.9338
Epoch 87/100
Epoch 1/100

Epoch 00087: val_loss did not improve from 0.23927
600/600 - 93s - loss: 0.1797 - acc: 0.9640 - val_loss: 0.2553 - val_acc: 0.9452
Epoch 88/100
Epoch 1/100

Epoch 00088: val_loss did not improve from 0.23927
600/600 - 94s - loss: 0.1774 - acc: 0.9641 - val_loss: 0.2673 - val_acc: 0.9417
Epoch 89/100
Epoch 1/100

Epoch 00089: val_loss did not improve from 0.23927
600/600 - 94s - loss: 0.1722 - acc: 0.9660 - val_loss: 0.2654 - val_acc: 0.9409
Epoch 90/100
Epoch 1/100

Epoch 00090: val_loss did not improve from 0.23927
600/600 - 93s - loss: 0.1681 - acc: 0.9669 - val_loss: 0.2907 - val_acc: 0.9339
Epoch 91/100
Epoch 1/100

Epoch 00091: val_loss did not improve from 0.23927
600/600 - 95s - loss: 0.1643 - acc: 0.9681 - val_loss: 0.2609 - val_acc: 0.9387
Epoch 92/100
Epoch 1/100

Epoch 00092: val_loss did not improve from 0.23927
600/600 - 94s - loss: 0.1601 - acc: 0.9702 - val_loss: 0.2772 - val_acc: 0.9368
Epoch 93/100
Epoch 1/100

Epoch 00093: val_loss did not improve from 0.23927
600/600 - 94s - loss: 0.1542 - acc: 0.9709 - val_loss: 0.2739 - val_acc: 0.9404
Epoch 94/100
Epoch 1/100

Epoch 00094: val_loss did not improve from 0.23927
600/600 - 95s - loss: 0.1512 - acc: 0.9728 - val_loss: 0.2558 - val_acc: 0.9451
Epoch 95/100
Epoch 1/100

Epoch 00095: val_loss did not improve from 0.23927
600/600 - 94s - loss: 0.1470 - acc: 0.9735 - val_loss: 0.2640 - val_acc: 0.9452
Epoch 96/100
Epoch 1/100

Epoch 00096: val_loss did not improve from 0.23927
600/600 - 94s - loss: 0.1396 - acc: 0.9766 - val_loss: 0.2508 - val_acc: 0.9486
Epoch 97/100
Epoch 1/100

Epoch 00097: val_loss did not improve from 0.23927
600/600 - 93s - loss: 0.1340 - acc: 0.9788 - val_loss: 0.2457 - val_acc: 0.9506
Epoch 98/100
Epoch 1/100

Epoch 00098: val_loss did not improve from 0.23927
600/600 - 95s - loss: 0.1300 - acc: 0.9799 - val_loss: 0.2689 - val_acc: 0.9441
Epoch 99/100
Epoch 1/100

Epoch 00099: val_loss did not improve from 0.23927
600/600 - 94s - loss: 0.1246 - acc: 0.9821 - val_loss: 0.2608 - val_acc: 0.9471
Epoch 100/100
Epoch 1/100

Epoch 00100: val_loss did not improve from 0.23927
600/600 - 94s - loss: 0.1194 - acc: 0.9834 - val_loss: 0.2645 - val_acc: 0.9473


Elapsed time for Keras training (s):  9450.026694


[INFO] evaluating network on Test and Validation datasets...
 128/5000 [..............................] - ETA: 1s - loss: 0.2860 - acc: 0.9219 384/5000 [=>............................] - ETA: 1s - loss: 0.2890 - acc: 0.9427 640/5000 [==>...........................] - ETA: 1s - loss: 0.3425 - acc: 0.9344 896/5000 [====>.........................] - ETA: 1s - loss: 0.3370 - acc: 0.93531152/5000 [=====>........................] - ETA: 1s - loss: 0.3158 - acc: 0.93751408/5000 [=======>......................] - ETA: 1s - loss: 0.3023 - acc: 0.93751664/5000 [========>.....................] - ETA: 0s - loss: 0.3132 - acc: 0.93571920/5000 [==========>...................] - ETA: 0s - loss: 0.3023 - acc: 0.93962176/5000 [============>.................] - ETA: 0s - loss: 0.2959 - acc: 0.93932432/5000 [=============>................] - ETA: 0s - loss: 0.2847 - acc: 0.94162688/5000 [===============>..............] - ETA: 0s - loss: 0.2776 - acc: 0.94272944/5000 [================>.............] - ETA: 0s - loss: 0.2802 - acc: 0.94333200/5000 [==================>...........] - ETA: 0s - loss: 0.2732 - acc: 0.94503456/5000 [===================>..........] - ETA: 0s - loss: 0.2710 - acc: 0.94593712/5000 [=====================>........] - ETA: 0s - loss: 0.2725 - acc: 0.94693968/5000 [======================>.......] - ETA: 0s - loss: 0.2689 - acc: 0.94764224/5000 [========================>.....] - ETA: 0s - loss: 0.2643 - acc: 0.94844480/5000 [=========================>....] - ETA: 0s - loss: 0.2570 - acc: 0.94984736/5000 [===========================>..] - ETA: 0s - loss: 0.2559 - acc: 0.94974992/5000 [============================>.] - ETA: 0s - loss: 0.2538 - acc: 0.95015000/5000 [==============================] - 2s 308us/sample - loss: 0.2538 - acc: 0.9500
Validation Loss: 0.254
validation Accuracy: 0.950
 128/5000 [..............................] - ETA: 1s - loss: 0.3266 - acc: 0.9375 384/5000 [=>............................] - ETA: 1s - loss: 0.2981 - acc: 0.9453 640/5000 [==>...........................] - ETA: 1s - loss: 0.2703 - acc: 0.9500 896/5000 [====>.........................] - ETA: 1s - loss: 0.2863 - acc: 0.94981152/5000 [=====>........................] - ETA: 1s - loss: 0.3018 - acc: 0.94361408/5000 [=======>......................] - ETA: 1s - loss: 0.2941 - acc: 0.94461664/5000 [========>.....................] - ETA: 0s - loss: 0.2960 - acc: 0.94291920/5000 [==========>...................] - ETA: 0s - loss: 0.2826 - acc: 0.94582176/5000 [============>.................] - ETA: 0s - loss: 0.2897 - acc: 0.94442432/5000 [=============>................] - ETA: 0s - loss: 0.2848 - acc: 0.94412688/5000 [===============>..............] - ETA: 0s - loss: 0.2883 - acc: 0.94082944/5000 [================>.............] - ETA: 0s - loss: 0.2874 - acc: 0.94233200/5000 [==================>...........] - ETA: 0s - loss: 0.2845 - acc: 0.94313456/5000 [===================>..........] - ETA: 0s - loss: 0.2817 - acc: 0.94273712/5000 [=====================>........] - ETA: 0s - loss: 0.2759 - acc: 0.94373968/5000 [======================>.......] - ETA: 0s - loss: 0.2738 - acc: 0.94384224/5000 [========================>.....] - ETA: 0s - loss: 0.2771 - acc: 0.94344480/5000 [=========================>....] - ETA: 0s - loss: 0.2742 - acc: 0.94464736/5000 [===========================>..] - ETA: 0s - loss: 0.2806 - acc: 0.94344992/5000 [============================>.] - ETA: 0s - loss: 0.2798 - acc: 0.94315000/5000 [==============================] - 2s 309us/sample - loss: 0.2795 - acc: 0.9432
Test Loss: 0.279
Test Accuracy: 0.943
              precision    recall  f1-score   support

         top       0.89      0.89      0.89       500
     trouser       1.00      1.00      1.00       500
    pullover       0.94      0.89      0.92       500
       dress       0.98      0.89      0.93       500
        coat       0.92      0.95      0.93       500
      sandal       0.99      0.99      0.99       500
       shirt       0.80      0.88      0.84       500
     sneaker       0.97      0.97      0.97       500
         bag       0.99      1.00      0.99       500
   ankleBoot       0.97      0.97      0.97       500

    accuracy                           0.94      5000
   macro avg       0.94      0.94      0.94      5000
weighted avg       0.94      0.94      0.94      5000


TRAINING miniResNet FINISHED

 
############################################################################################
KERAS to TENSORFLOW GRAPH CONVERSION for LeNet on FMNIST
############################################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From code/Keras2TF.py:73: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.

Model: "LeNet"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1_input (InputLayer)  [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 32, 50)        3800      
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 32, 50)        200       
_________________________________________________________________
activation_1 (Activation)    (None, 32, 32, 50)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 16, 16, 50)        0         
_________________________________________________________________
flatten (Flatten)            (None, 12800)             0         
_________________________________________________________________
dense (Dense)                (None, 500)               6400500   
_________________________________________________________________
activation_2 (Activation)    (None, 500)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                5010      
_________________________________________________________________
activation_3 (Activation)    (None, 10)                0         
=================================================================
Total params: 6,409,510
Trainable params: 6,409,410
Non-trainable params: 100
_________________________________________________________________

 TF input node name:
[<tf.Tensor 'conv2d_1_input:0' shape=(?, 32, 32, 3) dtype=float32>]

 TF output node name:
[<tf.Tensor 'activation_3/Softmax:0' shape=(?, 10) dtype=float32>]

FINISHED CREATING TF FILES

 
#################################################################################################
KERAS to TENSORFLOW GRAPH CONVERSION for miniVggNet  on FMNIST
#################################################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From code/Keras2TF.py:73: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.

Model: "miniVggNet"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1_input (InputLayer)  [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 32, 32, 32)        864       
_________________________________________________________________
batch_normalization (BatchNo (None, 32, 32, 32)        128       
_________________________________________________________________
activation (Activation)      (None, 32, 32, 32)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 32, 32, 32)        9216      
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
activation_1 (Activation)    (None, 32, 32, 32)        0         
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         
_________________________________________________________________
dropout (Dropout)            (None, 16, 16, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 16, 16, 64)        18432     
_________________________________________________________________
batch_normalization_2 (Batch (None, 16, 16, 64)        256       
_________________________________________________________________
activation_2 (Activation)    (None, 16, 16, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 16, 16, 64)        36864     
_________________________________________________________________
batch_normalization_3 (Batch (None, 16, 16, 64)        256       
_________________________________________________________________
activation_3 (Activation)    (None, 16, 16, 64)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 8, 8, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 4096)              0         
_________________________________________________________________
dense (Dense)                (None, 512)               2097664   
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
activation_4 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                5130      
_________________________________________________________________
activation_5 (Activation)    (None, 10)                0         
=================================================================
Total params: 2,170,986
Trainable params: 2,169,578
Non-trainable params: 1,408
_________________________________________________________________

 TF input node name:
[<tf.Tensor 'conv2d_1_input:0' shape=(?, 32, 32, 3) dtype=float32>]

 TF output node name:
[<tf.Tensor 'activation_5/Softmax:0' shape=(?, 10) dtype=float32>]

FINISHED CREATING TF FILES

 
#################################################################################################
KERAS to TENSORFLOW GRAPH CONVERSION for miniGoogleNet  on FMNIST
#################################################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Model: "miniGoogleNet"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
conv2d_1_input (InputLayer)     [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 32, 32, 96)   2688        conv2d_1_input[0][0]             
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 32, 32, 96)   384         conv2d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 32, 32, 96)   0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 32)   3104        activation[0][0]                 
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 32)   27680       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 32, 32, 64)   0           activation_1[0][0]               
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 32)   2080        concatenate[0][0]                
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 48)   27696       concatenate[0][0]                
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 48)   192         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 48)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 32, 32, 80)   0           activation_3[0][0]               
                                                                 activation_4[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 15, 15, 80)   57680       concatenate_1[0][0]              
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 15, 15, 80)   320         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 15, 15, 80)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 15, 15, 80)   0           concatenate_1[0][0]              
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 15, 15, 160)  0           activation_5[0][0]               
                                                                 max_pooling2d[0][0]              
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 15, 15, 112)  18032       concatenate_2[0][0]              
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 15, 15, 48)   69168       concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 15, 15, 112)  448         conv2d_6[0][0]                   
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 15, 15, 48)   192         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 15, 15, 112)  0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 15, 15, 48)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 15, 15, 160)  0           activation_6[0][0]               
                                                                 activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 15, 15, 96)   15456       concatenate_3[0][0]              
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 15, 15, 64)   92224       concatenate_3[0][0]              
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 15, 15, 96)   384         conv2d_8[0][0]                   
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 15, 15, 64)   256         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 15, 15, 96)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 15, 15, 64)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 15, 15, 160)  0           activation_8[0][0]               
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 15, 15, 80)   12880       concatenate_4[0][0]              WARNING:tensorflow:From code/Keras2TF.py:73: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.


__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 15, 15, 80)   115280      concatenate_4[0][0]              
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 15, 15, 80)   320         conv2d_10[0][0]                  
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 15, 15, 80)   320         conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 15, 15, 80)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 15, 15, 80)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 15, 15, 160)  0           activation_10[0][0]              
                                                                 activation_11[0][0]              
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 15, 15, 48)   7728        concatenate_5[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 15, 15, 96)   138336      concatenate_5[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 15, 15, 48)   192         conv2d_12[0][0]                  
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 15, 15, 96)   384         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 15, 15, 48)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 15, 15, 96)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 15, 15, 144)  0           activation_12[0][0]              
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 7, 7, 96)     124512      concatenate_6[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 7, 7, 96)     384         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 7, 7, 96)     0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 7, 7, 144)    0           concatenate_6[0][0]              
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 7, 7, 240)    0           activation_14[0][0]              
                                                                 max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 7, 7, 176)    42416       concatenate_7[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 7, 7, 160)    345760      concatenate_7[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 7, 7, 176)    704         conv2d_15[0][0]                  
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 7, 7, 160)    640         conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 7, 7, 176)    0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 7, 7, 160)    0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 7, 7, 336)    0           activation_15[0][0]              
                                                                 activation_16[0][0]              
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 7, 7, 176)    59312       concatenate_8[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 7, 7, 160)    484000      concatenate_8[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 7, 7, 176)    704         conv2d_17[0][0]                  
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 7, 7, 160)    640         conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 7, 7, 176)    0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 7, 7, 160)    0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 7, 7, 336)    0           activation_17[0][0]              
                                                                 activation_18[0][0]              
__________________________________________________________________________________________________
average_pooling2d (AveragePooli (None, 1, 1, 336)    0           concatenate_9[0][0]              
__________________________________________________________________________________________________
dropout (Dropout)               (None, 1, 1, 336)    0           average_pooling2d[0][0]          
__________________________________________________________________________________________________
flatten (Flatten)               (None, 336)          0           dropout[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 10)           3370        flatten[0][0]                    
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 10)           0           dense[0][0]                      
==================================================================================================
Total params: 1,656,250
Trainable params: 1,652,826
Non-trainable params: 3,424
__________________________________________________________________________________________________

 TF input node name:
[<tf.Tensor 'conv2d_1_input:0' shape=(?, 32, 32, 3) dtype=float32>]

 TF output node name:
[<tf.Tensor 'activation_19/Softmax:0' shape=(?, 10) dtype=float32>]

FINISHED CREATING TF FILES

 
#################################################################################################
KERAS to TENSORFLOW GRAPH CONVERSION for miniResNet  on FMNIST
#################################################################################################
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Model: "resnet"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
conv2d_1_input (InputLayer)     [(None, 32, 32, 3)]  0                                            
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 32, 32, 3)    12          conv2d_1_input[0][0]             
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 32, 32, 64)   1728        batch_normalization[0][0]        
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 32, 32, 64)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 32, 32, 16)   1024        activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 32, 32, 16)   2304        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 32, 32, 64)   1024        activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 32, 32, 64)   4096        activation[0][0]                 
__________________________________________________________________________________________________
add (Add)                       (None, 32, 32, 64)   0           conv2d_3[0][0]                   
                                                                 conv2d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         add[0][0]                        
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 32, 32, 16)   1024        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 16)   2304        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 64)   1024        activation_5[0][0]               
__________________________________________________________________________________________________
add_1 (Add)                     (None, 32, 32, 64)   0           conv2d_7[0][0]                   
                                                                 add[0][0]                        
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 32, 32, 16)   1024        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 32, 32, 16)   2304        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 64)   1024        activation_8[0][0]               
__________________________________________________________________________________________________
add_2 (Add)                     (None, 32, 32, 64)   0           conv2d_10[0][0]                  
                                                                 add_1[0][0]                      
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         add_2[0][0]                      
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 32, 32, 16)   1024        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 32, 32, 16)   64          conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 32, 32, 16)   0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 32, 32, 16)   2304        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 32, 32, 16)   64          conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 32, 32, 16)   0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 32, 32, 64)   1024        activation_11[0][0]              
__________________________________________________________________________________________________
add_3 (Add)                     (None, 32, 32, 64)   0           conv2d_13[0][0]                  
                                                                 add_2[0][0]                      
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 32, 32, 64)   256         add_3[0][0]                      
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 32, 32, 64)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 32, 32, 16)   1024        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 32, 32, 16)   64          conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 32, 32, 16)   0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 32, 32, 16)   2304        activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 32, 32, 16)   64          conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 32, 32, 16)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 32, 32, 64)   1024        activation_14[0][0]              
__________________________________________________________________________________________________
add_4 (Add)                     (None, 32, 32, 64)   0           conv2d_16[0][0]                  
                                                                 add_3[0][0]                      
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 32, 32, 64)   256         add_4[0][0]                      
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 32, 32, 64)   0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 32, 32, 16)   1024        activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 32, 32, 16)   64          conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 32, 32, 16)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 32, 32, 16)   2304        activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 32, 32, 16)   64          conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 32, 32, 16)   0           batch_normalization_18[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 32, 32, 64)   1024        activation_17[0][0]              
__________________________________________________________________________________________________
add_5 (Add)                     (None, 32, 32, 64)   0           conv2d_19[0][0]                  
                                                                 add_4[0][0]                      
__________________________________________________________________________________________________
batch_normalization_19 (BatchNo (None, 32, 32, 64)   256         add_5[0][0]                      
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 32, 32, 64)   0           batch_normalization_19[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 32, 32, 16)   1024        activation_18[0][0]              
__________________________________________________________________________________________________
batch_normalization_20 (BatchNo (None, 32, 32, 16)   64          conv2d_20[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 32, 32, 16)   0           batch_normalization_20[0][0]     
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 32, 32, 16)   2304        activation_19[0][0]              
__________________________________________________________________________________________________
batch_normalization_21 (BatchNo (None, 32, 32, 16)   64          conv2d_21[0][0]                  
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 32, 32, 16)   0           batch_normalization_21[0][0]     
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 32, 32, 64)   1024        activation_20[0][0]              
__________________________________________________________________________________________________
add_6 (Add)                     (None, 32, 32, 64)   0           conv2d_22[0][0]                  
                                                                 add_5[0][0]                      
__________________________________________________________________________________________________
batch_normalization_22 (BatchNo (None, 32, 32, 64)   256         add_6[0][0]                      
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 32, 32, 64)   0           batch_normalization_22[0][0]     
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 32, 32, 16)   1024        activation_21[0][0]              
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 32, 32, 16)   64          conv2d_23[0][0]                  
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 32, 32, 16)   0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 32, 32, 16)   2304        activation_22[0][0]              
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 32, 32, 16)   64          conv2d_24[0][0]                  
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 32, 32, 16)   0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 32, 32, 64)   1024        activation_23[0][0]              
__________________________________________________________________________________________________
add_7 (Add)                     (None, 32, 32, 64)   0           conv2d_25[0][0]                  
                                                                 add_6[0][0]                      
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 32, 32, 64)   256         add_7[0][0]                      
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 32, 32, 64)   0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 32, 32, 16)   1024        activation_24[0][0]              
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 32, 32, 16)   64          conv2d_26[0][0]                  
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 32, 32, 16)   0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 32, 32, 16)   2304        activation_25[0][0]              
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 32, 32, 16)   64          conv2d_27[0][0]                  
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 32, 32, 16)   0           batch_normalization_27[0][0]     
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 32, 32, 64)   1024        activation_26[0][0]              
__________________________________________________________________________________________________
add_8 (Add)                     (None, 32, 32, 64)   0           conv2d_28[0][0]                  
                                                                 add_7[0][0]                      
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 32, 32, 64)   256         add_8[0][0]                      
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 32, 32, 64)   0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 32, 32, 32)   2048        activation_27[0][0]              
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 32, 32, 32)   128         conv2d_29[0][0]                  
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 32, 32, 32)   0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 16, 16, 32)   9216        activation_28[0][0]              
__________________________________________________________________________________________________
batch_normalization_30 (BatchNo (None, 16, 16, 32)   128         conv2d_30[0][0]                  
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 16, 16, 32)   0           batch_normalization_30[0][0]     
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 16, 16, 128)  4096        activation_29[0][0]              
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 16, 16, 128)  8192        activation_27[0][0]              
__________________________________________________________________________________________________
add_9 (Add)                     (None, 16, 16, 128)  0           conv2d_31[0][0]                  
                                                                 conv2d_32[0][0]                  
__________________________________________________________________________________________________
batch_normalization_31 (BatchNo (None, 16, 16, 128)  512         add_9[0][0]                      
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 16, 16, 128)  0           batch_normalization_31[0][0]     
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 16, 16, 32)   4096        activation_30[0][0]              
__________________________________________________________________________________________________
batch_normalization_32 (BatchNo (None, 16, 16, 32)   128         conv2d_33[0][0]                  
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 16, 16, 32)   0           batch_normalization_32[0][0]     
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 16, 16, 32)   9216        activation_31[0][0]              
__________________________________________________________________________________________________
batch_normalization_33 (BatchNo (None, 16, 16, 32)   128         conv2d_34[0][0]                  
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 16, 16, 32)   0           batch_normalization_33[0][0]     
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 16, 16, 128)  4096        activation_32[0][0]              
__________________________________________________________________________________________________
add_10 (Add)                    (None, 16, 16, 128)  0           conv2d_35[0][0]                  
                                                                 add_9[0][0]                      
__________________________________________________________________________________________________
batch_normalization_34 (BatchNo (None, 16, 16, 128)  512         add_10[0][0]                     
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 16, 16, 128)  0           batch_normalization_34[0][0]     
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 16, 16, 32)   4096        activation_33[0][0]              
__________________________________________________________________________________________________
batch_normalization_35 (BatchNo (None, 16, 16, 32)   128         conv2d_36[0][0]                  
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 16, 16, 32)   0           batch_normalization_35[0][0]     
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 16, 16, 32)   9216        activation_34[0][0]              
__________________________________________________________________________________________________
batch_normalization_36 (BatchNo (None, 16, 16, 32)   128         conv2d_37[0][0]                  
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 16, 16, 32)   0           batch_normalization_36[0][0]     
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 16, 16, 128)  4096        activation_35[0][0]              
__________________________________________________________________________________________________
add_11 (Add)                    (None, 16, 16, 128)  0           conv2d_38[0][0]                  
                                                                 add_10[0][0]                     
__________________________________________________________________________________________________
batch_normalization_37 (BatchNo (None, 16, 16, 128)  512         add_11[0][0]                     
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 16, 16, 128)  0           batch_normalization_37[0][0]     
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 16, 16, 32)   4096        activation_36[0][0]              
__________________________________________________________________________________________________
batch_normalization_38 (BatchNo (None, 16, 16, 32)   128         conv2d_39[0][0]                  
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 16, 16, 32)   0           batch_normalization_38[0][0]     
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 16, 16, 32)   9216        activation_37[0][0]              
__________________________________________________________________________________________________
batch_normalization_39 (BatchNo (None, 16, 16, 32)   128         conv2d_40[0][0]                  
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 16, 16, 32)   0           batch_normalization_39[0][0]     
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 16, 16, 128)  4096        activation_38[0][0]              
__________________________________________________________________________________________________
add_12 (Add)                    (None, 16, 16, 128)  0           conv2d_41[0][0]                  
                                                                 add_11[0][0]                     
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 16, 16, 128)  512         add_12[0][0]                     
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 16, 16, 128)  0           batch_normalization_40[0][0]     
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 16, 16, 32)   4096        activation_39[0][0]              
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 16, 16, 32)   128         conv2d_42[0][0]                  
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 16, 16, 32)   0           batch_normalization_41[0][0]     
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 16, 16, 32)   9216        activation_40[0][0]              
__________________________________________________________________________________________________
batch_normalization_42 (BatchNo (None, 16, 16, 32)   128         conv2d_43[0][0]                  
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 16, 16, 32)   0           batch_normalization_42[0][0]     
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 16, 16, 128)  4096        activation_41[0][0]              
__________________________________________________________________________________________________
add_13 (Add)                    (None, 16, 16, 128)  0           conv2d_44[0][0]                  
                                                                 add_12[0][0]                     
__________________________________________________________________________________________________
batch_normalization_43 (BatchNo (None, 16, 16, 128)  512         add_13[0][0]                     
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 16, 16, 128)  0           batch_normalization_43[0][0]     
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 16, 16, 32)   4096        activation_42[0][0]              
__________________________________________________________________________________________________
batch_normalization_44 (BatchNo (None, 16, 16, 32)   128         conv2d_45[0][0]                  
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 16, 16, 32)   0           batch_normalization_44[0][0]     
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 16, 16, 32)   9216        activation_43[0][0]              
__________________________________________________________________________________________________
batch_normalization_45 (BatchNo (None, 16, 16, 32)   128         conv2d_46[0][0]                  
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 16, 16, 32)   0           batch_normalization_45[0][0]     
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 16, 16, 128)  4096        activation_44[0][0]              
__________________________________________________________________________________________________
add_14 (Add)                    (None, 16, 16, 128)  0           conv2d_47[0][0]                  
                                                                 add_13[0][0]                     
__________________________________________________________________________________________________
batch_normalization_46 (BatchNo (None, 16, 16, 128)  512         add_14[0][0]                     
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 16, 16, 128)  0           batch_normalization_46[0][0]     
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 16, 16, 32)   4096        activation_45[0][0]              
__________________________________________________________________________________________________
batch_normalization_47 (BatchNo (None, 16, 16, 32)   128         conv2d_48[0][0]                  
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 16, 16, 32)   0           batch_normalization_47[0][0]     
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 16, 16, 32)   9216        activation_46[0][0]              
__________________________________________________________________________________________________
batch_normalization_48 (BatchNo (None, 16, 16, 32)   128         conv2d_49[0][0]                  
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 16, 16, 32)   0           batch_normalization_48[0][0]     
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 16, 16, 128)  4096        activation_47[0][0]              
__________________________________________________________________________________________________
add_15 (Add)                    (None, 16, 16, 128)  0           conv2d_50[0][0]                  
                                                                 add_14[0][0]                     
__________________________________________________________________________________________________
batch_normalization_49 (BatchNo (None, 16, 16, 128)  512         add_15[0][0]                     
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 16, 16, 128)  0           batch_normalization_49[0][0]     
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 16, 16, 32)   4096        activation_48[0][0]              
__________________________________________________________________________________________________
batch_normalization_50 (BatchNo (None, 16, 16, 32)   128         conv2d_51[0][0]                  
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 16, 16, 32)   0           batch_normalization_50[0][0]     
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 16, 16, 32)   9216        activation_49[0][0]              
__________________________________________________________________________________________________
batch_normalization_51 (BatchNo (None, 16, 16, 32)   128         conv2d_52[0][0]                  
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 16, 16, 32)   0           batch_normalization_51[0][0]     
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 16, 16, 128)  4096        activation_50[0][0]              
__________________________________________________________________________________________________
add_16 (Add)                    (None, 16, 16, 128)  0           conv2d_53[0][0]                  
                                                                 add_15[0][0]                     
__________________________________________________________________________________________________
batch_normalization_52 (BatchNo (None, 16, 16, 128)  512         add_16[0][0]                     
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 16, 16, 128)  0           batch_normalization_52[0][0]     
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 16, 16, 32)   4096        activation_51[0][0]              
__________________________________________________________________________________________________
batch_normalization_53 (BatchNo (None, 16, 16, 32)   128         conv2d_54[0][0]                  
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 16, 16, 32)   0           batch_normalization_53[0][0]     
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 16, 16, 32)   9216        activation_52[0][0]              
__________________________________________________________________________________________________
batch_normalization_54 (BatchNo (None, 16, 16, 32)   128         conv2d_55[0][0]                  
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 16, 16, 32)   0           batch_normalization_54[0][0]     
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 16, 16, 128)  4096        activation_53[0][0]              
__________________________________________________________________________________________________
add_17 (Add)                    (None, 16, 16, 128)  0           conv2d_56[0][0]                  
                                                                 add_16[0][0]                     
__________________________________________________________________________________________________
batch_normalization_55 (BatchNo (None, 16, 16, 128)  512         add_17[0][0]                     
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 16, 16, 128)  0           batch_normalization_55[0][0]     
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 16, 16, 64)   8192        activation_54[0][0]              
__________________________________________________________________________________________________
batch_normalization_56 (BatchNo (None, 16, 16, 64)   256         conv2d_57[0][0]                  
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 16, 16, 64)   0           batch_normalization_56[0][0]     
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 8, 8, 64)     36864       activation_55[0][0]              
__________________________________________________________________________________________________
batch_normalization_57 (BatchNo (None, 8, 8, 64)     256         conv2d_58[0][0]                  
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 8, 8, 64)     0           batch_normalization_57[0][0]     
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 8, 8, 256)    16384       activation_56[0][0]              
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 8, 8, 256)    32768       activation_54[0][0]              
__________________________________________________________________________________________________
add_18 (Add)                    (None, 8, 8, 256)    0           conv2d_59[0][0]                  
                                                                 conv2d_60[0][0]                  
__________________________________________________________________________________________________
batch_normalization_58 (BatchNo (None, 8, 8, 256)    1024        add_18[0][0]                     
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 8, 8, 256)    0           batch_normalization_58[0][0]     
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 8, 8, 64)     16384       activation_57[0][0]              
__________________________________________________________________________________________________
batch_normalization_59 (BatchNo (None, 8, 8, 64)     256         conv2d_61[0][0]                  
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 8, 8, 64)     0           batch_normalization_59[0][0]     
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 8, 8, 64)     36864       activation_58[0][0]              
__________________________________________________________________________________________________
batch_normalization_60 (BatchNo (None, 8, 8, 64)     256         conv2d_62[0][0]                  
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 8, 8, 64)     0           batch_normalization_60[0][0]     
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 8, 8, 256)    16384       activation_59[0][0]              
__________________________________________________________________________________________________
add_19 (Add)                    (None, 8, 8, 256)    0           conv2d_63[0][0]                  
                                                                 add_18[0][0]                     
__________________________________________________________________________________________________
batch_normalization_61 (BatchNo (None, 8, 8, 256)    1024        add_19[0][0]                     
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 8, 8, 256)    0           batch_normalization_61[0][0]     
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 8, 8, 64)     16384       activation_60[0][0]              
__________________________________________________________________________________________________
batch_normalization_62 (BatchNo (None, 8, 8, 64)     256         conv2d_64[0][0]                  
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 8, 8, 64)     0           batch_normalization_62[0][0]     
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 8, 8, 64)     36864       activation_61[0][0]              
__________________________________________________________________________________________________
batch_normalization_63 (BatchNo (None, 8, 8, 64)     256         conv2d_65[0][0]                  
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 8, 8, 64)     0           batch_normalization_63[0][0]     
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 8, 8, 256)    16384       activation_62[0][0]              
__________________________________________________________________________________________________
add_20 (Add)                    (None, 8, 8, 256)    0           conv2d_66[0][0]                  
                                                                 add_19[0][0]                     
__________________________________________________________________________________________________
batch_normalization_64 (BatchNo (None, 8, 8, 256)    1024        add_20[0][0]                     
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 8, 8, 256)    0           batch_normalization_64[0][0]     
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 8, 8, 64)     16384       activation_63[0][0]              
__________________________________________________________________________________________________
batch_normalization_65 (BatchNo (None, 8, 8, 64)     256         conv2d_67[0][0]                  
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 8, 8, 64)     0           batch_normalization_65[0][0]     
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 8, 8, 64)     36864       activation_64[0][0]              
__________________________________________________________________________________________________
batch_normalization_66 (BatchNo (None, 8, 8, 64)     256         conv2d_68[0][0]                  
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 8, 8, 64)     0           batch_normalization_66[0][0]     
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 8, 8, 256)    16384       activation_65[0][0]              
__________________________________________________________________________________________________
add_21 (Add)                    (None, 8, 8, 256)    0           conv2d_69[0][0]                  
                                                                 add_20[0][0]                     
__________________________________________________________________________________________________
batch_normalization_67 (BatchNo (None, 8, 8, 256)    1024        add_21[0][0]                     
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 8, 8, 256)    0           batch_normalization_67[0][0]     
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 8, 8, 64)     16384       activation_66[0][0]              
__________________________________________________________________________________________________
batch_normalization_68 (BatchNo (None, 8, 8, 64)     256         conv2d_70[0][0]                  
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 8, 8, 64)     0           batch_normalization_68[0][0]     
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 8, 8, 64)     36864       activation_67[0][0]              
__________________________________________________________________________________________________
batch_normalization_69 (BatchNo (None, 8, 8, 64)     256         conv2d_71[0][0]                  
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 8, 8, 64)     0           batch_normalization_69[0][0]     
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 8, 8, 256)    16384       activation_68[0][0]              
__________________________________________________________________________________________________
add_22 (Add)                    (None, 8, 8, 256)    0           conv2d_72[0][0]                  
                                                                 add_21[0][0]                     
__________________________________________________________________________________________________
batch_normalization_70 (BatchNo (None, 8, 8, 256)    1024        add_22[0][0]                     
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 8, 8, 256)    0           batch_normalization_70[0][0]     
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 8, 8, 64)     16384       activation_69[0][0]              
__________________________________________________________________________________________________
batch_normalization_71 (BatchNo (None, 8, 8, 64)     256         conv2d_73[0][0]                  
__________________________________________________________________________________________________
activation_70 (Activation)      (None, 8, 8, 64)     0           batch_normalization_71[0][0]     
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 8, 8, 64)     36864       activation_70[0][0]              
__________________________________________________________________________________________________
batch_normalization_72 (BatchNo (None, 8, 8, 64)     256         conv2d_74[0][0]                  
__________________________________________________________________________________________________
activation_71 (Activation)      (None, 8, 8, 64)     0           batch_normalization_72[0][0]     
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 8, 8, 256)    16384       activation_71[0][0]              
__________________________________________________________________________________________________
add_23 (Add)                    (None, 8, 8, 256)    0           conv2d_75[0][0]                  
                                                                 add_22[0][0]                     
__________________________________________________________________________________________________
batch_normalization_73 (BatchNo (None, 8, 8, 256)    1024        add_23[0][0]                     
__________________________________________________________________________________________________
activation_72 (Activation)      (None, 8, 8, 256)    0           batch_normalization_73[0][0]     
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 8, 8, 64)     16384       activation_72[0][0]              
__________________________________________________________________________________________________
batch_normalization_74 (BatchNo (None, 8, 8, 64)     256         conv2d_76[0][0]                  
__________________________________________________________________________________________________
activation_73 (Activation)      (None, 8, 8, 64)     0           batch_normalization_74[0][0]     
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 8, 8, 64)     36864       activation_73[0][0]              
__________________________________________________________________________________________________
batch_normalization_75 (BatchNo (None, 8, 8, 64)     256         conv2d_77[0][0]                  
__________________________________________________________________________________________________
activation_74 (Activation)      (None, 8, 8, 64)     0           batch_normalization_75[0][0]     
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 8, 8, 256)    16384       activation_74[0][0]              
__________________________________________________________________________________________________
add_24 (Add)                    (None, 8, 8, 256)    0           conv2d_78[0][0]                  
                                                                 add_23[0][0]                     
__________________________________________________________________________________________________
batch_normalization_76 (BatchNo (None, 8, 8, 256)    1024        add_24[0][0]                     
__________________________________________________________________________________________________
activation_75 (Activation)      (None, 8, 8, 256)    0           batch_normalization_76[0][0]     
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 8, 8, 64)     16384       activation_75[0][0]              
__________________________________________________________________________________________________
batch_normalization_77 (BatchNo (None, 8, 8, 64)     256         conv2d_79[0][0]                  
__________________________________________________________________________________________________
activation_76 (Activation)      (None, 8, 8, 64)     0           batch_normalization_77[0][0]     
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 8, 8, 64)     36864       activation_76[0][0]              
__________________________________________________________________________________________________
batch_normalization_78 (BatchNo (None, 8, 8, 64)     256         conv2d_80[0][0]                  
__________________________________________________________________________________________________
activation_77 (Activation)      (None, 8, 8, 64)     0           batch_normalization_78[0][0]     
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 8, 8, 256)    16384       activation_77[0][0]              
__________________________________________________________________________________________________
add_25 (Add)                    (None, 8, 8, 256)    0           conv2d_81[0][0]                  
                                                                 add_24[0][0]                     
__________________________________________________________________________________________________
batch_normalization_79 (BatchNo (None, 8, 8, 256)    1024        add_25[0][0]                     
__________________________________________________________________________________________________
activation_78 (Activation)      (None, 8, 8, 256)    0           batch_normalization_79[0][0]     
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 8, 8, 64)     16384       activation_78[0][0]              
__________________________________________________________________________________________________
batch_normalization_80 (BatchNo (None, 8, 8, 64)     256         conv2d_82[0][0]                  
__________________________________________________________________________________________________
activation_79 (Activation)      (None, 8, 8, 64)     0           batch_normalization_80[0][0]     
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 8, 8, 64)     36864       activation_79[0][0]              
__________________________________________________________________________________________________
batch_normalization_81 (BatchNo (None, 8, 8, 64)     256         conv2d_83[0][0]                  
__________________________________________________________________________________________________
activation_80 (Activation)      (None, 8, 8, 64)     0           batch_normalization_81[0][0]     
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 8, 8, 256)    16384       activation_80[0][0]              
__________________________________________________________________________________________________
add_26 (Add)                    (None, 8, 8, 256)    0           conv2d_84[0][0]                  
                                                                 add_25[0][0]                     WARNING:tensorflow:From code/Keras2TF.py:73: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.


__________________________________________________________________________________________________
batch_normalization_82 (BatchNo (None, 8, 8, 256)    1024        add_26[0][0]                     
__________________________________________________________________________________________________
activation_81 (Activation)      (None, 8, 8, 256)    0           batch_normalization_82[0][0]     
__________________________________________________________________________________________________
average_pooling2d (AveragePooli (None, 1, 1, 256)    0           activation_81[0][0]              
__________________________________________________________________________________________________
flatten (Flatten)               (None, 256)          0           average_pooling2d[0][0]          
__________________________________________________________________________________________________
dense (Dense)                   (None, 10)           2570        flatten[0][0]                    
__________________________________________________________________________________________________
activation_82 (Activation)      (None, 10)           0           dense[0][0]                      
==================================================================================================
Total params: 886,102
Trainable params: 873,872
Non-trainable params: 12,230
__________________________________________________________________________________________________

 TF input node name:
[<tf.Tensor 'conv2d_1_input:0' shape=(?, 32, 32, 3) dtype=float32>]

 TF output node name:
[<tf.Tensor 'activation_82/Softmax:0' shape=(?, 10) dtype=float32>]

FINISHED CREATING TF FILES

 
##########################################################################
FREEZE GRAPH of LeNet on FMNIST
##########################################################################
rm: cannot remove './build/freeze/fmnist/Lenet/*': No such file or directory
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0609 15:13:31.492269 140610272552768 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2022-06-09 15:13:31.495325: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 15:13:31.545149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 15:13:31.545382: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 15:13:31.546565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 15:13:31.547628: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 15:13:31.547895: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 15:13:31.549344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 15:13:31.550445: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 15:13:31.553810: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 15:13:31.555247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 15:13:31.555574: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 15:13:31.578899: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 15:13:31.581152: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559ba176e040 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 15:13:31.581183: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 15:13:31.659659: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559ba12a6090 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 15:13:31.659703: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 15:13:31.662223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 15:13:31.662295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 15:13:31.662317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 15:13:31.662349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 15:13:31.662368: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 15:13:31.662388: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 15:13:31.662408: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 15:13:31.662428: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 15:13:31.666654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 15:13:31.666726: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 15:13:31.670043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 15:13:31.670068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 15:13:31.670079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 15:13:31.676105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22402 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ./build/tf_chkpts/fmnist/LeNet/float_model.ckpt
I0609 15:13:31.763631 140610272552768 saver.py:1284] Restoring parameters from ./build/tf_chkpts/fmnist/LeNet/float_model.ckpt
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0609 15:13:32.734112 140610272552768 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0609 15:13:32.734521 140610272552768 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 10 variables.
I0609 15:13:32.772203 140610272552768 graph_util_impl.py:334] Froze 10 variables.
INFO:tensorflow:Converted 10 variables to const ops.
I0609 15:13:32.826799 140610272552768 graph_util_impl.py:394] Converted 10 variables to const ops.
Loaded meta graph file './build/tf_chkpts/fmnist/LeNet/float_model.ckpt.meta
 
##########################################################################
INSPECT FROZEN GRAPH of LeNet  on FMNIST
##########################################################################
Op types used: 14 Const, 10 Identity, 3 BiasAdd, 2 MatMul, 2 Relu, 1 Conv2D, 1 FusedBatchNormV3, 1 MaxPool, 1 Pack, 1 Placeholder, 1 Reshape, 1 Shape, 1 Softmax, 1 StridedSlice

Found 1 possible inputs: (name=conv2d_1_input, type=float(1), shape=[?,32,32,3]) 
Found 1 possible outputs: (name=activation_3/Softmax, op=Softmax) 
 
##########################################################################
FREEZE GRAPH of miniVggNet  on FMNIST
##########################################################################
rm: cannot remove './build/freeze/fmnist/miniVggNet/*': No such file or directory
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0609 15:13:37.767265 139834677458752 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2022-06-09 15:13:37.770484: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 15:13:37.808598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 15:13:37.808796: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 15:13:37.809955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 15:13:37.810969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 15:13:37.811230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 15:13:37.812611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 15:13:37.813713: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 15:13:37.816771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 15:13:37.818203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 15:13:37.818484: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 15:13:37.840595: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 15:13:37.842783: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55963c4679f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 15:13:37.842817: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 15:13:37.920745: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55963bf32600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 15:13:37.920788: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 15:13:37.923184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 15:13:37.923259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 15:13:37.923282: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 15:13:37.923301: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 15:13:37.923320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 15:13:37.923340: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 15:13:37.923359: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 15:13:37.923379: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 15:13:37.927451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 15:13:37.927515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 15:13:37.930713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 15:13:37.930737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 15:13:37.930748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 15:13:37.934804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22402 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ./build/tf_chkpts/fmnist/miniVggNet/float_model.ckpt
I0609 15:13:38.049753 139834677458752 saver.py:1284] Restoring parameters from ./build/tf_chkpts/fmnist/miniVggNet/float_model.ckpt
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0609 15:13:39.041667 139834677458752 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0609 15:13:39.041979 139834677458752 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 28 variables.
I0609 15:13:39.090387 139834677458752 graph_util_impl.py:334] Froze 28 variables.
INFO:tensorflow:Converted 28 variables to const ops.
I0609 15:13:39.104493 139834677458752 graph_util_impl.py:394] Converted 28 variables to const ops.
Loaded meta graph file './build/tf_chkpts/fmnist/miniVggNet/float_model.ckpt.meta
 
##########################################################################
INSPECT FROZEN GRAPH of miniVggNet on FMNIST
##########################################################################
Op types used: 33 Const, 31 Identity, 5 Relu, 4 Conv2D, 4 FusedBatchNormV3, 3 Mul, 2 BiasAdd, 2 MatMul, 2 MaxPool, 2 AddV2, 1 Sub, 1 StridedSlice, 1 Softmax, 1 Shape, 1 Rsqrt, 1 Reshape, 1 Placeholder, 1 Pack

Found 1 possible inputs: (name=conv2d_1_input, type=float(1), shape=[?,32,32,3]) 
Found 1 possible outputs: (name=activation_5/Softmax, op=Softmax) 
##########################################################################
FREEZE GRAPH of miniGoogleNet  on FMNIST
##########################################################################
rm: cannot remove './build/freeze/fmnist/miniGoogleNet/*': No such file or directory
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0609 15:13:43.975872 139875440170816 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2022-06-09 15:13:43.985418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 15:13:44.038739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 15:13:44.038991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 15:13:44.040378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 15:13:44.041714: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 15:13:44.042013: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 15:13:44.043461: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 15:13:44.044565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 15:13:44.047912: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 15:13:44.049374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 15:13:44.049735: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 15:13:44.071800: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 15:13:44.073591: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5630c1bc02c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 15:13:44.073608: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 15:13:44.152997: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5630c2114750 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 15:13:44.153042: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 15:13:44.155549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 15:13:44.155640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 15:13:44.155669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 15:13:44.155695: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 15:13:44.155721: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 15:13:44.155747: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 15:13:44.155773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 15:13:44.155800: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 15:13:44.160095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 15:13:44.160182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 15:13:44.163249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 15:13:44.163271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 15:13:44.163280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 15:13:44.166927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22402 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ./build/tf_chkpts/fmnist/miniGoogleNet/float_model.ckpt
I0609 15:13:44.503227 139875440170816 saver.py:1284] Restoring parameters from ./build/tf_chkpts/fmnist/miniGoogleNet/float_model.ckpt
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0609 15:13:45.639814 139875440170816 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0609 15:13:45.640251 139875440170816 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 116 variables.
I0609 15:13:45.788899 139875440170816 graph_util_impl.py:334] Froze 116 variables.
INFO:tensorflow:Converted 116 variables to const ops.
I0609 15:13:45.820784 139875440170816 graph_util_impl.py:394] Converted 116 variables to const ops.
Loaded meta graph file './build/tf_chkpts/fmnist/miniGoogleNet/float_model.ckpt.meta
 
##########################################################################
INSPECT FROZEN GRAPH of miniGoogleNet  on FMNIST
##########################################################################
Op types used: 130 Const, 117 Identity, 20 BiasAdd, 19 Conv2D, 19 FusedBatchNormV3, 19 Relu, 10 ConcatV2, 2 MaxPool, 1 AvgPool, 1 MatMul, 1 Pack, 1 Placeholder, 1 Reshape, 1 Shape, 1 Softmax, 1 StridedSlice

Found 1 possible inputs: (name=conv2d_1_input, type=float(1), shape=[?,32,32,3]) 
Found 1 possible outputs: (name=activation_19/Softmax, op=Softmax) 
##########################################################################
FREEZE GRAPH of miniResNet  on FMNIST
##########################################################################
rm: cannot remove './build/freeze/fmnist/miniResNet/*': No such file or directory
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0609 15:13:50.675947 140302447474496 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2022-06-09 15:13:50.684867: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2022-06-09 15:13:50.727041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 15:13:50.727249: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 15:13:50.728540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 15:13:50.729651: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 15:13:50.729945: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 15:13:50.731452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 15:13:50.732578: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 15:13:50.736006: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 15:13:50.737444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 15:13:50.737761: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-06-09 15:13:50.760498: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2499950000 Hz
2022-06-09 15:13:50.762614: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5641499ffa60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-06-09 15:13:50.762635: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-06-09 15:13:50.836365: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56414a345bd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-06-09 15:13:50.836409: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P6000, Compute Capability 6.1
2022-06-09 15:13:50.838913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:65:00.0
2022-06-09 15:13:50.838981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 15:13:50.839004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2022-06-09 15:13:50.839023: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2022-06-09 15:13:50.839042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2022-06-09 15:13:50.839062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2022-06-09 15:13:50.839081: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2022-06-09 15:13:50.839101: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2022-06-09 15:13:50.843338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0
2022-06-09 15:13:50.843404: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2022-06-09 15:13:50.846676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-06-09 15:13:50.846702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 
2022-06-09 15:13:50.846713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N 
2022-06-09 15:13:50.851046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22402 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:65:00.0, compute capability: 6.1)
INFO:tensorflow:Restoring parameters from ./build/tf_chkpts/fmnist/miniResNet/float_model.ckpt
I0609 15:13:52.209835 140302447474496 saver.py:1284] Restoring parameters from ./build/tf_chkpts/fmnist/miniResNet/float_model.ckpt
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0609 15:13:54.029472 140302447474496 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/tools/freeze_graph.py:226: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0609 15:13:54.029851 140302447474496 deprecation.py:323] From /opt/vitis_ai/conda/envs/vitis-ai-tensorflow/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 419 variables.
I0609 15:13:54.571143 140302447474496 graph_util_impl.py:334] Froze 419 variables.
INFO:tensorflow:Converted 419 variables to const ops.
I0609 15:13:54.612373 140302447474496 graph_util_impl.py:394] Converted 419 variables to const ops.
Loaded meta graph file './build/tf_chkpts/fmnist/miniResNet/float_model.ckpt.meta
 
##########################################################################
INSPECT FROZEN GRAPH of miniResNet
##########################################################################
Op types used: 423 Const, 419 Identity, 85 Conv2D, 83 FusedBatchNormV3, 82 Relu, 27 AddV2, 1 AvgPool, 1 BiasAdd, 1 MatMul, 1 Pack, 1 Placeholder, 1 Reshape, 1 Shape, 1 Softmax, 1 StridedSlice

Found 1 possible inputs: (name=conv2d_1_input, type=float(1), shape=[?,32,32,3]) 
Found 1 possible outputs: (name=activation_82/Softmax, op=Softmax) 
 
##########################################################################
FREEZE GRAPH COMPLETED
##########################################################################
 
 
##########################################################################
EVALUATE FROZEN GRAPH of LeNet on FMNIST
##########################################################################

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist
 Top 1 accuracy with test dataset: 0.9070
 Top 5 accuracy with test dataset: 0.9986
FINISHED!
 
##########################################################################
EVALUATE FROZEN GRAPH of miniVggNet  on FMNIST
##########################################################################

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist
 Top 1 accuracy with test dataset: 0.9346
 Top 5 accuracy with test dataset: 0.9996
FINISHED!
 
##########################################################################
EVALUATE FROZEN GRAPH of GoogleNet  on FMNIST
##########################################################################

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist
 Top 1 accuracy with test dataset: 0.9086
 Top 5 accuracy with test dataset: 0.9964
FINISHED!
 
##########################################################################
EVALUATE FROZEN GRAPH of ResNet  on FMNIST
##########################################################################

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist
 Top 1 accuracy with test dataset: 0.9430
 Top 5 accuracy with test dataset: 1.0000
FINISHED!
 
##########################################################################
EVALUATE FROZEN GRAPH COMPLETED  on FMNIST
##########################################################################
 
 
##########################################################################
QUANTIZE LeNet on FMNIST
##########################################################################
2022-06-09 15:14:37.296264: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
  0% (0 of 20) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--
  5% (1 of 20) |#                        | Elapsed Time: 0:00:00 ETA:   0:00:02
 10% (2 of 20) |##                       | Elapsed Time: 0:00:00 ETA:   0:00:02
 15% (3 of 20) |###                      | Elapsed Time: 0:00:00 ETA:   0:00:02
 20% (4 of 20) |#####                    | Elapsed Time: 0:00:00 ETA:   0:00:02
 25% (5 of 20) |######                   | Elapsed Time: 0:00:00 ETA:   0:00:02
 30% (6 of 20) |#######                  | Elapsed Time: 0:00:00 ETA:   0:00:02
 35% (7 of 20) |########                 | Elapsed Time: 0:00:00 ETA:   0:00:01
 40% (8 of 20) |##########               | Elapsed Time: 0:00:01 ETA:   0:00:01
 45% (9 of 20) |###########              | Elapsed Time: 0:00:01 ETA:   0:00:01
 50% (10 of 20) |############            | Elapsed Time: 0:00:01 ETA:   0:00:01
 55% (11 of 20) |#############           | Elapsed Time: 0:00:01 ETA:   0:00:01
 60% (12 of 20) |##############          | Elapsed Time: 0:00:01 ETA:   0:00:01
 65% (13 of 20) |###############         | Elapsed Time: 0:00:01 ETA:   0:00:00
 70% (14 of 20) |################        | Elapsed Time: 0:00:01 ETA:   0:00:00
 75% (15 of 20) |##################      | Elapsed Time: 0:00:02 ETA:   0:00:00
 80% (16 of 20) |###################     | Elapsed Time: 0:00:02 ETA:   0:00:00
 85% (17 of 20) |####################    | Elapsed Time: 0:00:02 ETA:   0:00:00
 90% (18 of 20) |#####################   | Elapsed Time: 0:00:02 ETA:   0:00:00
 95% (19 of 20) |######################  | Elapsed Time: 0:00:02 ETA:   0:00:00
100% (20 of 20) |########################| Elapsed Time: 0:00:02 Time:  0:00:02
script running on folder  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code
CALIB DIR  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code/../build/dataset/fmnist/calib
INFO: Checking Float Graph...
INFO: Float Graph Check Done.
INFO: Calibrating for 20 iterations...
INFO: Calibration Done.
********************* Quantization Summary *********************      
INFO: Output:       
  quantize_eval_model: ../build/quantized_results/fmnist/LeNet/ quantize_eval_model.pb 
 
##########################################################################
QUANTIZE miniVggNet  on FMNIST
##########################################################################
2022-06-09 15:14:45.406326: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
  0% (0 of 20) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--
  5% (1 of 20) |#                        | Elapsed Time: 0:00:00 ETA:   0:00:03
 10% (2 of 20) |##                       | Elapsed Time: 0:00:00 ETA:   0:00:02
 15% (3 of 20) |###                      | Elapsed Time: 0:00:00 ETA:   0:00:02
 20% (4 of 20) |#####                    | Elapsed Time: 0:00:00 ETA:   0:00:02
 25% (5 of 20) |######                   | Elapsed Time: 0:00:00 ETA:   0:00:02
 30% (6 of 20) |#######                  | Elapsed Time: 0:00:00 ETA:   0:00:02
 35% (7 of 20) |########                 | Elapsed Time: 0:00:01 ETA:   0:00:01
 40% (8 of 20) |##########               | Elapsed Time: 0:00:01 ETA:   0:00:01
 45% (9 of 20) |###########              | Elapsed Time: 0:00:01 ETA:   0:00:01
 50% (10 of 20) |############            | Elapsed Time: 0:00:01 ETA:   0:00:01
 55% (11 of 20) |#############           | Elapsed Time: 0:00:01 ETA:   0:00:01
 60% (12 of 20) |##############          | Elapsed Time: 0:00:01 ETA:   0:00:01
 65% (13 of 20) |###############         | Elapsed Time: 0:00:01 ETA:   0:00:01
 70% (14 of 20) |################        | Elapsed Time: 0:00:02 ETA:   0:00:00
 75% (15 of 20) |##################      | Elapsed Time: 0:00:02 ETA:   0:00:00
 80% (16 of 20) |###################     | Elapsed Time: 0:00:02 ETA:   0:00:00
 85% (17 of 20) |####################    | Elapsed Time: 0:00:02 ETA:   0:00:00
 90% (18 of 20) |#####################   | Elapsed Time: 0:00:02 ETA:   0:00:00
 95% (19 of 20) |######################  | Elapsed Time: 0:00:02 ETA:   0:00:00
100% (20 of 20) |########################| Elapsed Time: 0:00:02 Time:  0:00:02
script running on folder  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code
CALIB DIR  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code/../build/dataset/fmnist/calib
INFO: Checking Float Graph...
INFO: Float Graph Check Done.
INFO: Calibrating for 20 iterations...
INFO: Calibration Done.
********************* Quantization Summary *********************      
INFO: Output:       
  quantize_eval_model: ../build/quantized_results/fmnist/miniVggNet/ quantize_eval_model.pb 
 
##########################################################################
QUANTIZE miniGoogleNet  on FMNIST
##########################################################################
2022-06-09 15:14:53.589751: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2022-06-09 15:14:53.805052: W tensorflow/contrib/decent_q/utils/quantize_utils.cc:862] [DECENT_WARNING] Scale output of avg_pool node average_pooling2d/AvgPool to simulate DPU. Kernel size is 7 * 7
  0% (0 of 20) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--
  5% (1 of 20) |#                        | Elapsed Time: 0:00:00 ETA:   0:00:09
 10% (2 of 20) |##                       | Elapsed Time: 0:00:00 ETA:   0:00:08
 15% (3 of 20) |###                      | Elapsed Time: 0:00:01 ETA:   0:00:08
 20% (4 of 20) |#####                    | Elapsed Time: 0:00:01 ETA:   0:00:07
 25% (5 of 20) |######                   | Elapsed Time: 0:00:02 ETA:   0:00:07
 30% (6 of 20) |#######                  | Elapsed Time: 0:00:02 ETA:   0:00:06
 35% (7 of 20) |########                 | Elapsed Time: 0:00:03 ETA:   0:00:06
 40% (8 of 20) |##########               | Elapsed Time: 0:00:03 ETA:   0:00:05
 45% (9 of 20) |###########              | Elapsed Time: 0:00:04 ETA:   0:00:05
 50% (10 of 20) |############            | Elapsed Time: 0:00:04 ETA:   0:00:04
 55% (11 of 20) |#############           | Elapsed Time: 0:00:05 ETA:   0:00:04
 60% (12 of 20) |##############          | Elapsed Time: 0:00:05 ETA:   0:00:03
 65% (13 of 20) |###############         | Elapsed Time: 0:00:06 ETA:   0:00:03
 70% (14 of 20) |################        | Elapsed Time: 0:00:06 ETA:   0:00:02
 75% (15 of 20) |##################      | Elapsed Time: 0:00:07 ETA:   0:00:02
 80% (16 of 20) |###################     | Elapsed Time: 0:00:07 ETA:   0:00:01
 85% (17 of 20) |####################    | Elapsed Time: 0:00:08 ETA:   0:00:01
 90% (18 of 20) |#####################   | Elapsed Time: 0:00:08 ETA:   0:00:00
 95% (19 of 20) |######################  | Elapsed Time: 0:00:08 ETA:   0:00:00
100% (20 of 20) |########################| Elapsed Time: 0:00:09 Time:  0:00:09
script running on folder  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code
CALIB DIR  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code/../build/dataset/fmnist/calib
INFO: Checking Float Graph...
INFO: Float Graph Check Done.
INFO: Calibrating for 20 iterations...
INFO: Calibration Done.
********************* Quantization Summary *********************      
INFO: Output:       
  quantize_eval_model: ../build/quantized_results/fmnist/miniGoogleNet/ quantize_eval_model.pb 
 
##########################################################################
QUANTIZE miniResNet  on FMNIST
##########################################################################
2022-06-09 15:15:08.566146: W tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
Relying on driver to perform ptx compilation. This message will be only logged once.
2022-06-09 15:15:08.890259: W tensorflow/contrib/decent_q/utils/quantize_utils.cc:862] [DECENT_WARNING] Scale output of avg_pool node average_pooling2d/AvgPool to simulate DPU. Kernel size is 8 * 8
  0% (0 of 20) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--
  5% (1 of 20) |#                        | Elapsed Time: 0:00:02 ETA:   0:00:38
 10% (2 of 20) |##                       | Elapsed Time: 0:00:03 ETA:   0:00:35
 15% (3 of 20) |###                      | Elapsed Time: 0:00:05 ETA:   0:00:30
 20% (4 of 20) |#####                    | Elapsed Time: 0:00:07 ETA:   0:00:28
 25% (5 of 20) |######                   | Elapsed Time: 0:00:09 ETA:   0:00:27
 30% (6 of 20) |#######                  | Elapsed Time: 0:00:11 ETA:   0:00:26
 35% (7 of 20) |########                 | Elapsed Time: 0:00:13 ETA:   0:00:24
 40% (8 of 20) |##########               | Elapsed Time: 0:00:14 ETA:   0:00:22
 45% (9 of 20) |###########              | Elapsed Time: 0:00:16 ETA:   0:00:20
 50% (10 of 20) |############            | Elapsed Time: 0:00:18 ETA:   0:00:18
 55% (11 of 20) |#############           | Elapsed Time: 0:00:20 ETA:   0:00:16
 60% (12 of 20) |##############          | Elapsed Time: 0:00:22 ETA:   0:00:14
 65% (13 of 20) |###############         | Elapsed Time: 0:00:24 ETA:   0:00:13
 70% (14 of 20) |################        | Elapsed Time: 0:00:26 ETA:   0:00:11
 75% (15 of 20) |##################      | Elapsed Time: 0:00:28 ETA:   0:00:09
 80% (16 of 20) |###################     | Elapsed Time: 0:00:29 ETA:   0:00:07
 85% (17 of 20) |####################    | Elapsed Time: 0:00:31 ETA:   0:00:05
 90% (18 of 20) |#####################   | Elapsed Time: 0:00:33 ETA:   0:00:03
 95% (19 of 20) |######################  | Elapsed Time: 0:00:35 ETA:   0:00:01
100% (20 of 20) |########################| Elapsed Time: 0:00:37 Time:  0:00:37
script running on folder  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code
CALIB DIR  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/code/../build/dataset/fmnist/calib
INFO: Checking Float Graph...
INFO: Float Graph Check Done.
INFO: Calibrating for 20 iterations...
INFO: Calibration Done.
********************* Quantization Summary *********************      
INFO: Output:       
  quantize_eval_model: ../build/quantized_results/fmnist/miniResNet/ quantize_eval_model.pb 
 
##########################################################################
QUANTIZATION COMPLETED  on FMNIST
##########################################################################
 
 
##########################################################################
EVALUATE QUANTIZED GRAPH of LeNet on FMNIST
##########################################################################

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist
 Top 1 accuracy with test dataset: 0.9110
 Top 5 accuracy with test dataset: 0.9988
FINISHED!
 
##########################################################################
EVALUATE QUANTIZED GRAPH of miniVggNet  on FMNIST
##########################################################################

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist
 Top 1 accuracy with test dataset: 0.9384
 Top 5 accuracy with test dataset: 0.9996
FINISHED!
 
##############################################################################
EVALUATE QUANTIZED GRAPH of miniGoogleNet  on FMNIST
##############################################################################

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist
 Top 1 accuracy with test dataset: 0.9082
 Top 5 accuracy with test dataset: 0.9966
FINISHED!
 
##########################################################################
EVALUATE QUANTIZED GRAPH of miniResNet  on FMNIST
##########################################################################

 eval_graph.py runs from  /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/dataset/fmnist
 Top 1 accuracy with test dataset: 0.9420
 Top 5 accuracy with test dataset: 1.0000
FINISHED!
 
##########################################################################
EVALUATE QUANTIZED GRAPH COMPLETED  on FMNIST
##########################################################################
 
 
##########################################################################
COMPILE WITH Vitis  on VCK190: LeNet on FMNIST
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/fmnist/LeNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/LeNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/fmnist/LeNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/18 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 18/18 [00:00<00:00, 13789.49it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/28 [00:00<?, ?it/s]                           [INFO] infer shape (NHWC)  :100%|| 28/28 [00:00<00:00, 911.56it/s]                 
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 846.37it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 768.96it/s]                   
[INFO] generate xmodel     :  0%|          | 0/20 [00:00<?, ?it/s]                           [INFO] generate xmodel     : 15%|        | 3/20 [00:00<00:02,  7.62it/s]                   [INFO] generate xmodel     :100%|| 20/20 [00:00<00:00, 50.46it/s]                  
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/LeNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA2_C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 28
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/LeNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/LeNet/LeNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 199b2fd1a73ede9c51828bf0918bd8c1, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/LeNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on VCK190: miniVggNet  on FMNIST
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/fmnist/miniVggNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniVggNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/fmnist/miniVggNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/33 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 33/33 [00:00<00:00, 15231.87it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/45 [00:00<?, ?it/s]                           [INFO] infer shape (NHWC)  :100%|| 45/45 [00:00<00:00, 3722.09it/s]                
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 556.74it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 524.96it/s]                   
[INFO] generate xmodel     :  0%|          | 0/31 [00:00<?, ?it/s]                           [INFO] generate xmodel     : 10%|         | 3/31 [00:00<00:01, 19.09it/s]                   [INFO] generate xmodel     :100%|| 31/31 [00:00<00:00, 191.68it/s]                 
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniVggNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA2_C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 51
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniVggNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniVggNet/miniVggNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is c11fa8badcaa629517f8ddaa9fb44384, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniVggNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on VCK190: miniGoogleNet  on FMNIST
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/fmnist/miniGoogleNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniGoogleNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/fmnist/miniGoogleNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/94 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 94/94 [00:00<00:00, 19252.14it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/149 [00:00<?, ?it/s]                          [INFO] infer shape (NHWC)  :100%|| 149/149 [00:00<00:00, 13963.52it/s]             
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 292.78it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 389.49it/s]                   
[INFO] generate xmodel     :  0%|          | 0/95 [00:00<?, ?it/s]                           [INFO] generate xmodel     :100%|| 95/95 [00:00<00:00, 4879.73it/s]                
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniGoogleNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA2_C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 173
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniGoogleNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniGoogleNet/miniGoogleNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 3ab3fb5e417c6fd2eb1da393ca524ea3, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniGoogleNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on VCK190: miniResNet  on FMNIST
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/fmnist/miniResNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniResNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/fmnist/miniResNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/458 [00:00<?, ?it/s]                          [INFO] parse raw model     :100%|| 458/458 [00:00<00:00, 15872.94it/s]             
[INFO] infer shape (NHWC)  :  0%|          | 0/692 [00:00<?, ?it/s]                          [INFO] infer shape (NHWC)  :100%|| 692/692 [00:00<00:00, 18689.24it/s]             
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 69.20it/s]                    
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 69.48it/s]                    
[INFO] generate xmodel     :  0%|          | 0/516 [00:00<?, ?it/s]                          [INFO] generate xmodel     :100%|| 516/516 [00:00<00:00, 6898.66it/s]              
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniResNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCVDX8G_ISA2_C32B3
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 798
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniResNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniResNet/miniResNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 960aa045209c09b2fa166a2918018167, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniResNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILATION COMPLETED  on FMNIST on VCK190
##########################################################################
 
 
##########################################################################
COMPILE WITH Vitis AI on ZCU102: LeNet on FMNIST
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/fmnist/LeNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/LeNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/fmnist/LeNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/18 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 18/18 [00:00<00:00, 12960.94it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/28 [00:00<?, ?it/s]                           [INFO] infer shape (NHWC)  :100%|| 28/28 [00:00<00:00, 868.70it/s]                 
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 1297.74it/s]                  
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 1307.25it/s]                  
[INFO] generate xmodel     :  0%|          | 0/20 [00:00<?, ?it/s]                           [INFO] generate xmodel     : 15%|        | 3/20 [00:00<00:02,  7.71it/s]                   [INFO] generate xmodel     :100%|| 20/20 [00:00<00:00, 51.08it/s]                  
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/LeNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 28
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/LeNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/LeNet/LeNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 90e3f913125c189811153ebe240a3a0c, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/LeNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on ZCU102: miniVggNet  on FMNIST
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/fmnist/miniVggNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniVggNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/fmnist/miniVggNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/33 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 33/33 [00:00<00:00, 16646.07it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/45 [00:00<?, ?it/s]                           [INFO] infer shape (NHWC)  :100%|| 45/45 [00:00<00:00, 3475.88it/s]                
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 560.89it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 532.17it/s]                   
[INFO] generate xmodel     :  0%|          | 0/31 [00:00<?, ?it/s]                           [INFO] generate xmodel     : 10%|         | 3/31 [00:00<00:01, 18.97it/s]                   [INFO] generate xmodel     :100%|| 31/31 [00:00<00:00, 190.60it/s]                 
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniVggNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 51
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniVggNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniVggNet/miniVggNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 9c17c70d3a7ada19a601027b9068a05e, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniVggNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on ZCU102: miniGoogleNet  on FMNIST
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/fmnist/miniGoogleNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniGoogleNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/fmnist/miniGoogleNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/94 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 94/94 [00:00<00:00, 19513.22it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/149 [00:00<?, ?it/s]                          [INFO] infer shape (NHWC)  :100%|| 149/149 [00:00<00:00, 13931.77it/s]             
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 296.24it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 395.88it/s]                   
[INFO] generate xmodel     :  0%|          | 0/95 [00:00<?, ?it/s]                           [INFO] generate xmodel     :100%|| 95/95 [00:00<00:00, 4871.43it/s]                
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniGoogleNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 173
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniGoogleNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniGoogleNet/miniGoogleNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is f4928df3c2580b910dfbc8167f89bb99, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniGoogleNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on ZCU102: miniResNet  on FMNIST
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/fmnist/miniResNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniResNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/fmnist/miniResNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/458 [00:00<?, ?it/s]                          [INFO] parse raw model     :100%|| 458/458 [00:00<00:00, 16113.40it/s]             
[INFO] infer shape (NHWC)  :  0%|          | 0/692 [00:00<?, ?it/s]                          [INFO] infer shape (NHWC)  :100%|| 692/692 [00:00<00:00, 18567.53it/s]             
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 66.19it/s]                    
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 67.04it/s]                    
[INFO] generate xmodel     :  0%|          | 0/516 [00:00<?, ?it/s]                          [INFO] generate xmodel     :100%|| 516/516 [00:00<00:00, 6781.56it/s]              
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniResNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 798
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniResNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniResNet/miniResNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 0897537b4a0a842c7d1995c3c3d51088, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniResNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILATION COMPLETED on ZCU102 on FMNIST
##########################################################################
 
 
##########################################################################
COMPILE WITH Vitis AI on ZCU104: LeNet on FMNIST
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/fmnist/LeNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/LeNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/fmnist/LeNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/18 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 18/18 [00:00<00:00, 13301.18it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/28 [00:00<?, ?it/s]                           [INFO] infer shape (NHWC)  :100%|| 28/28 [00:00<00:00, 855.26it/s]                 
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 862.08it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 796.97it/s]                   
[INFO] generate xmodel     :  0%|          | 0/20 [00:00<?, ?it/s]                           [INFO] generate xmodel     : 15%|        | 3/20 [00:00<00:02,  7.64it/s]                   [INFO] generate xmodel     :100%|| 20/20 [00:00<00:00, 50.62it/s]                  
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/LeNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 28
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/LeNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/LeNet/LeNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 40dd11465109c5443ff3578a381d6175, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/LeNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on ZCU104: miniVggNet  on FMNIST
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/fmnist/miniVggNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniVggNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/fmnist/miniVggNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/33 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 33/33 [00:00<00:00, 16213.19it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/45 [00:00<?, ?it/s]                           [INFO] infer shape (NHWC)  :100%|| 45/45 [00:00<00:00, 3820.26it/s]                
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 896.09it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 795.56it/s]                   
[INFO] generate xmodel     :  0%|          | 0/31 [00:00<?, ?it/s]                           [INFO] generate xmodel     : 10%|         | 3/31 [00:00<00:01, 19.36it/s]                   [INFO] generate xmodel     :100%|| 31/31 [00:00<00:00, 194.69it/s]                 
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniVggNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 51
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniVggNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniVggNet/miniVggNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 04ea4d035b6db14a4d30582ddcbd57b0, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniVggNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on ZCU104: miniGoogleNet  on FMNIST
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/fmnist/miniGoogleNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniGoogleNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/fmnist/miniGoogleNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/94 [00:00<?, ?it/s]                           [INFO] parse raw model     :100%|| 94/94 [00:00<00:00, 19370.37it/s]               
[INFO] infer shape (NHWC)  :  0%|          | 0/149 [00:00<?, ?it/s]                          [INFO] infer shape (NHWC)  :100%|| 149/149 [00:00<00:00, 13713.49it/s]             
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 282.58it/s]                   
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 396.57it/s]                   
[INFO] generate xmodel     :  0%|          | 0/95 [00:00<?, ?it/s]                           [INFO] generate xmodel     :100%|| 95/95 [00:00<00:00, 4912.33it/s]                
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniGoogleNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 173
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniGoogleNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniGoogleNet/miniGoogleNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is fcbfb582f4589555f49a722b2e09fe07, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniGoogleNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILE WITH Vitis AI on ZCU104: miniResNet  on FMNIST
##########################################################################
[INFO] Namespace(batchsize=1, inputs_shape=None, layout='NHWC', model_files=['./build/quantized_results/fmnist/miniResNet/quantize_eval_model.pb'], model_type='tensorflow', named_inputs_shape=None, out_filename='/tmp/miniResNet_org.xmodel', proto=None)
[INFO] tensorflow model: /workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/build/quantized_results/fmnist/miniResNet/quantize_eval_model.pb
[INFO] parse raw model     :  0%|          | 0/458 [00:00<?, ?it/s]                          [INFO] parse raw model     :100%|| 458/458 [00:00<00:00, 15470.28it/s]             
[INFO] infer shape (NHWC)  :  0%|          | 0/692 [00:00<?, ?it/s]                          [INFO] infer shape (NHWC)  :100%|| 692/692 [00:00<00:00, 17875.69it/s]             
[INFO] perform level-0 opt :  0%|          | 0/3 [00:00<?, ?it/s]                            [INFO] perform level-0 opt :100%|| 3/3 [00:00<00:00, 67.96it/s]                    
[INFO] perform level-1 opt :  0%|          | 0/6 [00:00<?, ?it/s]                            [INFO] perform level-1 opt :100%|| 6/6 [00:00<00:00, 69.33it/s]                    
[INFO] generate xmodel     :  0%|          | 0/516 [00:00<?, ?it/s]                          [INFO] generate xmodel     :100%|| 516/516 [00:00<00:00, 6884.83it/s]              
[INFO] dump xmodel ...[INFO] dump xmodel: /tmp/miniResNet_org.xmodel
[UNILOG][INFO] Compile mode: dpu
[UNILOG][INFO] Debug mode: function
[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2
[UNILOG][INFO] Graph name: quantize_eval_model, with op num: 798
[UNILOG][INFO] Begin to compile...
[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1
[UNILOG][INFO] Compile done.
[UNILOG][INFO] The meta json is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniResNet/meta.json"
[UNILOG][INFO] The compiled xmodel is saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniResNet/miniResNet.xmodel"
[UNILOG][INFO] The compiled xmodel's md5sum is 00490c8e11d02eb3c1845c1cf0cf5398, and has been saved to "/workspace/tutorials/VAI-KERAS-CUSTOM-GOOGLENET-RESNET/files/./build/compile/fmnist/miniResNet/md5sum.txt"
**************************************************
* VITIS_AI Compilation - Xilinx Inc.
**************************************************
 
##########################################################################
COMPILATION COMPLETED  on FMNIST on ZCU104
##########################################################################
 
